Topic,Class_str,Class_int,Count,Name,Representation,Representative_Docs
0,emissions_materials_substances,0,58,0_concrete_strength_cement_ash,"['concrete', 'strength', 'cement', 'ash', 'mix', 'mechanical', 'fly', 'materials', 'waste', 'aggregate', 'recycled', 'material', 'properties', 'mechanical properties', 'construction', 'design', 'cs', 'sand', 'error', 'ann', 'replacement', 'mixes', 'mpa', 'fa', 'formula presented', 'r2', 'mixture', 'formula', 'content', 'gep']","['a novel evolutionary learning to prepare sustainable concrete mixtures with supplementary cementitious materials abstract: in this study, sustainable mixture designs of three concrete types, including fly ash concrete, silica fume concrete, and ground granulated blast furnace slag concrete, were investigated. to this end, the compressive strength formulas of each concrete type made with supplementary cementitious materials were obtained by introducing a new machine learning algorithm, called coyote optimization programming. the accuracy of this algorithm proved to be greater than that of conventional and recently developed machine learning methods. an optimization problem is modeled, in which the compressive strengths, price, and environmental impact of the sustainable concrete mixture designs were estimated using global warming potential, energy consumption, and material consumption as the sustainability parameters. results reveal that increasing the compressive strength reduces the sustainability of concrete, and thus, manufacturing concrete with a higher compressive strength than the one obtained from the design process contradicts the concrete’s performance. moreover, the 30-mpa sustainable fly ash concrete was proven to be the most sustainable mix with a gray relational grade of 1. this optimal mixture designed in this study can decrease the unit cost, global warming potential, energy consumption, and material consumption by 36.6%, 51%, 43%, and 11%, respectively. research highlights: a novel machine learning method called coyote optimization programming was introduced in this study.applying optimization techniques to design concrete mixture proportions can reduce the unit cost by 36.6%.the introduced approach can decrease the global warming potential, energy consumption, and material consumption by 51%, 43%, and 11%, respectively.the application of supplementary cementitious materials in the concrete mixtures significantly enhances sustainability.', 'data-driven approach for investigating and predicting of compressive strength of fly ash–slag geopolymer concrete fly ash–slag geopolymer concrete is an intangible material that does not use conventional portland cement, thereby reducing co2 emissions into the environment, and helping to increase sustainable development. however, compared with conventional concrete, the compressive strength of fly ash–slag geopolymer concrete is complexly dependent on many factors. using the data-driven approach for investigating and predicting fly ash–slag geopolymer concrete compressive strength is a suitable choice. this study introduces 11 easily accessible machine learning models in open-source libraries of the python programming language such as support vector machine, random forest (rf), gradient boosting (gb), adaboost, decision trees, light gb machine, extreme gb (xgb), k-nearest neighbors, multivariable regression, gaussian process regression, and catboost (catb). based on a dataset of 158 samples, 14 inputs, and 1 output variable compressive strength, the performance of 11 machine learning models was evaluated through 4 criteria including coefficient of determination, root mean square error, mean absolute error, and mean absolute percentage error combined with 10 repeats of 10-fold cross-validation. four models have the best performance based on the above four criteria value in determining compressive strength for testing dataset sorted descending is catb > xgb > rf > gb. global shapley (shap) value-based catb and xgb indicates three groups of factors with decreasing influence on compressive strength of geopolymer concrete: group i (slag, molarity, coarse aggregate, curing temperature, and alkaline activator/binder) > group ii (na2sio3 content, naoh content, fine aggregate, fly ash content), curing period > group iii (extra water added, naoh/na2sio3, superplasticizer content, rest period). extra water added, naoh/na2sio3, superplasticizer content, rest period have insignificant influence on the compressive strength value of geopolymer concrete. the greater the slag content in the slag–fly ash mixture, the greater the compressive strength of geopolymer concrete. the optimum molarity of naoh concentration is about 14–16 m for designing the compressive strength of geopolymer concrete. shap values partial dependence plots (pdp) and pdp indicate that alkaline activator/binder optimal values exist to achieve high compressive strength. the compressive strength increases with curing temperature between 20 and 100°c. pdp values show that the tendency to increase compressive strength with increasing coarse aggregate content from about 750 to 1250 kg/m3.', 'concrete compressive strength prediction modeling utilizing deep learning long short-term memory algorithm for a sustainable environment one of the most critical parameters in concrete design is compressive strength. as the compressive strength of concrete is correctly measured, time and cost can be decreased. concrete strength is relatively resilient to impacts on the environment. the production of concrete compressive strength is greatly influenced by severe weather conditions and increases in humidity rates. in this research, a model has been developed to predict concrete compressive strength utilizing a detailed dataset obtained from previously published studies based on a deep learning method, namely, long short-term memory (lstm), and a conventional machine learning (ml) algorithm, namely, support vector machine (svm). the input variables of the model include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate, and age of specimens. to demonstrate the efficiency of the proposed models, three statistical indices, namely, the coefficient of determination (r2), mean absolute error (mae), and root mean square error (rmse), were used. findings shows that lstm outperformed svm with r2=0.98, r2= 0.78, mae=1.861, mae=6.152, and rmse=2.36, rmse=7.93, respectively. the results of this study suggest that high-performance concrete (hpc) compressive strength can be reliably measured using the proposed lstm model.']"
10,emissions_materials_substances,0,28,10_ship_shipping_ships_maritime,"['ship', 'shipping', 'ships', 'maritime', 'vessel', 'port', 'route', 'arctic', 'fuel', 'marine', 'emissions', 'trade', 'routes', 'gas', 'transport', 'sea', 'navigation', 'emission', 'transportation', 'international', 'logistics', 'decision', 'reduce', 'natural gas', 'support', 'noise', 'passenger', 'grid', 'routing', 'pollution']","['using artificial intelligence (ai) methods to combat climate change at marine ports marine ports operations are often associated with a variety of externalities including air pollution, noise, accidents, vibration, land take and visual intrusion. climate change is considered to be a crucial challenge that mankind has to confront nowadays special attention has to be paid to the emissions of greenhouse gasses from freight transport. when berthing at a port a vessel needs considerably large amount of electric power to support its operations such as loading, unloading, lighting, cooling, etc. the power is usually supplied by auxiliary machinery and the fuel used causes several gasses emission that results in air pollution. furthermore, this kind of engines produce noise pollution to a neighbourhood. the negative factors have an impact on the working environment and the quality of life of the citizens living in an area adjacent to a port. a universal method of shore-to-ship electrification, also known as cold ironing, has been recently applied for connection between all the types of ships or on-land electrical systems with different frequencies – 50 and 60 hz. although the cold ironing is a way to reduce ships‟ emission and air pollution of a port and its neighboring areas consequently, the fact that the ship is connected with a grid is a disadvantage. the disadvantage lies in its holistic approach to combat climate change. the electrical grid is powered by fossil fuels so the total contribution to air emission is limited. the zero emissions‟ port approach using a smart grid technology approach connected to renewable energy sources. the electrical grid is used only as a backup source in a situation where there is a deficit in power balance. the offered energy sources, found in nature, are wind, solar, geothermal, tidal and wave energy while there is also energy in biomass and earthquakes. although there are so many of them, the challenge is the conversion to electricity and the efficiency of the converting systems. the use of such sources for commercial electrical supply is only possible with the new “smart grid” concept. the optimal control of such systems soon will require up-to-date algorithms with the use of artificial intelligence(ai). in the paper, an overview of ai methods for smart grid energy management optimization are presented for ports discussing the potential application of each algorithm to zero-emission port concepts.', 'integrated ship energy flowchart: a digital twin to mitigate ghg emissions the alarming rate of climate change accentuates the need to reduce greenhouse gas (ghg) emissions produced from anthropogenic activities and consequently the consumption of fossil fuels. the transportation sector is one of the most energy-demanding activities, consisting around 27% of the global primary energy demand and one of the major contributors of ghg emissions to the atmosphere, while shipping transportation accounts for nearly 12% of its co2 emissions. decarbonization is vital for emission mitigation using innovative technologies, policies, and incentives at a local and international level. in this context, the presented integrated ship energy flowchart (isef), aims to create a digital twin of a ship and carry out deterministic calculations, such as engine power requirements and by extension fuel consumption and emissions, by modelling the various components of a ships energy flow. most modeling approaches depend on tracking data from automatic identification systems (ais) and commercial vessel databases, accompanied with prohibitive costs for many, as well as missing vessel characteristics. isef, on the other hand, aims to fill in the gap in case of missing or costly to obtain data while maintaining the flexibility to utilize field data if available. this is done by providing representative vessel characteristics, detailed engine modeling and simulating components such as environmental conditions (sea-state, wind). at the same time, isef develops a library of vessel data including ship particulars, engine and route information among others. thus, it is also suitable for the validation of tracking information and machine learning or other deterministic algorithms. additionally, this library will enable the development of a statistically representative ship describing the international fleet. this will therefore improve projection algorithms utilized in calculations and aid the evaluation of mitigation options regarding decarbonisation in terms of the overall fleet. such a model also enables the investigation of alternative fuels and fuel mixtures, route optimization, and inclusion of cold ironing amongst others. the current objectives include the validation of the core modelling implementation via comparisons with available raw data to serve as a reference case and build the necessary libraries. therefore, a case study of a specific ship utilizing real navigational data will be used to demonstrate the capabilities of the algorithm.', 'the feasibility of autonomous low-emission ro-ro passenger shipping in the adriatic sea digitalization, artificial intelligence, and awareness of climate change are the driving forces of the rapid development of the transportation industry. as seaborne trade grows, the maritime sector is exploring the applicability of alternative powering options and ways to implement new technologies to increase safety and efficiency. autonomous ships have recently generally been presented as more cost-effective and environmentally friendly than conventional manned ships. this paper deals with the feasibility of autonomous ro-ro passenger ships combined with the application of alternative power options, where the croatian short-sea shipping sector is taken as an illustrative example. an analysis is conducted for heavy fuel oil, marine diesel oil, liquefied natural gas, methanol, electricity, and fossil and renewable hydrogen for three different ships operating on short, medium, and relatively long routes. for the ship lifetime of 20 years, through five key performance indicators (kpis), the environmental and economic impact of the considered options is evaluated and the potential for their autonomous application is investigated. taking into account the contrasts between conventional and autonomous ships, the differences in the costs for autonomous ship are outlined, and the kpis of conventional and autonomous ro-ro passenger ships are compared. the results indicate that from an environmental and economic point of view, methanol and electricity-powered ships are the best option for all three routes. regarding autonomous shipping, the ecological and economic benefits are obvious for all the considered power options and ships, except for the renewable hydrogen-powered ship operating on the longest route.']"
18,emissions_materials_substances,0,79,18_waste_recycling_plastic_garbage,"['waste', 'recycling', 'plastic', 'garbage', 'waste management', 'plastics', 'classification', 'plastic waste', 'bin', 'sorting', 'iot', 'deep', 'deep learning', 'trash', 'management', 'environment', 'solid', 'image', 'bins', 'based', 'smart', 'solid waste', 'detection', 'disposal', 'vision', 'real', 'inspection', 'learning', 'automatic', 'paper']","['smart waste management and classification systems using cutting edge approach with a rapid increase in population, many problems arise in relation to waste dumps. these emits hazardous gases, which have negative effects on human health. the main issue is the domestic solid waste collection, management, and classification. according to studies, in america, nearly 75% of waste can be recycled, but there is a lack of a proper real-time waste-segregating mechanism, due to which only 30% of waste is being recycled at present. to maintain a clean and green environment, we need a smart waste management and classification system. to tackle the above-highlighted issue, we propose a real-time smart waste management and classification mechanism using a cutting-edge approach (swmacm-ca). it uses the internet of things (iot), deep learning (dl), and cutting-edge techniques to classify and segregate waste items in a dump area. moreover, we propose a waste grid segmentation mechanism, which maps the pile at the waste yard into grid-like segments. a camera captures the waste yard image and sends it to an edge node to create a waste grid. the grid cell image segments act as a test image for trained deep learning, which can make a particular waste item prediction. the deep-learning algorithm used for this specific project is visual geometry group with 16 layers (vgg16). the model is trained on a cloud server deployed at the edge node to minimize overall latency. by adopting hybrid and decentralized computing models, we can reduce the delay factor and efficiently use computational resources. the overall accuracy of the trained algorithm is over 90%, which is quite effective. therefore, our proposed (swmacm-ca) system provides more accurate results than existing state-of-the-art solutions, which is the core objective of this work.', 'municipal solid waste classification and real-time detection using deep learning methods waste management has become a significant issue in most developing countries. municipal solid waste generation has been steadily increasing over the past decade. recycling is gaining importance because it is the only method to maintain a healthy and sustainable environment. furthermore, recycling is not a fully autonomous operation; a significant amount of waste must be performed manually. new and innovative procedures must be implemented to deal with the increasing volume of waste products at recycling facilities. it is suggested that effective solid waste management systems have a practical approach to detecting and classifying waste materials. this study presents cnn and graph-lstm, two deep-learning techniques that can recognize typical waste products when handled on a belt conveyor in waste collection systems. this convolutional neural network-based solution is trained to use six object classes: cardboard, metal, glass, plastic, paper, and organic waste. the major advantage is the ability to model long-term dependencies, improved performance, better generalization, and easy access the experimental findings show that the suggested system can achieve 97.5% accuracy in real-world situations, outperforming existing methods identified in the literature.', 'a novel strategy for waste prediction using machine learning algorithm with iot based intelligent waste management system internet of things (iot) has now become an embryonic technology to elevate the whole sphere into canny cities. hasty enlargement of smart cities and industries leads to the proliferation of waste generation. waste can be pigeon-holed as materials-based waste, hazard potential based waste, and origin-based waste. these waste categories must be coped thoroughly to make certain of the ecological finest run-throughs irrespective of the origin or hazard potential or content. waste management should be incorporated into ecological preparation since it is a grave piece of natural cleanliness. the most important goalmouth of waste management is to maintain the pecuniary growth and snootier excellence of life by plummeting and exterminating adversative repercussions of waste materials on environment and human health. disposing of unused things is a significant issue, and this ought to be done in the best manner by deflecting waste development and keeping hold of cost, and it involves countless human resources to deal with the waste. these current techniques predominantly focus on cost-effective monitoring of waste management, and results are not imprecise, so that it could not be developed in real time or practically applications such as in educational organizations, hospitals, and smart cities. internet of things-based waste management system provides a real-time monitoring system for collecting the garbage waste, and it does not control the dispersion of overspill and blowout gases with poor odor. consequently, it leads to the emission of radiation and toxic gases and affects the environment and social well-being and induces global warming. motivated by these points, in this research work, we proposed an automatic method to achieve an effective and intelligent waste management system using internet of things by predicting the possibility of waste things. the wastage capacity, gas level, and metal level can be monitored continuously using iot based dustbins, which can be placed everywhere in city. then, our proposed method can be tested by machine learning classification techniques such as linear regression, logistic regression, support vector machine, decision tree, and random forest algorithm. the proposed method is investigated with machine learning classification techniques in terms of accuracy and time analysis. random forest algorithm gives the accuracy of 92.15% and time consumption of 0.2 milli seconds. from this analysis, our proposed method with random forest algorithm is significantly better compared to other classification techniques.']"
35,emissions_materials_substances,0,60,35_fuel_emission_vehicle_vehicles,"['fuel', 'emission', 'vehicle', 'vehicles', 'emissions', 'fuel consumption', 'engine', 'co2', 'consumption', 'combustion', 'transportation', 'engines', 'hydrogen', 'diesel', 'electric', 'road', 'co2 emissions', 'carbon', 'fuels', 'driving', 'real', 'electric vehicles', 'efficiency', 'traffic', 'co2 emission', 'carbon dioxide', 'duty', 'oil', 'transport', 'ignition']","['analysis and prediction model of fuel consumption and carbon dioxide emissions of light-duty vehicles due to the alarming rate of climate change, fuel consumption and emission estimates are critical in determining the effects of materials and stringent emission control strategies. in this research, an analytical and predictive study has been conducted using the government of canada dataset, containing 4973 light-duty vehicles observed from 2017 to 2021, delivering a comparative view of different brands and vehicle models by their fuel consumption and carbon dioxide emissions. based on the findings of the statistical data analysis, this study makes evidence-based recommendations to both vehicle users and producers to reduce their environmental impacts. additionally, convolutional neural networks (cnn) and various regression models have been built to estimate fuel consumption and carbon dioxide emissions for future vehicle designs. this study reveals that the univariate polynomial regression model is the best model for predictions from one vehicle feature input, with up to 98.6% accuracy. multiple linear regression and multivariate polynomial regression are good models for predictions from multiple vehicle feature inputs, with approximately 75% accuracy. convolutional neural network is also a promising method for prediction because of its stable and high accuracy of around 70%. the results contribute to the quantifying process of energy cost and air pollution caused by transportation, followed by proposing relevant recommendations for both vehicle users and producers. future research should aim towards developing higher performance models and larger datasets for building apis and applications.', 'predicting gasoline vehicle fuel consumption in energy and environmental impact based on machine learning and multidimensional big data the underestimation of fuel consumption impacts various aspects. in the vehicle market, manufacturers often advertise fuel economy for marketing. in fact, the fuel consumption reference value provided by the manufacturer is quite different from the real-world fuel consumption of the vehicles. the divergence between reference fuel consumption and real-world fuel consumption also has negative effect on the aspects of policy and environment. in order to effectively promote the sustainable development of transport, it is urged to recognize the real-world fuel consumption of vehicles. the gaps in previous studies includes small sample size, single data dimension, and lack of feature weight evaluation. to fill the research gap, in this study, we conduct a comparative analysis through building five regression models to forecast the real-world fuel consumption rate of light-duty gasoline vehicles in china based on big data from the perspectives of vehicle factors, environment factors, and driving behavior factors. results show that the random forest regression model performs best among the five candidate models, with a mean absolute error of 0.630 l/100 km, a mean absolute percentage error of 7.5%, a mean squared error of 0.805, an r squared of 0.776, and a 10-fold cross-validation score of 0.791. further, we capture the most important features affecting fuel consumption among the 25 factors from the above three perspectives. according to the relative weight of each factor in the most optimal model, the three most important factors are brake and accelerator habits, engine power, and the fuel economy consciousness of vehicle owners in sequence.', 'estimation of real-world fuel consumption rate of light-duty vehicles based on the records reported by vehicle owners private vehicle travel is the most basic mode of transportation, so that an effective way to control the real-world fuel consumption rate of light-duty vehicles plays a vital role in promoting sustainable economic growth as well as achieving a green low-carbon society. therefore, the factors impacting individual carbon emissions must be elucidated. this study builds five different models to estimate the real-world fuel consumption rate of light-duty vehicles in china. the results reveal that the light gradient boosting machine (lightgbm) model performs better than the linear regression, naïve bayes regression, neural network regression, and decision tree regression models, with a mean absolute error of 0.911 l/100 km, a mean absolute percentage error of 10.4%, a mean square error of 1.536, and an r-squared (r2) value of 0.642. this study also assesses a large pool of potential factors affecting real-world fuel consumption, from which the three most important factors are extracted, namely, reference fuel-consumption-rate value, engine power, and light-duty vehicle brand. furthermore, a comparative analysis reveals that the vehicle factors with the greatest impact are the vehicle brand, engine power, and engine displacement. the average air pressure, average temperature, and sunshine time are the three most important climate factors.']"
38,emissions_materials_substances,0,17,38_self powered_powered_self_energy harvesting,"['self powered', 'powered', 'self', 'energy harvesting', 'harvesting', 'sensors', 'sensor', 'energy', 'vibration', 'power', 'electrical', 'electronics', 'mechanical', 'bacteria', 'devices', 'materials', 'wearable', 'interfaces', 'material', 'human machine', 'applications', 'high', 'cotton', 'motions', 'ct', 'sustainable', 'ambient', 'electrical power', 'coffee', 'environments']","['comprehensive piezoelectric material application issues on energy harvesting for artificial intelligence systems artificial intelligence (ai) technologies are integrated into various modem devices and systems to make our living more convenience and comfortable. the initial of a fully functional ai system is integrated with computer, sensor network, actuator (robotic), and electrical power for the hardware components. for many situations, city power sources might not available. therefore, a reliable, renewable and sustainable, local power generators-based self-powered ai system is desired. piezoelectric energy harvesting (peh) technology, which use piezoelectric material-based device to harvest vibration/motion mechanical energy to electrical energy has the highest possibility to make self-powered ai system since 1) vibration/motion mechanical energy is unique that not weather dependent, 2) peh can harvest either tiny or massive vibration/motion mechanical energy into electrical energy. the piezoelectric materials are materials that generate electrical charges when a mechanical stress or force is exerted on them, on the other hand, they deformation when an electric voltage is applied on it. the implementation of piezoelectric materials-based energy harvesters are effective devices that promises future engineering systems that are more intelligent, reliable and environment friendly. however, designing a piezoelectric device is complicated and require comprehensive understating of many engineering disciplines including mechanical engineering, electrical engineering, materials sciences, and device physics. this paper offers a comprehensive discussion on the major piezoelectric materials and devices classifications as well as the state-of-the-art designs. in addition, the performance of various peh devices are also summarized.', 'deformable, resilient, and mechanically-durable triboelectric nanogenerator based on recycled coffee waste for wearable power and self-powered smart sensors maximizing resource recycling and finding renewable energy sources are important for a sustainable environment considering the development and consumption of electronic products with the advancement of the internet of things and artificial intelligence technologies. herein, an economic and environment-friendly triboelectric nanogenerator derived from the discarded coffee ground waste (cg-teng) is developed to serve as a light-weight and shape-adaptive energy source and self-powered sensitive sensor (cg-teng sensor). the coffee ground, embedded into the silicone rubber elastomer, is first used as the metal-free electrode feedstock to minimize waste generation. based on the triboelectrification and electrostatic induction, the shape-adaptive cg-teng, as wearable power, is capable of harvesting surrounding energy from human motions and extreme deformations, exhibiting excellent stretchability and mechanical durability. the generated electricity can be stored in the conductive coffee ground-derived capacitors to drive portable electronics, which is beneficial for building completely green power systems using coffee waste. furthermore, the self-powered cg-teng sensor with optimized structure endows its ultra-high sensitivity for sensing human physiological signals, monitoring motions, emulating gestures, as well as for developing smart tactile epidermal controller and intelligent vending coaster, paving the way for building large-scale energy-efficient artificial sensors and eco-environmental wearable electronics towards humanoid robotics and human-machine interfaces.', 'self-powered sensors: applications, challenges, and solutions sensing technology has long been an integral part of modern developments that have brought several benefits to various domains ranging from civil and military to commercial and healthcare applications. however, the sensor&#x2019;s inadequate energy is the primary concern that affects their long-term sustainable operations and remote location deployment. therefore, self-powered sensors with the ability to scavenge energy from the environment to self-drive the sensor&#x2019;s operation have received significant attention in recent years. self-powered sensors with energy harvesting technology can convert the ambient energy available in the environment, such as mechanical, thermal, wave, and solar energy, into electric energy to self-power the sensors for long-term sustainable operations. self-powered sensors have been a vital part of technological advances since the 21st century and the existing literature has thoroughly studied the fabrication material of self-powered sensors and their energy harvestingmechanisms. however, the current literature lacks a comprehensive review on state-of-the-art design solutions for self-powered sensors and their implementation challenges. to address the limitations of past and current reviews, we have reviewed state-of-the-art architectures of self-powered sensors and their applications in various domains (such as civil, automotive systems, environmental, robotic, human-machine interactions, healthcare and fitness applications). moreover, following the study of existing designs of self-powered sensors, we provide a general architecture for a self-powered sensor design, and we discuss the implementation challenges of self-powered sensors that affect their long-term operations. these challenges include energy, size, cost, robustness, stability, and artificial intelligence (ai). we also discuss possible solutions to address the implementation challenges we have identified to enable the design of cost-effective self-powered sensor-based monitoring systems for diverse applications.']"
39,emissions_materials_substances,0,21,39_methane_leak_co2_sensor,"['methane', 'leak', 'co2', 'sensor', 'gas', 'emissions', 'point cloud', 'deep', 'networks', 'images', 'detection', 'neural', 'deep learning', 'gans', 'source', 'synthetic', 'neural networks', 'activity recognition', 'trained', 'point', 'workflow', 'convolutional', 'convolutional neural', 'sources', 'rate', 'power', 'monitoring', 'network', 'training', 'coal']","['videogasnet: deep learning for natural gas methane leak classification using an infrared camera mitigating methane leakage from the natural gas system has become an increasing concern for climate change. efficacious methane leak detection and classification can make mitigation more efficient and cost effective. optical gas imaging (ogi) is widely used for the purpose of leak detection, but it does not directly provide quantitative detection results and leak sizes. in this study, we consider methane leak size classification as a video classification problem and develop deep learning models to classify the videos by leak volume. firstly we collected the first methane leak video dataset - gasvid, which has ∼0.7 m frames of labeled videos of methane leaks from different leaking equipment, covering a wide range of leak sizes (5.3–2051.6 gch4/h) and imaging distances (4.6–15.6 m). secondly, we studied three deep learning algorithms, including 2d convolutional neural networks (cnn) model, 3d cnn and the convolutional long short term memory (convlstm). we find that 3d cnn is the most accurate and robust architecture, and we name it videogasnet. the leak/non-leak binary detection accuracy approaches nearly 100%, and the highest small-medium-large classification accuracy is 78.2% with our 3d cnn network. in eight-class classification, accuracy drops to 39.1%, as this is a more challenging task. in summary, videogasnet greatly extends the capabilities of ir camera-based leak monitoring system from leak detection only to automated leak classification, and shows high accuracy for binary and three-class classification.', 'an efficient deep learning-based workflow for co<inf>2</inf> plume imaging using distributed pressure and temperature measurements geologic carbon dioxide (co2) sequestration has received significant attention from the scientific community as a response to global warming due to greenhouse gas emission. effective monitoring of co2 plume is critical to co2 storage safety throughout the life-cycle of a geologic co2 sequestration project. although simulation-based techniques such as history matching can be used for predicting the evolution of underground co2 saturation, the computational cost of the high-fidelity simulations can be prohibitive. recent development in data-driven models can provide a viable alternative for rapid co2 plume imaging. here, we present a novel deep learning-based workflow that can efficiently visualize co2 plume in near real-time. our deep learning framework utilizes field measurements, such as downhole pressure, distributed pressure and temperature as input to visualize the subsurface co2 plume images. however, the high output dimension of co2 plume images makes the training inefficient. we address this challenge in two ways: first, we output a single co2 onset time map rather than multiple saturation maps at different times; second, we apply an autoencoder-decoder network to identify lower dimensional latent variables that compress high dimensional output images. the onset time is the calendar time when the co2 saturation at a given location exceeds a specified threshold value. in our approach, a deep learning-based regression model is trained to predict latent variables of the autoencoder-decoder network. subsequently the latent variables are used as inputs of the trained decoder network to generate the 3d onset time image, visualizing the evolving co2 plume in near real-time. the power and efficacy of our approach are demonstrated using both synthetic and field-scale applications. we first validate the deep learning-based co2 plume imaging workflow using a 2d synthetic example. next, the visualization workflow is applied to a 3d field-scale reservoir to demonstrate the robustness and efficiency of the workflow. the monitoring data set consists of distributed temperature sensing (dts) data acquired at a monitoring well, flowing bottom-hole pressure data at the injection well, and time-lapse pressure measurements at several locations along the monitoring well. our approach is also extended to efficiently evaluate the uncertainty of predicted co2 plume images. additionally, an efficient workflow for optimizing data acquisition and measurement type is demonstrated using our deep learning-based framework. the novelty of this work is the development and applications of a unique and efficient deep learning-based subsurface visualization workflow for the spatial and temporal migration of the co2 plume. the efficiency and flexibility of the data-driven workflow make our approach suitable for field-scale applications.', 'deep learning models for methane emissions identification and quantification methane emissions are a crucial environmental concern for climate change. therefore, locating, quantifying, and mitigating these emissions sources is essential. methane signatures from orphan wells can be detecting from drones equipped with the appropriate sensors but a methodology is required to estimate methane flux from the measurements made by the drone. we trained a convolutional neural network (cnn) using large eddy simulations (les) dataset of methane plumes that mimic the real dataset of the next-generation airborne visible/infrared imaging spectrometer (aviris-ng) where wind speeds are varied from 1 m/s to 4 m/s with varying source flux rates of 20-100 kg/hr. we adopted a deep neural network architecture, similar to visual geometry group (vgg), to train our model on synthetic data efficiently. the model architecture is much simpler than vgg and is very fast to train. we optimized the model parameters and architecture to improve the models precision and quantified the source flux rates of methane plumes with high accuracy and tested our trained model using synthetic 2d images. in the synthetic dataset, the overall predictability of source flux rates from methane plume data is excellent, and the mean absolute percentage error for predicting the true source flux rate values of all the training images is 4.31%, for all validation images is 12.4%, and for all test images is 8.27%. once validated against field data, a machine learning model such as this can be used as a screening tool to determine which orphan wells are leaking large amounts of methane so that these wells can be prioritized for plugging and abandonment.']"
44,emissions_materials_substances,0,48,44_biomass_production_energy_fuel,"['biomass', 'production', 'energy', 'fuel', 'algae', 'waste', 'bioenergy', 'methane', 'fuels', 'oil', 'reaction', 'conversion', 'process', 'methane emissions', 'wastes', 'combustion', 'hydrothermal', 'bio', 'gas', 'optimization', 'alternative', 'feedstock', 'treatment', 'high', 'emissions', 'artificial', 'high throughput', 'biofuels', 'biofuel', 'sources']","['pyrolysis of lignocellulosic, algal, plastic, and other biomass wastes for biofuel production and circular bioeconomy: a review of thermogravimetric analysis (tga) approach fossil fuels are currently the most significant energy sources. they are expected to become less available and more expensive, leading to a great demand for energy conservation and alternative energy sources. as a sustainable and renewable energy source, biomass has piqued interest in generating bioenergy and biofuels over recent years. the thermal conversion of biomass through pyrolysis is an easy, useful, and low-cost process that can be applied to a wide variety of feedstocks. pyrolysis characteristics of different feedstock samples can be analyzed and examined through thermogravimetric analysis (tga). tga has been an essential tool and widely used to investigate the thermal characteristics of a substance under heating environments, such as thermodegradation dynamics and kinetics. studying the potential of waste biomass for generating sustainable bioenergy carves a pathway into a circular bioeconomy regime, and can help tackle our heavy reliance on nonrenewable energy sources. this study aims to give a deep insight into the wide use of tga in aiding in the research and development of pyrolysis of different waste biomass sources. the thermal characteristics portrayed by different biomass wastes through tga are discussed. the effects of significant pyrolysis operating parameters are also illustrated. a more comprehensive understanding of evolved products during the pyrolysis stage can be gained by combining tga with other analytical methods. the pros and cons of using tga are also outlined. overall, an in-depth literature review helps identify current trends and technological improvements (i.e., integrating artificial intelligence) of tga use with pyrolysis.', 'estimation of higher heating values (hhvs) of biomass fuels based on ultimate analysis using machine learning techniques and improved equation to have a sustainable economy and environment, several countries have widely inclined to the utilization of non-fossil fuels like biomass fuels to produce heat and electricity. the advantage of employing biomass for combustion is emerging as a potential renewable energy, which is regarded as a cheap fuel. chemical constituents or elements are essential properties in biomass applications, which would be costly and labor-intensive to experimentally estimate them. one of the criteria to evaluate the energy of biomass from an economic perspective is the higher heating value (hhv). in the present work, we have applied multilayer perceptron artificial neural network (mlp-ann), least-squares support vector machine (lssvm), ant colony-adaptive neuro-fuzzy inference system (aco-anfis), particle swarm optimization- anfis (pso-anfis), genetic algorithm-radial basis function (ga-rbf) and new multivariate nonlinear regression (mnr) as accurate correlation methods to estimate hhvs of biomass fuels based on the ultimate analysis. 535 experimental data were gathered from literature and categorized into eight classes of by-products of fruits, agri-wastes, wood chips/tree species, grasses/leaves/fibrous materials, other waste materials, briquettes/charcoals/pellets, cereal and industrial wastes. in the term of statistical analysis, average absolute relative deviation (aard) authenticates that mnr and ga-rbf algorithm with %aard of 3.5 and 3.4 could be used to estimate hhv. in addition, developed models results were compared to the results of 69 recently previously published empirical correlations and it confirms the reliability of our results. relevency factor shows the impact of biomass elements on hhv and outlier analysis indicates the unreliable experimental data. the results of this study can be used by researchers to design and optimize biomass combustion systems.', 'machine learning aided bio-oil production with high energy recovery and low nitrogen content from hydrothermal liquefaction of biomass with experiment verification hydrothermal liquefaction (htl) of biomass with high moisture (e.g., algae, sludge, manure, and food waste) is a promising and sustainable approach to produce renewable energy (bio-oil) and protect the environment. however, the production of bio-oil with high yield and preferable properties such as low nitrogen content (n_oil) is time/labor-consuming using the traditional htl experimental method. to this context, machine learning (ml) algorithms were employed to aid the bio-oil production with the consideration of related factors in htl, including biochemical and elemental compositions of biomass, process parameters, and solvents. results showed that the random forest (rf) algorithm was the best one (average r2 = 0.80) for the multi-task prediction of bio-oil yield (yield_oil), n_oil, and energy recovery (er_oil), hence employed for post feature interpretation and optimization. feature importance indicates that both yield_oil and er_oil follow the trend of lipid content in biomass > temperature > retention time, while n_oil follows the trend of n content in biomass > temperature > retention time. then ml-based optimization was conducted to guide the experimental research to produce bio-oil with high yield and low n content. the htl experiment verification based on the optimal solutions from the ml-based particle swarm optimization achieved the maximum yield_oil (54.30%) and minimal n_oil (2.60%) from model biomass (composed of protein content 28%, lipid 48%, and carbohydrates 21%) at 300 °c and 30 min. the experiment verification was successful as the results were comparable to the modeling results, with errors of less than 7% for yield_oil and er_oil, and 23% for n_oil, and the n_oil of the experiment n_oil (2.60%) was interestingly lower than the modeled one (3.37%). this work provides new insight and strategy to accelerate the engineered htl for desired bio-oil production.']"
48,emissions_materials_substances,0,22,48_chemicals_chemical_nanoparticles_adsorption,"['chemicals', 'chemical', 'nanoparticles', 'adsorption', 'exposure', 'toxicity', 'nano', 'human health', 'descriptors', 'health', 'silicon', 'environmental', 'human', 'risk', 'remediation', 'safe sustainable', 'enms', 'risks', 'safe', 'molecules', 'new', 'molecular', 'environment', 'sustainability', 'soil', 'chemically', 'computational', 'friendliness', 'life cycle', 'materials']","['improved machine learning models by data processing for predicting life-cycle environmental impacts of chemicals machine learning (ml) provides an efficient manner for rapid prediction of the life-cycle environmental impacts of chemicals, but challenges remain due to low prediction accuracy and poor interpretability of the models. to address these issues, we focused on data processing by using a mutual information-permutation importance (mi-pi) feature selection method to filter out irrelevant molecular descriptors from the input data, which improved the model interpretability by preserving the physicochemical meanings of original molecular descriptors without generation of new variables. we also applied a weighted euclidean distance method to mine the data most relevant to the predicted targets by quantifying the contribution of each feature, thereby the prediction accuracy was improved. on the basis of above data processing, we developed artificial neural network (ann) models for predicting the life-cycle environmental impacts of chemicals with r2 values of 0.81, 0.81, 0.84, 0.75, 0.73, and 0.86 for global warming, human health, metal depletion, freshwater ecotoxicity, particulate matter formation, and terrestrial acidification, respectively. the ml models were interpreted using the shapley additive explanation method by quantifying the contribution of each input molecular descriptor to environmental impact categories. this work suggests that the combination of feature selection by mi-pi and source data selection based on weighted euclidean distance has a promising potential to improve the accuracy and interpretability of the models for predicting the life-cycle environmental impacts of chemicals.', 'a horizon scan to support chemical pollution–related policymaking for sustainable and climate-resilient economies while chemicals are vital to modern society through materials, agriculture, textiles, new technology, medicines, and consumer goods, their use is not without risks. unfortunately, our resources seem inadequate to address the breadth of chemical challenges to the environment and human health. therefore, it is important we use our intelligence and knowledge wisely to prepare for what lies ahead. the present study used a delphi-style approach to horizon-scan future chemical threats that need to be considered in the setting of chemicals and environmental policy, which involved a multidisciplinary, multisectoral, and multinational panel of 25 scientists and practitioners (mainly from the united kingdom, europe, and other industrialized nations) in a three-stage process. fifteen issues were shortlisted (from a nominated list of 48), considered by the panel to hold global relevance. the issues span from the need for new chemical manufacturing (including transitioning to non-fossil-fuel feedstocks); challenges from novel materials, food imports, landfills, and tire wear; and opportunities from artificial intelligence, greater data transparency, and the weight-of-evidence approach. the 15 issues can be divided into three classes: new perspectives on historic but insufficiently appreciated chemicals/issues, new or relatively new products and their associated industries, and thinking through approaches we can use to meet these challenges. chemicals are one threat among many that influence the environment and human health, and interlinkages with wider issues such as climate change and how we mitigate these were clear in this exercise. the horizon scan highlights the value of thinking broadly and consulting widely, considering systems approaches to ensure that interventions appreciate synergies and avoid harmful trade-offs in other areas. we recommend further collaboration between researchers, industry, regulators, and policymakers to perform horizon scanning to inform policymaking, to develop our ability to meet these challenges, and especially to extend the approach to consider also concerns from countries with developing economies. environ toxicol chem 2023;42:1212–1228. © 2023 crown copyright and the authors. environmental toxicology and chemistry published by wiley periodicals llc on behalf of setac. this article is published with the permission of the controller of hmso and the kings printer for scotland.', 'development and evaluation of a holistic and mechanistic modeling framework for chemical emissions, fate, exposure, and risk background: large numbers of chemicals require evaluation to determine if their production and use pose potential risks to ecological and human health. for most chemicals, the inadequacy and uncertainty of chemical-specific data severely limit the application of exposure-and risk-based methods for screening-level assessments, priority setting, and effective management. objective: we developed and evaluated a holistic, mechanistic modeling framework for ecological and human health assessments to support the safe and sustainable production, use, and disposal of organic chemicals. methods: we consolidated various models for simulating the production-to-exposure (protex) continuum with empirical data sets and models for predicting chemical property and use function information to enable high-throughput (ht) exposure and risk estimation. the new protex-ht framework calculates exposure and risk by integrating mechanistic computational modules describing chemical behavior and fate in the socioeconomic system (i.e., life cycle emissions), natural and indoor environments, various ecological receptors, and humans. protex-ht requires only molecular structure and chemical tonnage (i.e., annual production or consumption volume) as input information. we evaluated the protex-ht framework using 95 organic chemicals commercialized in the united states and demonstrated its application in various exposure and risk assessment contexts. results: seventy-nine percent and 97% of the protex-ht human exposure predictions were within one and two orders of magnitude, respectively, of independent human exposure estimates inferred from biomonitoring data. protex-ht supported screening and ranking chemicals based on various exposure and risk metrics, setting chemical-specific maximum allowable tonnage based on user-defined toxicological thresholds, and identifying the most relevant emission sources, environmental media, and exposure routes of concern in the protex continuum. the case study shows that high chemical tonnage did not necessarily result in high exposure or health risks. conclusion: requiring only two chemical-specific pieces of information, protex-ht enables efficient screening-level evaluations of existing and premanufacture chemicals in various exposure-and risk-based contexts.']"
54,emissions_materials_substances,0,24,54_aerosol_aerosols_scattering_atmospheric,"['aerosol', 'aerosols', 'scattering', 'atmospheric', 'dust', 'particles', 'cloud', 'optical', 'aerosol optical', 'radiative', 'measurements', 'avhrr', 'clouds', 'lidar', 'liquid', 'properties', 'optical depth', 'vertical', 'satellite', 'instrument', 'retrieval', 'wavelength', 'coefficients', 'hyperspectral', 'absorption', 'pm', 'ambient', 'flight', 'size', 'geostationary']","['this is fast: multivariate full-permutation based stochastic forest method—improving the retrieval of fine-mode aerosol microphysical properties with multi-wavelength lidar despite the small size and ubiquitous presence, fine-mode aerosols play a vital role in climate change and human health, especially in highly populated regions. hitherto, some studies have been carried out to retrieve fine-mode aerosol microphysical properties from multi-wavelength lidar measurements. however, there has been a dearth of lidar-based methods with quality retrieval and high time efficiency to adequately quantify the impacts of fine-mode aerosols on earths energy budget. the reasons for the gap involve the limitations in current approaches and the nature of the under-determined problem with insufficient information of three backscattering coefficients (β) and two extinction coefficients (α), typically known as the 3β + 2α configuration. furthermore, the latest lidars, especially for spaceborne and airborne applications, are inherently difficult to perform routine diurnal 3β + 2α observations with high resolution and require high processing speed for massive data. to overcome these conundrums, we developed a novel unsupervised machine learning method—multivariate full-permutation based stochastic forest method (dubbed the fast method)—to improve the retrieval of fine-mode aerosol microphysical properties. to the best of knowledge, this work is the first time that machine learning algorithms are employed in attempts to retrieve the aerosol microphysical properties with stand-alone multi-wavelength lidar data. the major merits of the fast method include 1) high accuracy of fine-mode aerosol products with the typical 3β + 2α configuration, 2) acceptable performances for fewer input optical channels, and 3) high processing speed for large volume data. comprehensive simulations have been conducted to investigate the error characteristic of the fast method under different conditions. we also applied the fast method to the airborne lidar data acquired during the nasa discover-aq field campaign. the retrievals of the fast method provide high resolution time-height distributions of fine-mode aerosol microphysical properties at 20-s temporal resolution and 45-m vertical resolution. in situ measurements are compared with multi-wavelength lidar retrievals showing good agreements. we achieved 0.010, 0.014 and 0.016 in terms of mean absolute difference for retrieved 532-nm single-scattering albedo with 3β + 2α, 3β + 1α, and 2β + 1α configurations, respectively. the proposed method is expected to represent an important step toward improving microphysical retrievals from multi-wavelength lidar data, especially for airborne and spaceborne lidar missions.', 'a novel method for calculating ambient aerosol liquid water content based on measurements of a humidified nephelometer system water condensed on ambient aerosol particles plays significant roles in atmospheric environment, atmospheric chemistry and climate. before now, no instruments were available for real-time monitoring of ambient aerosol liquid water contents (alwcs). in this paper, a novel method is proposed to calculate ambient alwc based on measurements of a three-wavelength humidified nephelometer system, which measures aerosol light scattering coefficients and backscattering coefficients at three wavelengths under dry state and different relative humidity (rh) conditions, providing measurements of light scattering enhancement factor f (rh). the proposed alwc calculation method includes two steps: the first step is the estimation of the dry state total volume concentration of ambient aerosol particles, va(dry), with a machine learning method called random forest model based on measurements of the dry nephelometer. the estimated va(dry) agrees well with the measured one. the second step is the estimation of the volume growth factor vg(rh) of ambient aerosol particles due to water uptake, using f (rh) and the ångström exponent. the alwc is calculated from the estimated va(dry) and vg(rh). to validate the new method, the ambient alwc calculated from measurements of the humidified nephelometer system during the gucheng campaign was compared with ambient alwc calculated from isorropia thermodynamic model using aerosol chemistry data. a good agreement was achieved, with a slope and intercept of 1.14 and -8.6 μm3 cm-3 (r2 =0.92), respectively. the advantage of this new method is that the ambient alwc can be obtained solely based on measurements of a three-wavelength humidified nephelometer system, facilitating the real-time monitoring of the ambient alwc and promoting the study of aerosol liquid water and its role in atmospheric chemistry, secondary aerosol formation and climate change.', 'a physical knowledge-based machine learning method for near-real-time dust aerosol properties retrieval from the himawari-8 satellite data monitoring dust aerosol properties is critical for the studies of radiative transfer budget, climate change, and air quality. aerosol optical thickness (aot) and effective radius (reff) are two main parameters describing the optical and microphysical properties of airborne dust aerosol. satellite remote sensing provides an opportunity for estimating the two parameters in spatial coverage and continuously. to take the merits of machine learning algorithms and also utilize the physical knowledge discovered in the conventional retrieval algorithms, a physical-based machine learning method was proposed and applied on the himawari-8 geostationary satellite for robust retrieval of dust aerosol properties. the main concepts of this study comprise i) constructing the model input data by extracting highly informative features from the himawari observations according to physical knowledge and ii) exploiting the utility of six state-of-the-art machine learning algorithms in dust aerosol retrieval. the algorithms include artificial neural network (ann), extreme boost gradient tree (xgboost), extra tree (et), random forest (rf), support vector regression (svr), and kernel ridge regression (ridge). the ground-truth aot and reff data from aeronet stations were supplied as output labels. the cross-validation technique was adopted for model training and the results show that the ann model is superior to the other machine learning models for both aot and reff estimation, which exhibits the lowest mean absolute error (mae = 0.0292 and 0.0981) and the highest correlation coefficient (r = 0.98 and 0.84). when validated on an independent dataset, the ann model achieved the lowest mae (0.0334 and 0.1487), and the highest r (0.94 and 0.63). more importantly, when compared against representative physical-based algorithms, the developed ann model still retains the best performance. furthermore, the ann model shows an overall better performance than other machine learning models and also the jaxa himawari-8 level-2 aot product, with examples exhibited in three dust storm events and for continuous monitoring of one of the dust storm events. additionally, feature importance analysis implies that the important features of dust aerosol identified by the ann model are consistent with that in physical model-based algorithms. in summary, this study shows great potential for generating near-real-time products of dust aerosol properties from himawari satellite data. these products can provide a scientific basis for climate and meteorological study regarding severe dust storms.']"
58,emissions_materials_substances,0,32,58_materials_quantum_chemical_discovery,"['materials', 'quantum', 'chemical', 'discovery', 'molecular', 'synthesis', 'reaction', 'inf', 'material', 'calculations', 'structures', 'spherical', 'hydrogen', 'properties', 'catalysts', 'active learning', 'self driving', 'accelerated', 'catalyst', 'defect', 'inf inf', 'new', 'active', 'process', 'functional', 'design', 'energy', 'search', 'laboratories', 'efficient']","['active learning accelerated discovery of stable iridium oxide polymorphs for the oxygen evolution reaction the discovery of high-performing and stable materials for sustainable energy applications is a pressing goal in catalysis and materials science. understanding the relationship between a materials structure and functionality is an important step in the process, such that viable polymorphs for a given chemical composition need to be identified. machine-learning-based surrogate models have the potential to accelerate the search for polymorphs that target specific applications. herein, we report a readily generalizable active-learning (al) accelerated algorithm for identification of electrochemically stable iridium oxide polymorphs of iro2 and iro3. the search is coupled to a subsequent analysis of the electrochemical stability of the discovered structures for the acidic oxygen evolution reaction (oer). structural candidates are generated by identifying all 956 structurally unique ab2 and ab3 prototypes in existing materials databases (more than 38000). next, using an active learning approach, we find 196 iro2 polymorphs within the thermodynamic amorphous synthesizability limit and reaffirm the global stability of the rutile structure. we find 75 synthesizable iro3 polymorphs and report a previously unknown fef3-type structure as the most stable, termed α-iro3. to test the algorithms performance, we compare to a random search of the candidate space and report at least a 2-fold increase in the rate of discovery. additionally, the al approach can acquire the most stable polymorphs of iro2 and iro3 with fewer than 30 density functional theory optimizations. analysis of the structural properties of the discovered polymorphs reveals that octahedral local coordination environments are preferred for nearly all low-energy structures. subsequent pourbaix ir-h2o analysis shows that α-iro3 is the globally stable solid phase under acidic oer conditions and supersedes the stability of rutile iro2. calculation of theoretical oer surface activities reveal ideal weaker binding of the oer intermediates on α-iro3 than on any other considered iridium oxide. we emphasize that the proposed al algorithm can be easily generalized to search for any binary metal oxide structure with a defined stoichiometry.', 'a design-to-device pipeline for data-driven materials discovery conspectusthe world needs new materials to stimulate the chemical industry in key sectors of our economy: environment and sustainability, information storage, optical telecommunications, and catalysis. yet, nearly all functional materials are still discovered by trial-and-error, of which the lack of predictability affords a major materials bottleneck to technological innovation. the average molecule-to-market lead time for materials discovery is currently 20 years. this is far too long for industrial needs, as highlighted by the materials genome initiative, which has ambitious targets of up to 4-fold reductions in average molecule-to-market lead times. such a large step change in progress can only be realistically achieved if one adopts an entirely new approach to materials discovery. fortunately, a fundamentally new approach to materials discovery has been emerging, whereby data science with artificial intelligence offers a prospective solution to speed up these average molecule-to-market lead times.this approach is known as data-driven materials discovery. its broad prospects have only recently become a reality, given the timely and major advances in big data, artificial intelligence, and high-performance computing (hpc). access to massive data sets has been stimulated by government-regulated open-access requirements for data and literature. natural-language processing (nlp) and machine-learning (ml) tools that can mine data and find patterns therein are becoming mainstream. exascale hpc capabilities that can aid data mining and pattern recognition and also generate their own data from calculations are now within our grasp. these timely advances present an ideal opportunity to develop data-driven materials-discovery strategies to systematically design and predict new chemicals for a given device application.this account shows how data science can afford materials discovery via a four-step design-to-device pipeline that entails (1) data extraction, (2) data enrichment, (3) material prediction, and (4) experimental validation. massive databases of cognate chemical and property information are first forged from chemistry-aware natural-language-processing tools, such as chemdataextractor, and enriched using machine-learning methods and high-throughput quantum-chemical calculations. new materials for a bespoke application can then be predicted by mining these databases with algorithmic encodings of relationships between chemical structures and physical properties that are known to deliver functional materials. these may take the form of classification, enumeration, or machine-learning algorithms. a data-mining workflow short-lists these predictions to a handful of lead candidate materials that go forward to experimental validation. this design-to-device approach is being developed to offer a roadmap for the accelerated discovery of new chemicals for functional applications. case studies presented demonstrate its utility for photovoltaic, optical, and catalytic applications. while this account is focused on applications in the physical sciences, the generic pipeline discussed is readily transferable to other scientific disciplines such as biology and medicine.', 'autonomous chemical experiments: challenges and perspectives on establishing a self-driving lab conspectuswe must accelerate the pace at which we make technological advancements to address climate change and disease risks worldwide. this swifter pace of discovery requires faster research and development cycles enabled by better integration between hypothesis generation, design, experimentation, and data analysis. typical research cycles take months to years. however, data-driven automated laboratories, or self-driving laboratories, can significantly accelerate molecular and materials discovery. recently, substantial advancements have been made in the areas of machine learning and optimization algorithms that have allowed researchers to extract valuable knowledge from multidimensional data sets. machine learning models can be trained on large data sets from the literature or databases, but their performance can often be hampered by a lack of negative results or metadata. in contrast, data generated by self-driving laboratories can be information-rich, containing precise details of the experimental conditions and metadata. consequently, much larger amounts of high-quality data are gathered in self-driving laboratories. when placed in open repositories, this data can be used by the research community to reproduce experiments, for more in-depth analysis, or as the basis for further investigation. accordingly, high-quality open data sets will increase the accessibility and reproducibility of science, which is sorely needed.in this account, we describe our efforts to build a self-driving lab for the development of a new class of materials: organic semiconductor lasers (osls). since they have only recently been demonstrated, little is known about the molecular and material design rules for thin-film, electrically-pumped osl devices as compared to other technologies such as organic light-emitting diodes or organic photovoltaics. to realize high-performing osl materials, we are developing a flexible system for automated synthesis via iterative suzuki-miyaura cross-coupling reactions. this automated synthesis platform is directly coupled to the analysis and purification capabilities. subsequently, the molecules of interest can be transferred to an optical characterization setup. we are currently limited to optical measurements of the osl molecules in solution. however, material properties are ultimately most important in the solid state (e.g., as a thin-film device). to that end and for a different scientific goal, we are developing a self-driving lab for inorganic thin-film materials focused on the oxygen evolution reaction.while the future of self-driving laboratories is very promising, numerous challenges still need to be overcome. these challenges can be split into cognition and motor function. generally, the cognitive challenges are related to optimization with constraints or unexpected outcomes for which general algorithmic solutions have yet to be developed. a more practical challenge that could be resolved in the near future is that of software control and integration because few instrument manufacturers design their products with self-driving laboratories in mind. challenges in motor function are largely related to handling heterogeneous systems, such as dispensing solids or performing extractions. as a result, it is critical to understand that adapting experimental procedures that were designed for human experimenters is not as simple as transferring those same actions to an automated system, and there may be more efficient ways to achieve the same goal in an automated fashion. accordingly, for self-driving laboratories, we need to carefully rethink the translation of manual experimental protocols.']"
63,emissions_materials_substances,0,43,63_co2_capture_oil_storage,"['co2', 'capture', 'oil', 'storage', 'gas', 'injection', 'geothermal', 'inf', 'wells', 'co2 capture', 'carbon', 'oil gas', 'shale', 'petroleum', 'carbon dioxide', 'dioxide', 'geological', 'reservoir', 'trapping', 'saline', 'inf inf', 'process', 'pressure', 'ccs', 'aquifers', 'formations', 'carbon capture', 'production', 'hydrocarbon', 'fluid']","['assessment of co<inf>2</inf> storage as hydrates in saline aquifers using machine learning algorithms global warming is one of the most serious issues the world is currently facing. the major reason is attributed to emission of greenhouse gases and in particular carbon dioxide, co2. the most promising methods that could allow significant reduction in co2 emissions are capture and geological storage of co2. one major concern against storage of co2 is the possibility of its leakage. one process that could lead to more reliable trapping of co2 is hydrate formation – that leads to trapping of co2 in the solid form. in this study, machine learning algorithms and reservoir simulation software were used to conduct sensitivity studies on some of the main reservoir parameters, to understand which characteristics have most impact on stability of co2storage in the form of hydrates. the hydrate formation curve calculated by hydraflash software was considered to be a benchmark for experiments conducted in this study.', 'application of robust intelligent schemes for accurate modelling interfacial tension of co<inf>2</inf> brine systems: implications for structural co<inf>2</inf> trapping given the current global climate change, renewable energy sources, carbon capture, utilization, and storage (ccus) are being considered as a potential solutions to this critical global issue. structural and capillary processes can be used to store carbon dioxide (co2) in deep saline aquifers in a way that is safe and does not harm the environment. due to this fact, the interfacial tension (ift) of the co2-brine system is an important factor influencing the capacity of storage formations to sequester co2. as a result, ift is essential for conducting a thorough and accurate evaluation in order to determine the optimal strategy for co2 storage projects. this paper used intelligent models such as gaussian process regression (gpr), radial basis function (rbf), and random forest (rf) to forecast ift in the co2-brine system with high precision and substantial time saving. the results reveal that the constructed rf model could deliver excellent performance in predicting ift with the lowest average absolute percent relative error (aapre = 1.9156 percent), highest coefficient of determination (r2 = 0.9907), and lowest root mean squared error (rmse = 0.7279). furthermore, a sensitivity analysis was performed to ascertain the most critical parameters in the rf model to be considered. the parametric analysis found that both pure and non-pure co2 systems had a significant impact on ift prediction. also, the rf model was used to assess the structural trapping capacity of a storage location in the cuu long basin. the estimation results obtained in this study agreed perfectly with the previous ones. the findings of this study can aid in a better understanding of how machine learning models can be applied to predict ift values for the evaluation of the structural co2 storage capacity.', 'modeling co<inf>2</inf> geologic storage using machine learning over the upcoming years, storing co2 into geological formations would contribute significantly to the international efforts to address climate challenges due to greenhouse gas emissions. carbon capture and storage (ccs) projects require immense capital investments to complete multiple phases, namely careful site selection, planning, design, and execution. modeling of surface and subsurface co2 flow plays a major role not only in design optimization but also in site screening and capacity estimation. this study focuses on modeling multiphase flow of co2 in underground formations with particular emphasis on the fraction of the co2 injected that can be trapped. key interest is given to a trapping mechanism that can keep co2 stored for long-term in target formations, namely residual trapping. the main objective of this work is to find more efficient ways to proxy model this process with its complex physics. there have been multiple recent reservoir simulator numerical enhancements to model co2 trapping in ccs accurately. however, these complex enhancements have created computational difficulties when attempting to capture unique co2 fluid physical and chemical subsurface processes such as relative permeability hysteresis. such numerical challenges make the modeling inefficient and computationally expensive. therefore, this study introduces more-efficient modeling techniques based on machine learning to make simulations more practical and accessible. by generating a sufficiently large training dataset utilizing a computationally-enhanced numerical simulator with a wide range of input parameters including permeability, porosity, etc., a machine learning model was constructed as an alternative to conventional numerical simulation. the effectiveness of the machine-learning models is presented using a test case of a 2d rectangular grid domain of heterogeneous permeability representing a saline aquifer. the goal is to model an injection of co2 into water under gravity segregation to estimate the fraction of co2 trapped at the bottom prevented from reaching the top. even though the training dataset used in this study is relatively small, the machine-learning alternative is able to achieve at least 95% accuracy when tested with new input data in 103 to 104 faster run-times. it is believed that this accuracy can be improved further by increasing the size of the training dataset and exploring other machine-learning models with new hyperparameters. in this study, only a limited number of widely-used techniques is compared, including: random forest, k-nearest neighbors (knn), and multi-output regression. accurately modeling the amount of co2 that can be trapped during ccs applications is vital as this will dictate the available storage capacity for injected co2; however, this may be a difficult task for most commercial simulators. this study proposes new ways to model such process with enhanced efficiency compared to existing techniques. as most global efforts are adopting an accelerated strategy towards sustainability, the presented approach is timely and of great importance.']"
64,emissions_materials_substances,0,28,64_co2_adsorption_capture_inf,"['co2', 'adsorption', 'capture', 'inf', 'ch4', 'solubility', 'porous', 'separation', 'inf inf', 'molecular', 'materials', 'carbon capture', 'carbon', 'o2', 'properties', 'co2 ch4', 'metal', 'organic', 'process', 'parameters', 'gas', 'carbon dioxide', 'dioxide', 'experimental', 'catalyst', 'co2 capture', 'frameworks', 'capacity', 'pore', 'fossil']","['accurate prediction of carbon dioxide capture by deep eutectic solvents using quantum chemistry and a neural network carbon dioxide (co2) emissions from fossil fuel combustion are a significant source of greenhouse gas, contributing in a major way to global warming and climate change. carbon dioxide capture and sequestration is gaining much attention as a potential method for controlling these greenhouse gas emissions. among the environmentally friendly solvents, deep eutectic solvents (dess) have demonstrated the potential capability for carbon capture. to establish a theoretical framework for des activity, thermodynamics modeling and solubility predictions are significant factors to anticipate and understand the system behavior. here, we combine the cosmo-rs model with machine learning techniques to predict the solubility of co2 in various deep eutectic solvents. a comprehensive data set was established comprising 1973 co2 solubility data points in 132 different dess at a variety of temperatures, pressures, and des molar ratios. this data set was then utilized for the further verification and development of the cosmo-rs model. the co2 solubility (ln(xco2)) in dess calculated with the cosmo-rs model differs significantly from the experiment with an average absolute relative deviation (aard) of 23.4%. a multilinear regression model was developed using the cosmo-rs predicted solubility and a temperature-pressure dependent parameter, which improved the aard to 12%. finally, a machine learning model using cosmo-rs-derived features was developed based on an artificial neural network algorithm. the results are in excellent agreement with the experimental co2 solubilities, with an aard of only 2.72%. the ml model will be a potentially useful tool for the design and selection of dess for co2 capture and utilization.', 'modeling of co<inf>2</inf> capture ability of [bmim][bf<inf>4</inf>] ionic liquid using connectionist smart paradigms the burning of fossil fuels produces large amounts of exhaust gases containing carbon dioxide (co2). the emission of co2 into the atmosphere is widely known as the leading cause of global warming and climate change. the separation processes are responsible for capturing the co2 to reduce its undesirable effects on the environment. since the conventional processes have their drawbacks, it is crucial to find a more environment-friendly process for co2 capture. recently, ionic liquids (ils) have become an interesting candidate for co2 capture. in this study, the solubility of co2 in the 1-n-butyl-3-methylimidazolium tetrafluoroborate ([bmim][bf4]) is estimated using six different artificial intelligence (ai) techniques, including four artificial neural networks (ann), support vector machines (ls-svm), adaptive neuro-fuzzy interface system (anfis). the cascade feed-forward neural network has been found as the best model for the considered matter. this model predicts overall experimental datasets with excellent accuracy of aard = 6.88%, mse=8×10−4, and r2=0.98808. the maximum mole fraction of co2 in the ionic liquid (i.e., 0.8) can be obtained at the highest pressure and the lowest temperature.', 'improving co<inf>2</inf> absorption using artificial intelligence and modern optimization for a sustainable environment one of the essential factors in maintaining environmental sustainability is to reduce the harmful effects of carbon dioxide (co2) emissions. this can be performed either by reducing the emissions themselves or capturing and storing the emitted co2. this work studies the solubility of carbon dioxide in the capturing solvent, which plays a crucial role in the effectiveness and cost-efficiency of carbon capture and storage (ccs). therefore, the study aims to enhance the solubility of co2 by integrating artificial intelligence (ai) and modern optimization. accordingly, this study consists of two consecutive stages. in the first stage, an adaptive neuro-fuzzy inference system (anfis) model as an ai tool was developed based on experimental data. the mol fraction was targeted as the model’s output in terms of three operating parameters; the concentration of tetrabutylphosphonium methanesulfonate [tbp][meso3], temperature, and pressure of co2. the operating ranges are (2–20 wt%), (30–60 °c), and (2–30 bar), respectively. based on the statistical measures of the root mean squared error (rmse) and the predicted r2, the anfis model outperforms the traditional analysis of variance (anova) modeling technique, where the resulting values were found to be 0.126 and 0.9758 for the entire samples, respectively. in the second stage, an improved grey wolf optimizer (igwo) was utilized to determine the optimal operating parameters that increase the solubility of co2. the optimal values of the three operating parameters that improve the co2 solubility were found to be 3.0933 wt%, 40.5 °c, and 30 bar, respectively. with these optimal values, the collaboration between the anfis and igwo produced an increase of 13.4% in the mol fraction compared to the experimental data and the response surface methodology. to demonstrate the efficacy of igwo, the obtained results were compared to the results of four competitive optimization techniques. the comparison showed that the igwo demonstrates superior performance. overall, this study provided a cost-efficient approach based on ai and modern optimization to enhance co2 solubility in ccs.']"
65,emissions_materials_substances,0,53,65_air_pollution_air pollution_air quality,"['air', 'pollution', 'air pollution', 'air quality', 'lstm', 'ozone', 'quality', 'pollutants', 'deep', 'deep learning', 'term memory', 'long short', 'memory', 'short term', 'pm', 'pm2', 'short', 'prediction', 'neural', 'concentration', 'monitoring', 'term', 'particulate matter', 'long', 'particulate', 'forecasting', 'network', 'rnn', 'health', 'proposed']","['air quality prediction using cnn+lstm-based hybrid deep learning architecture air pollution prediction based on variables in environmental monitoring data gains further importance with increasing concerns about climate change and the sustainability of cities. modeling of the complex relationships between these variables by sophisticated methods in machine learning is a promising field. the objectives of this work are to develop a supervised model for the prediction of air pollution by using real sensor data and to transfer the model between cities. the combination of a convolutional neural network and a long short-term memory deep neural network model was proposed to predict the concentration of air pollutants in multiple locations of a city by using spatial-temporal relationships. two approaches have been adopted: the univariate model contains the information of one pollutant whereas the multivariate model contains the information of all pollutants and meteorology data for prediction. the study was carried out for different pollutants which are in the publicly available data of the cities of barcelona, kocaeli, and i̇stanbul. the hyperparameters of the model (filter, frame, and batch sizes; number of convolutional/lstm layers and hidden units; learning rate; and parameters for sample selection, pooling, and validation) were tuned to determine the architecture that achieved the lowest test error. the proposed model improved the prediction performance (measured by the root mean square error) by 11–53% for particulate matter, 20–31% for ozone, 9–47% for nitrogenoxides, and 18–46% for sulfurdioxide with respect to the 1-hidden layer long short-term memory networks utilized in the literature. the multivariate model without using meteorological data revealed the best results. regarding transfer learning, the network weights were transferred from the source city to the target city. the model has more accurate prediction performance with the transfer of the network from kocaeli to i̇stanbul as those neighbor cities have similar air pollution and meteorological characteristics.', 'multi-site air pollutant prediction using long short term memory the current pandemic highlights the significance and impact of air pollution on individuals. when it comes to climate sustainability, air pollution is a major challenge. because of the distinctive nature, unpredictability, and great changeability in the reality of toxins and particulates, detecting air quality is a puzzling task. simultaneously, the ability to predict or classify and monitor air quality is becoming increasingly important, particularly in urban areas, due to the well documented negative impact of air pollution on resident’s health and the environment. to better comprehend the current condition of air quality, this research proposes predicting air pollution levels from real-time data. this study proposes the use of deep learning techniques to forecast air pollution levels. layers, activation functions, and a number of epochs were used to create the suggested long short-term memory (lstm) network based neural layer design. the use of proposed deep learning as a structure for high-accuracy air quality prediction is investigated in this research and obtained better accuracy of nearly 82% compared to earlier records. determining the air quality index (aqi) and danger levels would assist the government in finding appropriate ways to authorize approaches to reduce pollutants and keep inhabitants informed about the findings.', 'air pollution prediction using lstm deep learning and metaheuristics algorithms air pollution is a leading cause of health concerns and climate change, one of humanitys most dangerous problems. this problem has been exacerbated by an overabundance of automobiles, industrial output pollution, transportation fuel consumption, and energy generation. as a result, air pollution forecasting has become vital. as a result of the large amount and variety of data acquired by air pollution monitoring stations, air pollution forecasting has become a popular topic, particularly when applying deep learning models of long short-term memory (lstm). the ability of these models to learn long-term dependencies in air pollution data sets them apart. however, lstm models using many other statistical and machine learning approaches may not offer adequate prediction results due to noisy data and improper hyperparameter settings. as a result, to define the pollution levels for a group of contaminants, an ideal representation of the lstm is required. to address the problem of identifying the best hyperparameters for the lstm model, in this paper, we propose a model based on the genetic algorithm (ga) algorithm as well as the long short-term memory (lstm) deep learning algorithm. the model aims to find the best hyperparameters for lstm and the pollution level for the next day using four types of pollutants pm10, pm2.5, co, and nox. the proposed model modified by optimization algorithms shows more accurate results with less experience and more speed than machine learning models and lstm models.']"
66,emissions_materials_substances,0,124,66_air_air quality_pollution_pm2,"['air', 'air quality', 'pollution', 'pm2', 'air pollution', 'ozone', 'concentrations', 'quality', 'no2', 'pollutants', 'concentration', 'health', 'china', 'pollutant', 'meteorological', 'pm', 'emission', 'particulate', 'inf', 'particulate matter', 'emissions', 'meteorology', 'traffic', 'air pollutants', 'atmospheric', 'matter', 'urban', 'lockdown', 'sources', 'exposure']","['a machine learning-based study on the impact of covid-19 on three kinds of pollution in beijing-tianjin-hebei region large-scale restrictions on anthropogenic activities in china in 2020 due to the corona virus disease 2019 (covid-19) indirectly led to improvements in air quality. previous studies have paid little attention to the changes in nitrogen dioxide (no2), fine particulate matter (pm2.5) and ozone (o3) concentrations at different levels of anthropogenic activity limitation and their interactions. in this study, machine learning models were used to simulate the concentrations of three pollutants during periods of different levels of lockdown, and compare them with observations during the same period. the results show that the difference between the simulated and observed values of no2 concentrations varies at different stages of the lockdown. variation between simulated and observed o3 and pm2.5 concentrations were less distinct at different stages of lockdowns. during the most severe period of the lockdowns, no2 concentrations decreased significantly with a maximum decrease of 65.28 %, and o3 concentrations increased with a maximum increase of 75.69 %. during the first two weeks of the lockdown, the titration reaction in the atmosphere was disrupted due to the rapid decrease in no2 concentrations, leading to the redistribution of ox (no2 + o3) in the atmosphere and eventually to the production of o3 and secondary pm2.5. the effect of traffic restrictions on the reduction of no2 concentrations is significant. however, it is also important to consider the increase in o3 due to the constant volatile organic compounds (vocs) and the decrease in nox (no+no2). traffic restrictions had a limited effect on improving pm2.5 pollution, so other beneficial measures were needed to sustainably reduce particulate matter pollution. research on covid-19 could provide new insights into future clean air action.', 'impact of climate change and air pollution forecasting using machine learning techniques in bishkek during recent years, severe air-pollution problems have garnered worldwide attention due to their effects on human health and the environment. air pollution in bishkek, kyrgyz republic, is an ever-increasing problem with little research conducted on the impact of air pollutants on public health. we evaluate the performance of several machine learning algorithms applied to air quality and meteorology datasets and compare prediction accuracies of bishkek air quality given its significant public importance. data on 16 synoptic atmospheric process were collected by kyrgyzhydromet from 2016 to 2018 and used to train and build a forecasting model. the model was then tested using data collected in 2020. climate change in bishkek and the impact on air pollution was assessed via the frequency of days characterized by daytime temperature inversions and air stagnation. atmospheric stability increased from 2015 to 2020 with ongoing climate change leading to more temperature inversions. about 80%–90% of days with temperature inversions are associated with winter heating seasons and these numbers increased two-fold during the past 5 years. the impact of lockdown during covid-19 (22 march–11 may 2020) on air quality in bishkek is also shown. during the lockdown period, co, no, no2, so2, and pm2.5 decreased by 64%, 1.5%, 75%, 24%, and 54%, respectively, compared to concentrations of these pollutants in 2019. where identified, emissions from vehicles make up a significant part of the air pollution.', 'artificial intelligence-assisted air quality monitoring for smart city management background. the environment has been significantly impacted by rapid urbanization, leading to a need for changes in climate change and pollution indicators. the 4ir offers a potential solution to efficiently manage these impacts. smart city ecosystems can provide well-designed, sustainable, and safe cities that enable holistic climate change and global warming solutions through various community-centred initiatives. these include smart planning techniques, smart environment monitoring, and smart governance. an air quality intelligence platform, which operates as a complete measurement site for monitoring and governing air quality, has shown promising results in providing actionable insights. this article aims to highlight the potential of machine learning models in predicting air quality, providing data-driven strategic and sustainable solutions for smart cities. methods. this study proposed an end-to-end air quality predictive model for smart city applications, utilizing four machine learning techniques and two deep learning techniques. these include ada boost, svr, rf, knn, mlp regressor and lstm. the study was conducted in four different urban cities in selangor, malaysia, including petaling jaya, banting, klang, and shah alam. the model considered the air quality data of various pollution markers such as pm2.5, pm10, o3, and co. additionally, meteorological data including wind speed and wind direction were also considered, and their interactions with the pollutant markers were quantified. the study aimed to determine the correlation variance of the dependent variable in predicting air pollution and proposed a feature optimization process to reduce dimensionality and remove irrelevant features to enhance the prediction of pm2.5, improving the existing lstm model. the study estimates the concentration of pollutants in the air based on training and highlights the contribution of feature optimization in air quality predictions through feature dimension reductions. results. in this section, the results of predicting the concentration of pollutants (pm2.5, pm10, o3, and co) in the air are presented in r2 and rmse. in predicting the pm10 and pm2.5 concentration, lstm performed the best overall high r2 values in the four study areas with the r2 values of 0.998, 0.995, 0.918, and 0.993 in banting, petaling, klang and shah alam stations, respectively. the study indicated that among the studied pollution markers, pm2.5, pm10, no2, wind speed and humidity are the most important elements to monitor. by reducing the number of features used in the model the proposed feature optimization process can make the model more interpretable and provide insights into the most critical factor affecting air quality. findings from this study can aid policymakers in understanding the underlying causes of air pollution and develop more effective smart strategies for reducing pollution levels.']"
71,emissions_materials_substances,0,18,71_waste_solid waste_solid_waste management,"['waste', 'solid waste', 'solid', 'waste management', 'municipal solid', 'landfill', 'generation', 'municipal', 'xgboost', 'wastewater', 'landfills', 'wwtps', 'waste generation', 'management', 'treatment', 'quantity', 'ghg', 'regression', 'end life', 'disposal', 'cities', 'compositions', 'collection', 'socioeconomic', 'gas', 'dioxide concentration', 'sanitary', 'wastewater treatment', 'emission', 'error']","['machine learning based prediction for chinas municipal solid waste under the shared socioeconomic pathways reliable forecast of municipal solid waste (msw) generation is crucial for sustainable and efficient waste management. big data analysis is a novel method to forecast msw more accurately. thus, this study employs five kinds of supervised machine learning approaches including linear regression, polynomial regression, support vector machine, random forest, and extreme gradient boosting (xgboost) to examine their forecast performances. chinas msw generation from 2020 to 2060 under five shared socioeconomic pathways (ssps) is further predicted and the mechanisms between msw generation and socioeconomic features are explored. results show that population and gdp are two dominant indicators in msw prediction, and xgboost model is proved to be effective in msw forecast. msw generation of china in 2060 is estimated to be 464–688 megatons under different ssps scenarios, about four to six times of that in 2000. ssp3 that has the most population, least gdp and the highest climate change challenges is the only scenario showing a potential of msw peak during the study period. the key for msw increase is mainly the increase of per capita msw caused by gdp. finally, several policy recommendations are raised to reduce the overall msw generation.', 'predicting the quantity of municipal solid waste using xgboost model the quantity of municipal solid waste (msw) gets intensified, based on various factors such as population growth, monetary status and consumption patterns. the insufficiency of elementary trash data is a critical problem for managing the msw. in this study, the goal is to forecast the msw generation of northern ireland. a precise model was developed to estimate the total amount of waste produced for every quarter of the year, by employing the machine learning techniques. the seasonal arima (s-arima) and extreme gradient boosting (xgboost) models were employed to estimate the amount of waste produced. on comparing both the models, xgboost performed better. thus, the parameters of the xgboost were tuned to yield the optimal outcome. the xgboost with the tuned hyperparameters achieved an optimum result with the higher coefficient of determination (r2) value as 0.5325 and lower rmse value of 13215.97. the prediction of the msw weight would help the decision-makers in treating and disposing solid waste appropriately.', 'forecasting msw generation using artificial neural network time series model: a study from metropolitan city forecasting the quantity of municipal solid waste generation is an essential task for sustainable solid waste management and strategy implementation. the estimation of future waste generation rates can help to motivate for analyzing gaps in existing waste management and better planning strategies. improper management and unsafe disposal of solid waste create a threat to the environment and human health. hence, a sound forecasting of solid waste generation is very crucial for planning and management accordingly. artificial intelligence is an excellent and new application of soft computing which is used as a forecasting tool. the main objective of this study is to apply ann time series model along with autoregressive technique to forecast the monthly solid waste generation in kolkata. for the same, data related to the monthly solid waste generation was gathered from 2010 to 2017. total data of 96\xa0months were divided into three categories, i.e., 70%, 15%, and 15% for training, validation, and testing, respectively. the model was evaluated based on performance value of mean square error, root mean square error, and regression coefficient. the ann structure of 1-19-1 was considered as optimized model for solid waste forecasting because it has the lowest mean square error and the highest regression coefficient. the applied time series model forecasts that kolkata will generate about 5205\xa0mt/day municipal solid waste in 2030 which will add more than 1000\xa0mt/day waste with the existing rate of generation. the present study helps in estimating and allocating essential resources that need in future for sound solid waste management and preparing alternative strategies to reach the sustainable goals.']"
72,emissions_materials_substances,0,204,72_emissions_carbon_emission_economic,"['emissions', 'carbon', 'emission', 'economic', 'co2', 'carbon emissions', 'co2 emissions', 'energy', 'carbon emission', 'reduction', 'consumption', 'ghg', 'development', 'economic growth', 'dioxide', 'price', 'countries', 'growth', 'china', 'green', 'environmental', 'carbon dioxide', 'forecasting', 'gas', 'dioxide emissions', 'global', 'ghg emissions', 'economy', 'renewable', 'greenhouse']","['study on the method of carbon emission modelling in china based on machine learning climate change and carbon dioxide emission reduction have attracted worldwide attention. however, the existing carbon emission calculation methods and standards are different, and it is difficult to accurately calculate the carbon emission. this paper uses the algorithm of machine learning to fit and calculate carbon emission. starting from the current carbon emissions situation, we analyze the driving factors of carbon emissions in energy structure and industrial structure. based on the data, we formulate fourteen impact indicators and construct a multi-layer feedforward bp neural network. the training data is 14 processed indicator data, and the comparison data is the real carbon emissions data in china. through the convergence and simulation of neurons, a function curve that is relatively close to the real carbon emissions is constructed. by comparing $r^{2}$ and mse indicators, bp neural network has better effect compared with other fitting algorithms.', 'driving factors of co<inf>2</inf> emissions: further study based on machine learning greenhouse gases, especially carbon dioxide (co2) emissions, are viewed as one of the core causes of climate change, and it has become one of the most important environmental problems in the world. this paper attempts to investigate the relation between co2 emissions and economic growth, industry structure, urbanization, research and development (r&d) investment, actual use of foreign capital, and growth rate of energy consumption in china between 2000 and 2018. this study is important for china as it has pledged to peak its carbon dioxide emissions (co2) by 2030 and achieve carbon neutrality by 2060. we apply a suite of machine learning algorithms on the training set of data, 2000–2015, and predict the levels of co2 emissions for the testing set, 2016–2018. employing rmse for model selection, results show that the nonlinear model of k-nearest neighbors (knn) model performs the best among linear models, nonlinear models, ensemble models, and artificial neural networks for the present dataset. using knn model, sensitivity analysis of co2 emissions around its centroid position was conducted. the findings indicate that not all provinces should develop its industrialization. some provinces should stay at relatively mild industrialization stage while selected others should develop theirs as quickly as possible. it is because co2 emissions will eventually decrease after saturation point. in terms of urbanization, there is an optimal range for a province. at the optimal range, the co2 emissions would be at a minimum, and it is likely a result of technological innovation in energy usage and efficiency. moreover, china should increase its r&d investment intensity from the present level as it will decrease co2 emissions. if r&d reinvestment is associated with actual use of foreign capital, policy makers should prioritize the use of foreign capital for r&d investment on green technology. last, economic growth requires consuming energy. however, policy makers must refrain from consuming energy beyond a certain optimal growth rate. the above findings provide a guide to policy makers to achieve dual-carbon strategy while sustaining economic development.', 'the estimation of the carbon dioxide emission and driving factors in china based on machine learning methods global warming can be reduced and the ecological environment can be enhanced by reducing carbon dioxide emissions. therefore, it is imperative to determine how to calculate urban carbon dioxide emissions. besides, as the country with the largest carbon dioxide emissions, exploring the influencing factors of carbon dioxide emissions is conducive to providing support for emission reduction actions. in the first place, the paper makes use of chinas provincial carbon dioxide emissions data and nighttime light data to build an inversion model from 2000 to 2019 that calculates carbon emission data for prefecture-level cities. furthermore, this paper employs machine learning methods such as decision tree and random forest to determine the factors affecting carbon dioxide emissions. the main conclusion is that carbon dioxide emissions are highest in the eastern regions with higher economic development. additionally, cities which are dependent on resources to develop have higher carbon dioxide emissions and a rising trend. factors such as gross domestic production, financial general budget revenue and foreign investment can influence carbon dioxide emissions. according to the random forest results, the feature importance of gdp, financial general budget revenue and foreign investment is 0.45, 0.12 and 0.08, respectively. accordingly, different regions cannot ignore carbon dioxide emissions when developing their economies. as the growth rate of emissions has slowed in recent years in part due to government policies, chinas ongoing implementation of low-carbon transformation must continue to be implemented, including low-carbon city pilot programs, carbon trading markets, etc. as well, areas with serious carbon dioxide emissions, such as shanghai, tianjin, and chongqing, should be prioritized as areas for low-carbon economic development.']"
12,risk_management,1,44,12_flood_images_sar_sentinel,"['flood', 'images', 'sar', 'sentinel', 'detection', 'flooding', 'damage', 'segmentation', 'flooded', 'disaster', 'disasters', 'flood events', 'flood detection', 'crack', 'radar', 'sar images', 'events', 'floods', 'water', 'deep', 'deep learning', 'natural disasters', 'areas', 'synthetic aperture', 'aperture radar', 'aperture', 'dataset', 'mapping', 'natural', 'image']","['flood event detection from sentinel 1 and sentinel 2 data: does land use matter for performance of u-net based flood segmenters? floods are among the most costly weather hazards for societies and businesses globally. with increasing global warming, these events have become even more frequent and more devastating. thus, accurate flood mapping has become critical for disaster relief, risk management and mitigation. current flood segmentation methods use either threshold-based approaches or deep-learning schemes, e.g. using the u-net architecture, to differentiate between water-covered bodies or dry land on earth observation images. many schemes are exploiting imagery from synthetic aperture radar (e.g. sentinel 1 satellites) or visual bands of satellites such as the sentinel 2, but often restrict themselves to using one or very few modalities, i.e. spectral wavelengths, despite the availability of many more wavelengths or pre-processed indices with potential value to the challenge. in support of operationalizing flood segmentation on a global scale using deep learning, we propose semantic flood segmentation exploiting optionally many different modalities (i.e. multimodal flood segmentation), making the approach largely immune to geographic differences across the globe. using u-net at the core of our work, we observe very good generalisation of our segmentation model to unseen flood events in our holdout set at the level of 0.95 f1 score (0.92 iou) for both no water and water class, and 0.53 f1 score (0.43 iou) for water class, respectively.', 'flood area detection using sar images with deep neural network during, 2020 kyushu flood japan disasters are on the rise and with global warming, the frequency of floods has seen an exponential increase. flood area detection becomes necessary to prioritize the focus area for reducing the response time and saving more lives. as flood inundates larger area (several hundreds of square-km), remote sensing becomes important to monitor this affected area. however, flood and bad weather are in general occurs simultaneously, this restricts us to use optical images and thereby synthetic aperture radar (sar) images become the obvious choice. in the recent kyushu flood in japan, which occurred due to concentrated rain in the first week of july 2020, led to the flooding of kuma river and its tributaries. sar images of previous flood events in japan were utilized as training data. due to the limited training data and binary class (flooded vs non-flooded area) segmentation problem, this study chooses u-net, as it has shown better results in the above-mentioned situation. in this study, u-net with alternate 5x5 convolution to 3x3 convolution in the encoder part has been used to grasp more contextual information. the result has been compared with the gsi’s (geospatial information authority of japan) flood inundation map as a reference and thresholding-based approach as a comparative result. our proposed method shows a significant improvement compared to the thresholding method. also, the detected flooded regions are more connected and homogenous in comparison to the patchy result of the thresholding method.', 'sen1floods11: a georeferenced dataset to train and test deep learning flood algorithms for sentinel-1 accurate flood mapping at global scale can support disaster relief and recovery efforts. improving flood relief efforts with more accurate data is of great importance due to expected increases in the frequency and magnitude of flood events due to climate change. to assist efforts to operationalize deep learning algorithms for flood mapping at global scale, we introduce sen1floods11, a surface water data set including raw sentinel-1 imagery and classified permanent water and flood water. this dataset consists of 4, 831 512x512 chips covering 120, 406 km2 and spans all 14 biomes, 357 ecoregions, and 6 continents of the world across 11 flood events. we used sen1floods11 to train, validate, and test fully convolutional neural networks (fcnns) to segment permanent and flood water. we compare results of classifying permanent, flood, and total surface water from training a fcnn model on four subsets of this data: i) 446 hand labeled chips of surface water from flood events; ii) 814 chips of publicly available permanent water data labels from landsat (jrc surface water dataset); iii) 4, 385 chips of surface water classified from sentinel-2 images from flood events and iv) 4, 385 chips of surface water classified from sentinel-1 imagery from flood events. we compare these four models to a common remote sensing approach of thresholding radar backscatter to identify surface water. results show the fcnn model trained on classifications of sentinel-2 flood events performs best to identify flood and total surface water, while backscatter thresholding yielded the best result to identify permanent water classes only. our results suggest deep learning models for flood detection of radar data can outperform threshold based remote sensing algorithms, and perform better with training labels that include flood water specifically, not just permanent surface water. we also find that fcnn models trained on plentiful automatically generated labels from optical remote sensing algorithms perform better than models trained on scarce hand labeled data. future research to operationalize computer vision approaches to mapping flood and surface water could build new models from sen1floods11 and expand this dataset to include additional sensors and flood events. we provide sen1floods11, as well as our training and evaluation code at: https://github.com/cloudtostreet/sen1floods11.']"
47,risk_management,1,42,47_landslide_landslides_susceptibility_hazard,"['landslide', 'landslides', 'susceptibility', 'hazard', 'geological', 'risk', 'susceptibility mapping', 'auc', 'rainfall', 'area', 'debris', 'hazards', 'induced', 'earthquake', 'loess', 'assessment', 'areas', 'seismic', 'disasters', 'mining', 'region', 'prevention', 'high', 'future', 'susceptibility assessment', 'climate', 'regions', 'rbfnn', 'slope', 'deep']","['evaluation and prediction of landslide susceptibility in yichang section of yangtze river basin based on integrated deep learning algorithm landslide susceptibility evaluation (lse) refers to the probability of landslide occurrence in a region under a specific geological environment and trigger conditions, which is crucial to preventing and controlling landslide risk. the mainstream of the yangtze river in yichang city belongs to the largest basin in the three gorges reservoir area and is prone to landslides. affected by global climate change, seismic activity, and accelerated urbanization, geological disasters such as landslide collapses and debris flows in the study area have increased significantly. therefore, it is urgent to carry out the lse in the yichang section of the yangtze river basin. the main results are as follows: (1) based on historical landslide catalog, geological data, geographic data, hydrological data, remote sensing data, and other multi-source spatial-temporal big data, we construct the lse index system; (2) in this paper, unsupervised deep embedding clustering (dec) algorithm and deep integration network (capsule neural network based on senet: se-capnet) are used for the first time to participate in non-landslide sample selection, and lse in the study area and the accuracy of the algorithm is 96.29; (3) based on the constructed sensitivity model and rainfall forecast data, the main driving mechanisms of landslides in the yangtze river basin were revealed. in this paper, the study area’s mid-long term lse prediction and trend analysis are carried out. (4) the complete results show that the method has good performance and high precision, providing a reference for subsequent lse, landslide susceptibility prediction (lsp), and change rule research, and providing a scientific basis for landslide disaster prevention.', 'using deep learning to formulate the landslide rainfall threshold of the potential large-scale landslide the complex and extensive mechanism of landslides and their direct connection to climate change have turned these hazards into critical events on a global scale, which can have significant negative influences on the long-term sustainable development of nations. taiwan experiences numerous landslides on different scales almost every year. however, typhoon morakot (2009), with large-scale landslides that trapped people, demonstrated the importance of an early warning system. the absence of an effective warning system for landslides along with the impossibility of its accurate monitoring highlighted the necessity of landslide rainfall threshold prediction. accordingly, the prediction of the landslide rainfall threshold as an early warning system could be an effective tool with which to develop an emergency evacuation protocol. the purpose of this study is to present the capability of the deep learning algorithm to determine the distribution of landslide rainfall thresholds in a potential large-scale landslide area and to assess the distribution of recurrence intervals using probability density functions, as well as to assist decision makers in early responses to landslides and reduce the risk of large-scale landslides. therefore, the algorithm was developed for one of the potential large-scale landslide areas (the alishan d098 sub-basin), taiwan, which is classified as a type ii landslide priority area. the historical landslide data, maximum daily rainfall, 11 topographic factors from 2004 to 2017, and the keras application programming interface (api) python library were used to develop two deep learning models for landslide susceptibility classification and landslide rainfall threshold regression. the predicted result shows the lowest landslide rainfall threshold is located primarily in the northeastern downstream of the alishan catchment, which poses an extreme risk to the residential area located upstream of the landslide area, particularly if large-scale landslides were to be triggered upstream of alishan. the landslide rainfall threshold under controlled conditions was estimated at 780 mm/day (20-year recurrence interval), or 820 mm/day (25-year recurrence interval). since the frequency of extreme rainfall events caused by climate change is expected to rise in the future, the overall landslide rainfall threshold was considered 980 mm/day for the entire area.', 'community perceptions of landslide risk and susceptibility: a multi-country study landslides are commonly occurring global geological hazards, with negative and far-reaching consequences for human life, and the economic and natural environment. this study analyzes the risk, vulnerability, and resilience of several landslide-prone areas in eight countries based on community perception and attitude related to landslide events through an online survey questionnaire that includes four sections: (1) natural disaster and landslide susceptibility, (2) knowledge and awareness of the causes of landslides, (3) psychological effects of landslide events, and (4) sociodemographic and socioeconomic characteristics. to gain an in-depth understanding of the landslide risk and sustainability, three state-of-the-art machine learning (ml) approaches, decision tree (dt), xtreme gradient boosting (xgb), and random forest (rf), and a statistical approach, logistic regression (lr), were used to investigate and compare the performance of the ml models. the results suggested that indirect losses caused by landslides significantly affect the communities across the globe both psychologically and economically. furthermore, heavy rain is the most relevant event that was perceived by the communities to trigger a potential landslide, while major civil infrastructure systems, including building and road damages, were found as the main threats to the community due to landslides. the most significant factors related to landslide risk based on the gini coefficient were found to be land classification, environmental factors, and building material. in addition, based on lr, land classification, landslide health impact, environmental factors, and emergency response were significantly related to landslide risk perception. in summary, the xgb model is feasible in the assessment of landslide susceptibility; the prediction performance is also robust and produced better prediction capacities compared to other ml models. this research can considerably assist stakeholders in making better decisions toward effective landslide hazard management. future research should focus on expanding this study to other critical landslide-prone communities by involving more diverse population.']"
56,risk_management,1,48,56_land_land use_gis_use,"['land', 'land use', 'gis', 'use', 'agricultural', 'suitability', 'decision', 'planning', 'citrus', 'spatial', 'information', 'decision support', 'development', 'dss', 'ecological', 'criteria', 'support', 'land suitability', 'units', 'rural', 'use planning', 'poverty', 'spatial decision', 'areas', 'sdss', 'evaluation', 'management', 'resources', 'sustainable', 'agriculture']","['developing gis decision support to stakeholders on agricultural land use planning in ha tinh province, vietnam land use planning in vietnam has been conducted but still insufficient for many years. in rural areas, it has been predominantly relying on assessing the suitability of soil for agricultural production. besides, the planning has extensively incorporated statistics. the current rapid economic growth of vietnam during its economic transition has increased the pressure on land and water resources which leads to a necessity to adjust the approach to land use planning in order to support the economic growth of the country and provide a framework to manage the development in a sustainable manner. there is a need to have a closer alignment between socio-economic development plans, the overall environmental strategy and the land use plans. therefore, the role of land use planning must be broaden through taking into account environmental, socio-economic parameters as well as involving different stakeholders who are affected by changes in land use. it is necessary to have a tool to support policy makers and planners in developing, appraising, and selecting planning scenarios which collaborate different social, economic, and environmental development dimensions in order to gain sustainable development goals. information that have been collected including land use and management by applying geographical information systems (gis) to stakeholders activities in relationship with land use planning. informatics and mathematical modeling techniques have been used in data processing, optimal solution determining and information providing for making decision support. the decision support system (dss) providing tools for planners, policy makers and other stakeholders to improve policies on land use planning and implementation of other rural development and land resource management programs with adaptation to climate change.', 'the framework of an agricultural land-use decision support system based on ecological environmental constraints agricultural land use is a complicated systems engineering. modern agriculture faces increasingly more risks. managers should obtain reliable information to assist in decision-making through certain methods that allow them to achieve an economic, social, and ecological environment that is crucial in coordinating the development of agricultural land-use patterns. this study proposes a framework of an agricultural land-use decision support system (ldss) based on ecological environmental constraints according to the dss design philosophy, which provides a scientific basis for managers to allocate land resources. this framework of ldss consists of a land quality assessment module, an eco-economic coupling module, and a land-use optimisation module. firstly, it establishes a natural-society-economic land quality evaluation system to simulate the comprehensive benefit relationship of land use. secondly, it analyses the risks of soil, water, and ecological security in the process of land use; simulates and reveals the mechanism of occurrence; and completes the correlation equation expression between natural-economic-social indictor and land use risk. based on different scenarios design, the ecological environmental risk factors are used as constraints. finally, the multi-objective linear programming method is employed to calculate the optimal comprehensive benefits of land use and optimal land-use structure based on the constraints of the ecological environment. then the study takes changsha county, a high-intensification, main grain-producing area in central and south china, as a case area to demonstrate the feasibility of the framework of ldss, and draws the highest comprehensive benefits and optimal structure of land use under the premise that the rural ecological environment conforms to national standards. case study shows that the ldss framework is feasible, easy to operate, and easy to promote. the research results can provide efficient and practical support for managers to allocate land resources and formulate sustainable land-use policies rationally.', 'simulation of land use change of erhai lake basin based on ant colony optimization studies of erhai basin indicate that land use change by human activities in the watershed is the leading cause of regional climate, hydrology, water quality and ecological changes. therefore, it is necessary to study the relationships between human activities and land use/cover change (lucc), which is beneficial to offer the scientific decision support for reasonable land planning and land use. combined with gis technologies of spatial analysis and using the artificial intelligence algorithm ant colony optimization(aco) for optimizing, in this paper, we applied the method of agent-based modeling to establish the spatiotemporal process model of lucc in order to simulating the dynamic change of land use in whole watershed. firstly, we made a choice and evaluation for impact factors of land use changes, as well as constructions of the cost of land use change equations in order to construct more reasonable decision rules of land use choice. then, we have extracted three agents composed by microcosmic and macrocosmic systems which were farm agent, resident agent and government agent. also, microcosmic rules of decision and behavior were created according to aco. on the other hand, we have established macrocosmic decision rules according to a resistance coefficient system from the land use planning, as well as a comprehensive decision rule. and then, based on java language and repast platform of modeling, the program design, implementation and simulation of model were given in detail. finally, the validation, calibration and verification of model and analysis of the simulated results were also conducted. our conclusions from the experiment were three: 1) ant colony algorithm was more effective in promoting the significant moving and decision of agents, and the simulated results gained better accuracies in both mathematics (up 5.6%) and geometry (up 3.4%) than using a random algorithm. however, the merit of aco was not suitable for its use in all of land-use types. for an instance, there were no any improvements and sometimes even reduction in accuracy for those land-use types which were less affected by human activities, such as forest, grassland and wetland uses. thereby, we suggested that aco was more sensitive to interaction between human and land-use changes, and it was suitable for optimizing human behaviors and decisions of land-use transfer. 2) if the policy on land use was kept unchanged, the major contradiction between human and land in the future ten years should be the persistent reduction of agricultural land (127.64 hm2 cultivated lands and 11.20 hm2 garden lands) and the continuous increase of urbanized land (95.80 hm2). this indicated a big cost of urbanization in erhai lake basin, which also gave a warning of increasing impervious surfaces (is) produced in future rapid urbanization, and the is may raise risks of urban non-point source pollution in the future. 3) the fast increasing of wetlands (growth rate of 50.07% was the fastest in the change of all land use types) indicated that the governmental land use policies to ecosystem protection have played a better role in macro control of land resources allocation. from this research, we suggested that the local government should maintain the existing strategies of ecological environment protection to reduce the risk of water pollution in erhai lake basin. the competition in the market economy model of land-resources-commercial should be encouraged to balance the next major conflicts between human activities and land resources.']"
101,risk_management,1,42,101_ontology_emergency_earth_science,"['ontology', 'emergency', 'earth', 'science', 'orbital', 'knowledge', 'information', 'project', 'services', 'decision support', 'support', 'decision', 'national', 'space', 'eo', 'integration', 'weather', 'domain', 'climate', 'sciences', 'platform', 'big', 'ontologies', 'development', 'open', 'processing', 'portal', 'stakeholders', 'systems', 'debris']","['space-based earth observation in support of the unfccc paris agreement space-based earth observation (eo), in the form of long-term climate data records, has been crucial in the monitoring and quantification of slow changes in the climate system—from accumulating greenhouse gases (ghgs) in the atmosphere, increasing surface temperatures, and melting sea-ice, glaciers and ice sheets, to rising sea-level. in addition to documenting a changing climate, eo is needed for effective policy making, implementation and monitoring, and ultimately to measure progress and achievements towards the overarching goals of the united nations framework convention on climate change (unfccc) paris agreement to combat climate change. the best approach for translating eo into actionable information for policymakers and other stakeholders is, however, far from clear. for example, climate change is now self-evident through increasingly intense and frequent extreme events—heatwaves, droughts, wildfires, and flooding—costing human lives and significant economic damage, even though single events do not constitute “climate”. eo can capture and visualize the impacts of such events in single images, and thus help quantify and ultimately manage them within the framework of the unfccc paris agreement, both at the national level (via the enhanced transparency framework) and global level (via the global stocktake). we present a transdisciplinary perspective, across policy and science, and also theory and practice, that sheds light on the potential of eo to inform mitigation, including sinks and reservoirs of greenhouse gases, and adaptation, including loss and damage. yet to be successful with this new mandate, eo science must undergo a radical overhaul: it must become more user-oriented, collaborative, and transdisciplinary; span the range from fiducial to contextual data; and embrace new technologies for data analysis (e.g., artificial intelligence). only this will allow the creation of the knowledge base and actionable climate information needed to guide the unfccc paris agreement to a just and equitable success.', 'nasa earth science research results enhance public health science for society the purpose of nasas earth science program is to develop a scientific understanding of earths system and its response to natural or human-induced changes and to improve prediction of climate, weather, and natural hazards. as one of the twelve areas of national priority for the applied sciences program, the public health program element extends the benefits of increased knowledge and capabilities resulting from nasa earth science satellite observations, model predictive capabilities, and technology, into partners decision support systems for public health, medical, and environmental health issues. through the public health program, nasa and partnering organizations have built a network that focuses on the relationships between nasa earth observation systems, modeling systems, and partner-led decision support systems for epidemiologic surveillance in the areas of infectious disease, environmental health (including air quality), and emergency response and preparedness. the next-generation of nasa earth-observing satellites will be launched over the next few years. these satellites will provide observations of even greater temporal and spatial resolution to further enhance decision support for society.', 'establishing a policy framework for global change earth observations global change of our environment is a key issue for our times. one hundred years ago the planet had a population of around 2 billion people and a pace of life where we did not have to anticipate much beyond a few months into the future. with their unique vantage point in space, earth observation satellites contribute significantly to monitoring and understanding changes to our planet, and beyond that, enhancing our ability to forecast significant weather, climate, and global change events. now, as we move towards a future with more than 9 billion people and an increasingly complex and dynamic earth, it is even more important to be able to anticipate changes that are coming and impact our populations. this requires two aspects: 1) a policy framework for planning, developing, investing, and deploying the systems and underlying information infrastructure to inform decision-making, and 2) a monitoring system focused on coherent global change detection information that is collected globally, focused regionally, and useful locally to underpin policy solutions across our societies - from health policy, to economic policy, to foreign policy, to environmental policy. for the first aspect, the accomplishments of space, weather, and climate, modeling, and decision support systems in understanding global change rely upon the policy foundation that enables successful application of advanced technologies to our daily lives. achieving better future capabilities includes recognizing the value and capacity for policies enabling improvements to: • earth observation systems planned, deployed, and sustained over decades • prototype research earth observation systems enabling next generation operational systems • efficiently transition of prototype research to operational capabilities • interoperable data access • earth observation system data assimilated into weather, climate and hazard models • industry and the private sector-based innovation of new technologies and solutions • organizational coordination and communication of advancements in science and technology • improved cost efficiency and effectiveness (doing more with less) the second aspect relies upon the space, weather, climate, modeling, and decision support systems for assuring the citizenry has the information it needs to formulate and apply broad policies useful to governing societies, enhancing economies, and addressing urgent needs. we have entered a phase in our history with many new deepening societal and environmental challenges. we also have the benefit of major technological advances offering opportunities for us to collectively advance our national and international policies to meet the societal needs of the 21 st century. copyright © 2008 by copyright © 2008 by northrop grumman corporation.']"
102,risk_management,1,37,102_infrastructure_resilience_structural_structures,"['infrastructure', 'resilience', 'structural', 'structures', 'underground', 'rescue', 'infrastructures', 'damage', 'heritage', 'bridge', 'risk', 'critical infrastructure', 'engineering', 'buildings', 'construction', 'civil', 'critical', 'monitoring', 'resilient', 'historic', 'search rescue', 'safety', 'space', 'bridges', 'urban', 'built environment', 'emergency', 'service', 'technologies', 'design']","['multimodal sensing for sustainable structural health monitoring of critical infrastructures and built environment in the last years, there is an increasing attention to the management and monitoring of critical infrastructure with a particular focus on the transport ones. in fact, several infrastructures have experienced (especially bridges) significant safety criticalities, which in few cases led to collapse with significant social and economic impacts. in this context, structural health monitoring (shm) is necessary in order to provide information, which can support the strategies for increasing the life-time of the structure, correctly planning maintenance interventions, so to ensure safety conditions of the infrastructure under any environmental condition and hazard occurrence. at the sight of the above considerations, at present, significant research efforts are on-going towards development of shm systems able to get the opportunity of technological advances in sensing and, at the same time, to be economically sustainable. in particular, there is the necessity to design and implement integration strategies, where the modern sensing technologies, coming from several research fields, are combined with the usual tools and models exploited in civil engineering. therefore, the present contribution deals with a unified approach to the design and development of systems for early warning, monitoring and quick damage assessment of the built environment and critical infrastructures and networks. in this context, the integration is the key factor for exploiting the synergy among different kind of sensing/diagnostics technologies (including new concept of operation, such as the citizen as a sensor and sensors no sensors) and advanced spatial data infrastructure and ict architectures. it is also important to assimilate monitoring data and indicators coming from the sensing into civil engineering; this is crucial to identify actions and strategies for an effective and economically sustainable management of the infrastructure. in addition, the integrated shm has to be designed and implemented according to several stages exploiting different sensing technologies, which are organized according to a temporal and logical workflow characterized by different levels of observation and knowledge, where the information gained at a level is used to decide whether to activate the next level. at conference, several examples of the proposed integrated shm approach will be presented, with a focus on transport infrastructure and built environment.', 'the response of the rescue system to large scale emergencies a case study: the collapse of the morandi bridge part 2 of 2: technologies for rescue service when a complex scenario occurs (e.g. earthquakes, forest fires, floods), the italian national fire and rescue service 1 mobilizes a large response by local first responders and additional specialized teams. in this age of disaster, also due to climate change and critical anthropic environments, the effectiveness of the response to various scenarios is also played by the availability of proper technologies that may support the skills and the expertise of rescuers. taking into consideration some collapses of civil engineering structures occurred in genoa in recent years, with a particular focus on the collapse of the morandi bridge, the paper illustrates how the interaction between humans and technology is nowadays able to facilitate both the speedy risk-based decision making in the operational activities of the emergency phase and the residual risk assessments in the early recovery phase. the issues of the paper are: the technological tools of urban search & rescue teams for a fast localization of victims; the application of a search and rescue methodology according to international procedures and guidelines (insarag2); the structural risk management for an immediate evaluation of the collapsed structures and rubble to care the rescuers safety and the numerical modelling of artefacts for post-emergency evaluations; a description of a monitoring alarm system - designed by experts of the scientific community and industry and installed by firefighters in a synergic role play - to ensure safety measures for rescuers and population during the activity of the asset recovery in the built environments under the residual structures of the bridge; the use of geolocalization technologies in the forensic assistance; the use of modern technologies and instruments in the aeronautics industry (drones and helicopters) as valid support in search and rescue activity, giving a different point of view of the scenario to be analysed; the use of artificial intelligence, augmented reality, iot for reducing the workload of the flight rescuers and increasing their operational efficacy. new challenges: the use of humanoid robots in search & rescue activities or in industrial applications, characterized by high risk for humans, to reach inaccessible zone and to ensure shorter time response.', 'the response of the rescue system to large scale emergencies a case study: the collapse of the morancli bridge part 2 of 2: technologies for rescue service when a complex scenario occurs (e.g. earthquakes, forest fires, floods), the italian national fire and rescue service 1 mobilizes a large response by local first responders and additional specialized teams. in this age of disaster, also due to climate change and critical anthropic environments, the effectiveness of the response to various scenarios is also played by the availability of proper technologies that may support the skills and the expertise of rescuers. taking into consideration some collapses of civil engineering structures occurred in genoa in recent years, with a particular focus on the collapse of the morandi bridge, the paper illustrates how the interaction between humans and technology is nowadays able to facilitate both the speedy risk-based decision making in the operational activities of the emergency phase and the residual risk assessments in the early recovery phase. the issues of the paper are: the technological tools of urban search & rescue teams for a fast localization of victims; the application of a search and rescue methodology according to international procedures and guidelines (insarag2); the structural risk management for an immediate evaluation of the collapsed structures and rubble to care the rescuers safety and the numerical modelling of artefacts for post-emergency evaluations; a description of a monitoring alarm system - designed by experts of the scientific community and industry and installed by firefighters in a synergic role play - to ensure safety measures for rescuers and population during the activity of the asset recovery in the built environments under the residual structures of the bridge; the use of geolocalization technologies in the forensic assistance; the use of modem technologies and instruments in the aeronautics industry (drones and helicopters) as valid support in search and rescue activity, giving a different point of view of the scenario to be analysed; the use of artificial intelligence, augmented reality, iot for reducing the workload of the flight rescuers and increasing their operational efficacy. new challenges: the use of humanoid robots in search & rescue activities or in industrial applications, characterized by high risk for humans, to reach inaccessible zone and to ensure shorter time response.']"
105,risk_management,1,20,105_pipeline_pipe_pipelines_leaks,"['pipeline', 'pipe', 'pipelines', 'leaks', 'maintenance', 'membrane', 'leak', 'water', 'graph', 'sewer', 'failures', 'failure', 'deterioration', 'detection', 'replacement', 'infrastructure', 'utilities', 'water distribution', 'erosion', 'anomaly detection', 'predictive maintenance', 'outlier detection', 'infrastructure systems', 'dam', 'netherlands', 'anomaly', 'accident', 'functional', 'outlier', 'concrete']","['an exploration on the machine learning approaches to determine the erosion rates for liquid hydrocarbon transmission pipelines towards safer and cleaner transportations erosion is commonly found in pipelines with existence of interactions between solid particles, fluid product transported and the surrounding materials. erosion may lead to material degradation in pipeline and may eventually cause pipeline failure. this failure may cause catastrophic consequence to the surrounding health, property and the environment. therefore, a better estimation on the pipeline erosion rate is critical for the pipeline engineers to maintain a safer, more cost-effective and sustainable operating condition. in this study, two erosion rate models were used to create an erosion rate database and three machine-learning approaches were developed to help pipeline engineers evaluate the optimal flow velocity. the major results of this study are threefold: first, a purely visual approach was used to present critical information to decision makers. second, three machine learning approaches (classification tree, artificial neural network and bayesian network) were established for the erosion rate determination. third, the benefits and limits of each approach were discussed and evaluated. as novel methodologies to estimate pipeline erosion rate, the machine learning approaches can serve as simple or comprehensive tools to address the various pipeline operating scenarios, with certain or uncertain information. this study provides innovative, intuitive and cost-effective tools for the pipeline operators to predict the erosion rate under various operating conditions and handles the uncertainty via a probabilistic approach, aiming to build safer and more environmental-friendly pipelines.', 'predicting natural gas pipeline failures caused by natural forces: an artificial intelligence classification approach pipeline networks are a crucial component of energy infrastructure, and natural force damage is an inevitable and unpredictable cause of pipeline failures. such incidents can result in catastrophic losses, including harm to operators, communities, and the environment. understanding the causes and impact of these failures is critical to preventing future incidents. this study investigates artificial intelligence (ai) algorithms to predict natural gas pipeline failures caused by natural forces, using climate change data that are incorporated into pipeline incident data. the ai algorithms were applied to the publicly available pipeline and hazardous material safety administration (phmsa) dataset from 2010 to 2022 for predicting future patterns. after data pre-processing and feature selection, the proposed model achieved a high prediction accuracy of 92.3% for natural gas pipeline damage caused by natural forces. the ai models can help identify high-risk pipelines and prioritize inspection and maintenance activities, leading to cost savings and improved safety. the predictive capabilities of the models can be leveraged by transportation agencies responsible for pipeline management to prevent pipeline damage, reduce environmental damage, and effectively allocate resources. this study highlights the potential of machine learning techniques in predicting pipeline damage caused by natural forces and underscores the need for further research to enhance our understanding of the complex interactions between climate change and pipeline infrastructure monitoring and maintenance.', 'using artificial intelligence for water pipeline infrastructure asset management it is critical for society that we transform our siloed water management and infrastructure systems into smart, connected, sustainable, and resilient systems. this transformation can help us to address the effects of increasing extreme climate events, ecosystem demands, rapid global urbanization, and infrastructure deterioration from age and neglect. as water utilities improve their asset management programs, it is imperative that the data-driven decision support systems represent the complexities within water pipeline infrastructure systems. artificial intelligence (ai) techniques can enable modelers to train mathematical algorithms to learn complex patterns from data and represent the water pipeline infrastructure systems accurately. pipeid is a national database platform that uses artificial intelligence (ai) and machine learning techniques to assess the performance and risk of water pipelines to help utilities better assess pipe replacement decisions and allocate funding. pipeid (pipeline infrastructure database) will assist water sector utilities to manage water pipeline infrastructure systems more effectively for performance, resiliency, and sustainability. pipeid will provide the secure, robust, and centralized web-based database platform to address all three major infrastructure asset management levels: strategic, tactical, and operational for utilities of all sizes (small, medium, and large) across the country. the research team collected field performance data for potable, raw, and reuse water pipelines made from materials reflecting the wide range of pipes currently in the ground throughout the us, including cast and ductile iron, prestressed concrete cylinder pipe, reinforced concrete, steel, thermoplastic, pvc, and asbestos. the researchers worked to collect data distributed across different ecological areas, or cohorts, organized based on the climatic conditions of the 500 water utilities locations. these cohorts included coastal, arid, arctic, and mountainous regions. researchers factored in environmental conditions, such as soil corrosivity, traffic loading, and frost action, that can affect pipelines. the team enhanced the data set out further with the help of external sources like the united states geological survey (usgs), the soil survey geographic database from the united states department of agriculture (usda), and additional field data collected by a group of 25 water utilities across the country that were selected to validate the models and tools. this paper will present various applications of artificial intelligence and machine learning algorithms for advanced asset management of water pipeline infrastructure systems.']"
106,risk_management,1,33,106_water_water supply_smart water_conservancy,"['water', 'water supply', 'smart water', 'conservancy', 'smart', 'management', 'supply', 'iot', 'monitoring', 'dimension', 'river', 'meters', 'sensor', 'internet things', 'things', 'internet', 'water quality', 'water level', 'real', 'quality', 'challenges', 'utilities', 'water distribution', 'real time', 'systems', 'drinking', 'water monitoring', 'solution', 'water demand', 'knowledge']","['analysis of smart water conservancy smart water conservancy is a significant sign of high-quality development of water conservancy. based on the natural water system, the water conservancy engineering system and water conservancy management system, it uses big data, artificial intelligence, internet of things, cloud computing, mobile inter- net and other new generation information and communication technologies, to smart deliver the appropriate quatity and quality of water to the right place at the right time, which can support sustainable water security, high-quality water resources, livable water environment and healthy water ecology. the connotation of smart water conservancy includes six dimensions, namely goal dimension, object dimension, space-time dimension, technology dimension, value dimension and effect dimension. the goal dimension includes problem identification and governance objectives, the object dimension includes service subject and water conservancy business, the space-time dimension includes monitoring link and decision-making process, the technology dimension includes enabling system and core technology, the value dimension includes evolution mod- el and value-added path, and the effect dimension includes functional representation and construction vi- sion. in the development of smart water conservancy, data is the basis, so great attention should be paid to the collection, opening and sharing of water conservancy data; information is the key to intelligent water conservancy, so great attention should be paid to the information extraction and analysis from monitoring data; knowledge is the core of intelligent water conservancy, so it is necessary to strengthen the mining and upgrading of water conservancy information, and the accumulation of knowledge; intelligence and wisdom are the purpose of intelligent water conservancy, and attention should be paid to the use of artificial intelligence and the construction of standardized management system.', 'smart water solutions for the operation and management of a water supply system in aracatuba, brazil because of population growth, rapid urbanization, and climate change, many water supply utilities globally struggle to provide water that is safe to drink. a particular problem is the aging of the water supply facilities, which is exacerbated by their inefficient operation and maintenance (o&m). for this reason, many water utilities have recently been actively adopting intelligent and integrated water supply o&m solutions that utilize information and communication technology, the internet of things, big data, and artificial intelligence to solve water supply system problems. in this study, smart water solutions (gswaters) were implemented to enhance the efficiency of the water supply system in the city of aracatuba, brazil. they were used to monitor and analyze the operating conditions of the water supply system in real time, thus allowing for the effective management of water supply assets. gswaters also supports the design and optimization of district metered areas, the reduction and management of water losses, real-time water network analysis, and big data analysis using artificial intelligence. economic analysis revealed that gswaters produces various direct and indirect benefits for the water supply system.', 'smartwater: a service-oriented and sensor cloud-based framework for smart monitoring of water environments due to the sharp increase in global industrial production, as well as the over-exploitation of land and sea resources, the quality of drinking water has deteriorated considerably. furthermore, nowadays, many water supply systems serving growing human populations suffer from shortages since many rivers, lakes, and aquifers are drying up because of global climate change. to cope with these serious threats, smart water management systems are in great demand to ensure vigorous control of the quality and quantity of drinking water. indeed, water monitoring is essential today since it allows to ensure the real-time control of water quality indicators and the appropriate management of resources in cities to provide an adequate water supply to citizens. in this context, a novel iot-based framework is proposed to support smart water monitoring and management. the proposed framework, named smartwater, combines cutting-edge technologies in the field of sensor clouds, deep learning, knowledge reasoning, and data processing and analytics. first, knowledge graphs are exploited to model the water network in a semantic and multi-relational manner. then, incremental network embedding is performed to learn rich representations of water entities, in particular the affected water zones. finally, a decision mechanism is defined to generate a water management plan depending on the water zones’ current states. a real-world dataset has been used in this study to experimentally validate the major features of the proposed smart water monitoring framework.']"
109,risk_management,1,29,109_flood_disaster_disasters_urban flood,"['flood', 'disaster', 'disasters', 'urban flood', 'warning', 'sensor', 'iot', 'real time', 'monitoring', 'information', 'natural disasters', 'early warning', 'rainfall', 'real', 'disaster management', 'wsn', 'natural', 'emergency', 'wireless', 'floods', 'management', 'urban', 'flood forecasting', 'flooding', 'early', 'mobile', 'wireless sensor', 'iot based', 'weather', 'inundation']","['flood detection and forecast by iot technology under the influence of climate change, the scale and impact of flood disasters have become more and more severe in taiwan due to the increase in rainfall intensity and urbanization. to strengthen the technology of flood detection and forecast in urban areas, an iot (internet of things) based flood sensor (named flood box) and a flood prediction model based on machine learning (ml) technology were developed in this study. for flood detection, the flood boxes are installed at several low-lying locations in tainan, taiwan, for inundation depth measurement. pressure tests show that the flood boxes functioned normally under outdoor rainy weather conditions. for flood forecast, the observed data by flood sensors are processed by a svr (support vector regression) ml model to predict the inundation depth at the locations where flood sensors are absent or malfunctioned in a flood event on 13 august in 2019. satisfactory agreements between prediction and observation are found with the overall rmse (root-mean-square error) equivalent to 5.73 cm.', 'development of a spatial decision support system for real-time flood early warning in the vu gia-thu bon river basin, quang nam province, vietnam vu gia-thu bon (vgtb) river basin is an area where flash flood and heavy flood events occur frequently, negatively impacting the local community and socio-economic development of quang nam province. in recent years, structural and non–structural solutions have been implemented to mitigate damages due to floods. however, under the impact of climate change, natural disasters continue to happen unpredictably day by day. it is, therefore, necessary to develop a spatial decision support system for real-time flood warnings in the vgtb river basin, which will support in ensuring the area’s socio-economic development. the main purpose of this study is to develop an online flood warning system in real-time based on internet-of-things (iot) technologies, gis, telecommunications, and modeling (soil and water assessment tool (swat) and hydrologic engineering center’s river analysis system (hec–ras)) in order to support the local community in the vulnerable downstream areas in the event of heavy rainfall upstream. the structure of the designed system consists of these following components: (1) real-time hydro-meteorological monitoring network, (2) iot communication infrastructure (global system for mobile communications (gsm), general packet radio service (gprs), wireless networks), (3) database management system (bio-physical, socio-economic, hydro-meteorological, and inundation), (4) simulating and predicting model (swat, hec–ras), (5) automated simulating and predicting module, (6) flood warning module via short message service (sms), (7) webgis, application for providing and managing hydro-meteorological and inundation data, and (8) users (citizens and government officers). the entire operating processes of the flood warning system (i.e., hydro-meteorological data collecting, transferring, updating, processing, running swat and hec–ras, visualizing) are automated. a complete flood warning system for the vgtb river basin has been developed as an outcome of this study, which enables the prediction of flood events 5 h in advance and with high accuracy of 80%.', 'review of the status of urban flood monitoring and forecasting in tc region with the impacts of rapid urbanization and climate change, the urban flood has increasingly become a major hazard risk faced by human being in recent decades. the catastrophic urban flood events appear every year in the world, especially in asia and pacific region due to its geographical composition, density population and un-even economic and social development. to reduce the urban flood risk and enhance the resilience of vulnerable communities, especially coastal communities, the members of escap/wmo typhoon committee (tc) have made their great efforts including engineering and non-engineering measures based on their different national conditions. as a key part of non-engineering measures, it is recognized that improving urban flood monitoring and forecasting is a measure with high benefit related to cost on urban flood risk reduction. in recent years, tc members enhanced their capacity building on urban flood monitoring, forecasting and simulation, inundation mapping, etc. in order to enhance the technical cooperation and exchange on this aspect, typhoon committee working group on hydrology (wgh) conducted two projects on “urban flood risk management in typhoon committee area (ufrm)” and “operation system for urban flood forecasting and inundation mapping (osuffim)” in the past years. this paper generally reviewed the situation and causes of urban flood in tc region; briefly summarized the progresses and shortages on urban flood monitoring and forecasting in tc members; and initially discussed the areas to be enhanced in future for improvement of urban flood monitoring, forecasting and simulation, and inundation mapping with up-to-date development of weather radar and satellite monitoring, image-based monitoring, information technology (it), internet of things (iot), big data and artificial intelligence (ai).']"
117,risk_management,1,64,117_flood_risk_disaster_resilience,"['flood', 'risk', 'disaster', 'resilience', 'coastal', 'flood risk', 'disasters', 'community', 'hazards', 'vulnerability', 'hurricane', 'risk reduction', 'disaster risk', 'preparedness', 'hazard', 'assessment', 'damage', 'natural', 'floods', 'decision', 'evacuation', 'sea level', 'risk assessment', 'weather', 'adaptation', 'climate', 'events', 'flooding', 'areas', 'level rise']","['spatial heterogeneities of current and future hurricane flood risk along the u.s. atlantic and gulf coasts we evaluate the spatial heterogeneities of hurricane flood risk along the united states (u.s.) atlantic and gulf coasts under two different climate scenarios (current and future). the flood hazard is presented as the hurricane surge flood level with 1% annual exceedance probability (100-year flood) under the two scenarios, where the future scenario considers the effect of hurricane climatology change and sea level rise towards late-21st-century under a high emission scenario (rcp8.5). this hazard information is combined with estimated vulnerability and disaster resilience of coastal communities to map the relative current and future risk employing different risk definitions. several geographical techniques and spatial distributional models (e.g., spatial autocorrelation, spatial hotspot analysis, and spatial multivariate clustering analysis) are applied to systematically analyze the risk and identify statistically significant hotspots of the highest risk. most of the high-risk hotspots are found in the gulf coast region, particularly along the west coast of florida. however, two out of three risk evaluation approaches also indicate new york city as a risk hotspot under the future climate—showing that the resultant risk is sensitive to the consideration of evaluation factors (i.e., hazard, vulnerability, and resilience). additionally, we apply a machine-learning algorithm-based spatial multivariate approach to map the spatially distinct groups based on the values of risk, hazard, vulnerability, and resilience. the results show that the counties in the highest risk group (value >3rd quartile, 15% of total counties, including new york city) in the future lack specifically in the community capital and the social components of community resilience. this assessment of coastal risk to hurricane flood has important policy-relevant implications to provide a focus-for-action for risk reduction and resilience enhancement for the u.s., where 6.5 million households live in the hurricane flood-prone areas.', 'community flood resilience categorization framework coupled with climate change, the expansive developments of urban areas are causing a significant increase in flood-related disasters worldwide. however, most flood risk analysis and categorization efforts have been focused on the hydrologic features of flood hazards (e.g., inundation depth, extent, and duration), rarely considering the resulting long-term losses and recovery time (i.e., the communitys flood resilience). this paper aims at developing a data-driven community flood resilience categorization framework that can be utilized for the development of realistic disaster management strategies and proactive risk mitigation measures to better protect urban centers from future catastrophic flood events. this approach considers key resilience goals such as the robustness of the exposed community and its recovery rapidity. such categorization that calls on the two resilience means, namely resourcefulness and redundancy, can empower decision makers to learn from past events and guide future resilience strategies. to demonstrate the applicability of the developed framework, a data-driven framework was applied on historical mainland flood disaster records collected by the us national weather services between 1996 and 2019. descriptive analysis was conducted to identify the features of this dataset as well as the interdependence between the different variables considered. to further demonstrate the utilization of the developed data-driven framework, a spatial analysis was conducted to quantify community flood resilience across different counties within the affected states. beyond the work presented in this paper, the developed framework lays the foundation to adopt data driven approaches for disasters prediction to guide proactive risk mitigation measures and develop community resilience management insights.', 'data-driven community flood resilience prediction climate change and the development of urban centers within flood-prone areas have significantly increased flood-related disasters worldwide. however, most flood risk categorization and prediction efforts have been focused on the hydrologic features of flood hazards, often not considering subsequent long-term losses and recovery trajectories (i.e., community’s flood resilience). in this study, a two-stage machine learning (ml)-based framework is developed to accurately categorize and predict communities’ flood resilience and their response to future flood hazards. this framework is a step towards developing comprehensive, proactive flood disaster management planning to further ensure functioning urban centers and mitigate the risk of future catastrophic flood events. in this framework, resilience indices are synthesized considering resilience goals (i.e., robustness and rapidity) using unsupervised ml, coupled with climate information, to develop a supervised ml prediction algorithm. to showcase the utility of the framework, it was applied on historical flood disaster records collected by the us national weather services. these disaster records were subsequently used to develop the resilience indices, which were then coupled with the associated historical climate data, resulting in high-accuracy predictions and, thus, utility in flood resilience management studies. to further demonstrate the utilization of the framework, a spatial analysis was developed to quantify communities’ flood resilience and vulnerability across the selected spatial domain. the framework presented in this study is employable in climate studies and patio-temporal vulnerability identification. such a framework can also empower decision makers to develop effective data-driven climate resilience strategies.']"
118,risk_management,1,102,118_flood_flooding_floods_flood susceptibility,"['flood', 'flooding', 'floods', 'flood susceptibility', 'susceptibility', 'flood risk', 'risk', 'flash', 'rainfall', 'auc', 'machine', 'flash flood', 'urban', 'machine learning', 'areas', 'disaster', 'prediction', 'area', 'hazards', 'flood forecasting', 'damage', 'high', 'inundation', 'learning', 'river', 'hazard', 'curve', 'events', 'urban flood', 'disasters']","['mapping the spatial and temporal variability of flood hazard affected by climate and land-use changes in the future the predicts current and future flood risk in the kalvan watershed of northwestern markazi province, iran. to do this, 512 flood and non-flood locations were identified and mapped. twenty flood-risk factors were selected to model flood risk using several machine learning techniques: conditional inference random forest (cirf), the gradient boosting model (gbm), extreme gradient boosting (xgb) and their ensembles. to investigate the future (year 2050) effects of changing climates and changing land use on future flood risk, a general circulation model (gcm) with representative concentration pathways (rcps) of the 2.6 and 8.5 scenarios by 2050 was tested for impacts on 8 precipitation variables. in addition, future land uses in 2050 was prepared using a ca-markov model. the performances of the flood risk models were validated with receiver operating characteristic-area under curve (roc-auc) and other statistical analyses. the auc value of the roc curve indicates that the ensemble model had the highest predictive power (auc = 0.83) and was followed by gbm (auc = 0.80), xgb (auc = 0.79), and cirf (auc = 0.78). the results of climate and land use changes on future flood-prone areas showed that the areas classified as having moderate to very high flood risk will increase by 2050. due to the changes occurring with land uses and in climates, the area classified as moderate to very high risk increased in the predictions from all four models. the areal proportion classes of the risk zones in 2050 under the rcp 2.6 scenario using the ensemble model have changed of the following proportions from the current distribution very low = −12.04 %, low = −8.56 %, moderate = +1.56 %, high = +11.55 %, and very high = +7.49 %. the rcp 8.5 scenario has caused the following changes from the present percentages: very low = −14.48 %, low = −6.35 %, moderate = +4.54 %, high = +10.61 %, and very high = +5.67 %. the results of current and future flood risk mapping can aid planners and flood hazard managers in their efforts to mitigate impacts.', 'assessment analysis of flood susceptibility in tropical desert area: a case study of yemen flooding is one of the catastrophic natural hazards worldwide that can easily cause devastating effects on human life and property. remote sensing devices are becoming increasingly important in monitoring and assessing natural disaster susceptibility and hazards. the proposed research work pursues an assessment analysis of flood susceptibility in a tropical desert environment: a case study of yemen. the base data for this research were collected and organized from meteorological, satellite images, remote sensing data, essential geographic data, and various data sources and used as input data into four machine learning (ml) algorithms. in this study, rs data (sentinel-1 images) were used to detect flooded areas in the study area. we also used the sentinel application platform (snap 7.0) for sentinel-1 image analysis and detecting flood zones in the study locations. flood spots were discovered and verified using google earth images, landsat images, and press sources to create a flood inventory map of flooded areas in the study area. four ml algorithms were used to map flash flood susceptibility (ffs) in tarim city (yemen): k-nearest neighbor (knn), naïve bayes (nb), random forests (rf), and extreme gradient boosting (xgboost). twelve flood conditioning factors were prepared, assessed in multicollinearity, and used with flood inventories as input parameters to run each model. a total of 600 random flood and non-flood points were chosen, where 75% and 25% were used as training and validation datasets. the confusion matrix and the area under the receiver operating characteristic curve (auroc) were used to validate the susceptibility maps. the results obtained reveal that all models had a high capacity to predict floods (auc > 0.90). further, in terms of performance, the tree-based ensemble algorithms (rf, xgboost) outperform other ml algorithms, where the rf algorithm provides robust performance (auc = 0.982) for assessing flood-prone areas with only a few adjustments required prior to training the model. the value of the research lies in the fact that the proposed models are being tested for the first time in yemen to assess flood susceptibility, which can also be used to assess, for example, earthquakes, landslides, and other disasters. furthermore, this work makes significant contributions to the worldwide effort to reduce the risk of natural disasters, particularly in yemen. this will, therefore, help to enhance environmental sustainability.', 'flash-flood hazard susceptibility mapping in kangsabati river basin, india flood-susceptibility mapping is an important component of flood risk management to control the effects of natural hazards and prevention of injury. we used a remote-sensing and geographic information system (gis) platform and a machine-learning model to develop a flood susceptibility map of kangsabati river basin, india where flash flood is common due to monsoon precipitation with short duration and high intensity. and in this subtropical region, climate change’s impact helps to influence the distribution of rainfall and temperature variation. we tested three models-particle swarm optimization (pso), an artificial neural network (ann), and a deep-leaning neural network (dlnn)-and prepared a final flood susceptibility map to classify flood-prone regions in the study area. environmental, topographical, hydrological, and geological conditions were included in the models, and the final model was selected based on the relations between potentiality of causative factors and flood risk based on multi-collinearity analysis. the model results were validated and evaluated using the area under receiver operating characteristic (roc) curve (auc), which is an indicator of the current state of the environment and a value >0.95 implies a greater risk of flash floods. the auc values for ann, dlnn, and pso for training datasets were 0.914, 0.920, and 0.942, respectively. among these three models, pso showed the best performance with an auc value of 0.942. the pso approach is applicable for flood susceptibility mapping of the eastern part of india, a subtropical region, to allow flood mitigation and help to improve risk management in this region.']"
119,risk_management,1,65,119_decision_decision support_support_dss,"['decision', 'decision support', 'support', 'dss', 'coastal', 'management', 'making', 'decision making', 'systems', 'assessment', 'decisions', 'tools', 'support systems', 'information', 'makers', 'aquaculture', 'policy', 'knowledge', 'development', 'decision makers', 'sustainable', 'marine', 'web', 'ontology', 'climate', 'methodology', 'stakeholders', 'tourism', 'impacts', 'project']","['customer focused science and knowledge management for sustainability in new south wales, australia the new south wales (australia) state governments direction is set through its nsw 2021: a plan to make nsw number one ten year strategic plan. key to the governments reforms is the objective to increase the devolution of decision making to the community by providing open access to data, information and services via transparent processes. for the nsw office of environment and heritage (oeh), this is achieved through the open oeh program. the vision is to make oeh data and services open, available, accessible and useful to the community anywhere and anytime. this requires customer focused delivery that may depend on cross agency collaborations to set and deliver priority outcomes. science within state government improves objective decision making at all levels and provides a lens through which often competing perspectives and demands from multiple customers may be better viewed and considered. the expectation is that science will bring to the table and illuminate rigorous and transparent data and information on relevant issues and matters. oeh is seeking to better fulfil these expectations through its corporate emphasis on engaging customers. understanding customer values is emerging as an important element in project management. a customer focus will progressively inform strategic priority setting for end to end project delivery of oeh science and knowledge management. state government science does not duplicate that of other recognised science providers. science within federal agencies and universities is more exploratory in nature whereas state government science is directed research, often drawing information from imperfect/incomplete knowledge. this knowledge is enhanced through participatory end user engagement, which allows for understanding of the data limitations and assumptions. critical to this process is the concept of scientist social capital; that is, the benefits that arise from the science practitioners connections, reputation, status and relationships. paramount to success is developing and maintaining trust with an often diverse customer base. in the oeh the science division has undergone a major restructure to respond to the needs of nsw 2021. the new ecosystem management science (ems) branch consists of cross functional teams designed to deliver products and services to customers and colleagues undertaking policy and program design and delivery. the teams in the branch are headed by recognised scientific leaders who run programs using collective decision making. this seeks to optimise benefits from the science social capital of their connections, reputations and knowledge. it also enables them and their teams to work within a matrix management model to provide rapid and responsive science solutions while working towards long term strategic science program goals. there is a strong interdependency among all the ems teams and this encourages interactions between individuals making possible a broad canvassing of ideas and knowledge. an important feature of the ems branch is that its six teams are orientated on outcomes rather than science disciplines. this acts to further facilitate improved access to science and knowledge across teams and greater sharing of project components. to enhance knowledge creation and flow to customers, a dedicated science knowledge service team has been created to instill rigor in knowledge management and thus directly contribute to open oeh. this team will be strongly supported by an evaluation team that uses modeling and decision support systems. these systems will provide transparent, repeatable forecasting and scenario analysis capabilities. clear two-way communication, will increase customer engagement, understanding, evidence-based decision making and action. ultimately this will lead to better informed and more transparent discussions about ecosystem management including options for trade-offs and multiple co-benefits from government decisions and community investment. this will boost prosperity and sustainability by empowering community and government decision makers to improve ecosystem services and better sustain landscape and community productivity and function.', 'meeting climate change challenges: searching for more adaptive and innovative decisions in the last decades, research in the management sciences, the modeling and analysis of systems, and artificial intelligence (ai), have provided ways of extracting useful knowledge from data and how that knowledge can better inform decision making. such models are designed and used to support decision makers and hence these methods as a group are often termed decision support systems. interactive decision support systems that incorporate visualization to allow users, including decision makers to explore the impacts of possible decisions and assumptions are well known and widely used in the water resources and environmental systems communities, as they are in many other fields. they are now needed to address and analyze possible decisions that may help all of us adapt to, and mitigate the adverse impacts caused by, a changing climate. the literature is full of studies explaining how to develop and use such models for adaptive planning and decision making. yet studies on how decision makers use the results of these analyses and to the extent they have helped decision makers formulate their decisions are scarce. what is clear, however, is that their use does not always guarantee that they were helpful in addressing a particular challenge or issue. the information these analytical tools provide is only part of the information decision makers consider when making their decisions. this paper considers what might be done to assist decision makers improve the decisions they may make in the post-analysis stages of decision-making processes, especially when they are faced with challenging (and often surprising) situations where there appears to be no satisfactory solution. addressing climate change causes and impacts are among these challenges. without modeling support, making and implementing decisions is typically left to each decision maker’s experience, biases, mindset, habits, and beliefs based on their evaluation of all the information available to them. with some additional aids to facilitate the creation of new ideas, i.e., to help individuals adapt and innovate, it just may be possible to convert (find acceptable solutions to) some issues or problems considered unsolvable to solvable ones. a case study involving renewable hydroelectric and solar energy development in the mekong river basin illustrates how these methods can help enable more adaptive and innovative planning and management decisions.', 'a study of the factors leading to sustainable group decisions developed in a creative decision support system problems facing modern society are unstructured and complex rendering traditional decision support with the formation of single solutions inadequate. decision support systems (dss) have recently sought to address this issue by providing creative decision support to facilitate the generation of several alternative solutions to a problem, with particular emphasis on groups. although it is recognised that decision support can improve the quality of decision making, asofyet there has been no exploration of the sustainability of these decisions. is group decision support akin to management fads that over time fade through a lack of durable results? this paper presents a study which sought to take the first steps to inform this question by investigating retrospectively the sustainability of the decisions made by a group supported through a group decision support system (gdss). through an explorative, qualitative research process we sought to understand whether group decision support provides fundamentally short-term gain, or if long-term effects are possible. in-depth interviews were conducted firstly with experts in decision-making theory and support; and, secondly, with a group that had received decision support in a creative group decision support environment. we found that to ensure decisions made in group decision support systems the decisions must be reinforced also after the initial decision has been made to ensure sustainability. this has implications for all decision support aimed at supporting complex decisions where the decision is not an isolated occurrence.']"
120,risk_management,1,148,120_water_management_water resources_decision,"['water', 'management', 'water resources', 'decision', 'decision support', 'dss', 'resources', 'basin', 'support', 'river', 'water supply', 'supply', 'planning', 'water management', 'groundwater', 'scenarios', 'watershed', 'river basin', 'aquifer', 'resources management', 'demand', 'water resource', 'integrated', 'water quality', 'systems', 'stakeholders', 'economic', 'criteria', 'reservoir', 'tool']","['a decision support system for sustainable urban water supply under rapidly growing water demand and the potential prolonged drought due to climate changes, a long-term sustainable water supply must be assured. this study develops a generally applicable decision support system (dss) tool as an aid in the planning of long-term water supply. the model incorporates multiple water sources (ex, ground water, surface water, natural and artificial recharge), users (ex, municipal, agricultural, industrial, and natural evapotranspiration), and water qualities (ex, raw water from sources, treated potable and reclaimed water). in addition, sustainability indicators are developed and applied in general water system to quantify future water supply sustainability based on multiple scenarios (representing future conditions of water supply and demand) utilizing the developed dss model. the dss model and sustainability quantification indices will support the public and stakeholders in decision making process through the scenario analysis representing potential future water conditions. the application of the dss model will be demonstrated through a case study on the regional water supply system in tucson, az. © 2011 asce.', 'a decision support system for integrated river basin management of the german elbe water resources management on the river-basin scale as requested by the european water framework directive is a highly complex task: not only the complex network of interdependencies of elements of the natural, ecological and socio-economical systems and their linkage but also the interests of various stakeholder groups must be taken into account. for this purpose a decision support system for integrated river basin management of the german part of the elbe river basin (elbe-dss) has been developed, which involves taking into account water quantity, chemical quality, and ecological status of surface waters. starting from identification of user needs by repeated consultation of stakeholders a list of management objectives, measures, and external scenarios turned out, which was taken as the basis for the dss development. a comprehensive system analysis was carried out to meet the various spatial and temporal scales when dealing with hydrologic, ecologic, economic, and social aspects related to water quantity, quality and ecological status (matthies et al. 2005). the system is build up by integrating only already existing models and data. system and software design are strongly oriented on management tasks: starting from selected management objectives the effects of external scenarios of climate, agro economic and demographic change, and selected measures to achieve the desired state of good water quantity and quality can be investigated. analysis tools are integrated to assist the user in evaluating the various management options. the system is implemented by using dss-generator geonamica® developed by research institute of knowledge systems (riks) (hahn and engelen 2000), which is also used in other dss projects (oxley et al. 2004). the implemented measures on catchment scale can be classified into the groups reduction of pollution from urban areas, modification of agricultural land allocation, changes in agricultural practices and political and legislative requirements concerning nutrient surplus. this paper focuses on water quality related questions on the catchment level. the effects of selected management options on the management objective reduction of nutrient loads are presented to demonstrate how the dss can be used for strategic management tasks as well as for participation and negotiation processes. the results indicate the different effects of each simulated measure on nitrogen and phosphorus inputs and concentrations in the river system. it could also be shown that efficiency of measures tend to show varying spatial patterns. the same holds for the simulated climate change scenarios where positive or negative effects depend on local and regional conditions. in the final version tools for economical evaluation of measures will also be implemented to assess cost effectiveness of management options.', 'managed aquifer recharge as a key element in sonora river basin management, mexico arid regions frequently have competing demands for scarce water mainly for urban supply and irrigation. this paper presents a decision support system (dss) as an instrument to find the optimal scheme for management of water resources from multiple supplies using managed aquifer recharge (mar). the dss capabilities are demonstrated through its application to the sonora river basin. the watershed is located in northern mexico and has a wide variety of agricultural zones and many small towns along the river. the most important urban center is the city of hermosillo with a concentrated water demand for urban and industrial uses. the dss includes the hydrological modeling of the entire system, stream flow in rivers, the groundwater-flow model including mar, and the characterization of the water quality from various sources. the result from this study establishes that it will be difficult to satisfy the increasing demand for water using the current supply-and-demand-allocation schemes. alternative sources are thus needed to mitigate the future difference between supply and demand. the mar with treated wastewater represents an option that could sustain development of urban water supply and irrigation within this basin. it is concluded that the best strategy (least groundwater-storage depletion) is to restrict irrigation and allocate all the effluent from the wastewater treatment plant (wwtp) to recharge the aquifer through infiltration lagoons. however, if there is not a limitation to irrigation, urban demand increases at 1.7% per year and without artificial recharge, the aquifers will be depleted in 25 years. alternatively, if the wwtp effluent is shared between agriculture and mar, it is possible to allow irrigation of 2,800 ha with small groundwater depletion. the dds also allows for studying the potential improvements in water quality and resilience to global climate change.']"
17,energy_power,2,45,17_charging_electric_battery_ev,"['charging', 'electric', 'battery', 'ev', 'electric vehicles', 'vehicles', 'vehicle', 'evs', 'electric vehicle', 'charge', 'power', 'ion', 'lithium', 'energy', 'batteries', 'state', 'energy storage', 'grid', 'cell', 'storage', 'energy management', 'placement', 'voltage', 'stations', 'vehicle charging', 'control', 'station', 'vehicles evs', 'ems', 'transportation']","['electric vehicle charging reservation under preemptive service electric vehicles (ev) are environment-friendly with lower co2 emissions, and financial affordability (in term of battery based refuel) benefits. here, when and where to recharge are sensitive factors significantly impacting the environmental and financial gains, these are still challenges to be tackled. in this paper, we propose a sustainable and smart ev charging scheme enables the preemptive charging functions for heterogeneous evs equipped with various charging capabilities and brands. our scheme intents to address the problems when evs are with various ownerships and priority, in related to the services agreed with charging infrastructure operators. particularly, the anticipated evs charging reservations information with heterogeneity (are multiscale) including their ev type, expected arrival time and charging waiting time at the charging stations (css), have been considered for design, planning and optimal decision making on the selection (i.e., where to charge) among the candidature css. we have conducted extensive simulation studies, by taking the realistic helsinki city geographical and traffic scenarios as an example. the numerical results have confirmed that our proposed preemptive approach is better than the first-come-first-serve (fcfs) based system, associated with its significant improvement on the reservation feature in ev charging.', 'electric vehicles charging management using deep reinforcement learning considering vehicle-to-grid operation and battery degradation evs are becoming more popular and widely used worldwide due to their environmentally friendliness as part of the world efforts to decrease the effects of climate change. moreover, more users are buying evs due to governmental incentives, development of charging technologies and cheaper maintenance costs. thus, the increased electrical loads on the distribution grid caused by the charging of evs can have negative impacts such as high voltage fluctuations, power losses and power overloads. thus, a power system management solution is required to protect the distribution grid from the harmful effects of evs charging through the regulation of the charging of evs. in this paper, a deep rl-based evs charging management solution is presented, while considering fast charging, conventional charging and v2g operation, in order to satisfy the requirements of the user and the utility. deep rl is utilized to model the ev chargers and the ev users. the ev chargers are considered the rl environment and the ev users are considered the rl agent. finally, the system was tested with a range of case studies using real-life evs charging data, which proved the effectiveness and reliability of the system to protect the distribution grid and satisfy the ev users charging requirements.', 'electric-vehicle energy management and charging scheduling system in sustainable cities and society plug-in electric vehicles (pevs) have gained the users attention due to their smart and cost-effective and environment-friendly services. with many benefits, the pevs services have suffered from different challenges like battery charging management, increasing electric charges, and availability of charging stations and battery life estimation. various different types of algorithms, deep learning and machine learning solutions, artificial intelligence solutions have been proposed for pevs. however, the existing solutions have focused the one or two components of pevs. to address these limitations, we propose an electric vehicle-intelligent energy management and chargings scheduling system (ev-emss) for charging station and pevs management system. the proposed system provides convenient energy management services by using battery control units and communication with charging stations for charging decisions. this system facilitates the drivers to take the best charging decision and communicate with charging stations for further decision. the proposed system has a secure mechanism to protect all the data from any unauthorized access. the results show the better performance of the proposed system in dense and sparse traffic conditions.']"
33,energy_power,2,45,33_energy_decision_decision support_renewable,"['energy', 'decision', 'decision support', 'renewable', 'energy systems', 'renewable energy', 'support', 'systems', 'solar', 'planning', 'based decision', 'sustainable energy', 'optimization', 'power', 'dss', 'energy sources', 'tool', 'development', 'environmental', 'policy', 'sources', 'geothermal', 'distributed energy', 'criteria', 'tidal', 'makers', 'sustainable', 'energy planning', 'gis', 'obstacles']","['research activities in renewable energy sources integration with gis at ciemat since 1994, ciemat has been involved in several research activities regarding the integration of renewable energy sources (res) as well as resource assessment based on geographic information technologies. one of the first projects was solargis which aimed to demonstrate the viability of applying gis to the res integration in rural electrification programs. this research line has been improved since then, through the permanent actualization and update of the methodology. in this sense, the collaboration with upm (polytechnic university of madrid) has been decisive. nowadays, ciemat gis team is applying this methodology to latin america countries like cuba or colombia, in the general framework of renewable energy integration gis project (inti-gis). in addition to gis application for rural electrification research line, our team has also accomplished resource assessments and siting studies for distributed electrical generation based on res. on this topic, ciemat has developed several projects like mersoterm, solbio and others related to biomass or wind resource assessment. finally, in order to promote res, our team has collaborated in the accomplishment of projects aimed to define a feasible technology mix based on renewables, like siger project, or decision support systems (dss) supporting the decision-making for a sustainable development based on high penetration of res in latin-american countries like colombia. according to our experience, we strongly believe that gis technology constitutes a powerful tool in order to promote and integrate renewable energy sources in the path of a new energy international model more respectful with the environment and also with the sustainable development of the society.', 'location support system for energy clusters management at regional level this study provides the location support system solution for the new polish energy policy to 2040. the location support system combing geographic information system (gis) with business intelligence (bi) analytic environment is developed. the decision support system in this research integrates three renewable energy sources (res): biomass, solar and wind. the renewable energy technical potentials are analyzed in relation to the local human development index (lhdi) and the average use of low-voltage electricity [kwh per capita] in rural households. the research indicates internal diversification of the country in terms of energy consumption, level of development and potential to renewable energy production. the most developed rural areas are in the west of the country and in the vicinity of large cities. regions suitable for biomass production are located in s-w and w part. the best conditions for solar energy sector are in the s, s-e and central regions. good wind conditions are in central poland and locally at the baltic coast as well as in sub mountainous regions in the south. the newly developed analytical system can be effective instrument, which can strengthen the production and consumption of renewable energy in rural areas. as an added value, it should improve the quality of life of local communities. the results of the study support decision makers in sustainable energy cluster allocation and management.', 'a decision support system to assess the feasibility of onshore renewable energy infrastructure this article introduces a new web-based decision support system created for early-stage feasibility assessments of renewable energy technologies in england, uk. the article includes a review of energy policy and regulation in england and a critical evaluation of literature on similar decision support systems. overall, it shows a novel solution for a repeatable, scalable digital evidence base for the policy compliant deployment of renewable energy technologies. data4sustain is a spatial decision support system developed to quickly identify the feasibility of seven renewable energy technologies across large areas including wind, solar, hydro, shallow and geothermal. a multi-actor approach was used to identify the key factors that influence the technical feasibility of these technologies to generate electricity or heat for local consumption or regional distribution. the research demonstrates opportunities to improve the links between policy and regulation with deployment of renewable energy technologies using novel approaches to digital planning. deployed, resilient, cost-effective and societally accepted renewable energy generation infrastructure has a role to play in ensuring universal access to affordable, reliableand modern energy supply. this is central to supporting a concerted transition to a low-carbon future in order to address climate change. the selection and siting of renewable energy technology is driven by natural resource availability and physical and regulatory constraints. these factors inform early-stage feasibility of renewables, helping to focus investment of time and money. understanding their relative importance and identifying the most suitable technologies is a highly complex task due to the disparate and often unconnected sources of data and information needed. data4sustain help to overcome these challenges.']"
41,energy_power,2,49,41_heat_hydrogen_energy_power,"['heat', 'hydrogen', 'energy', 'power', 'gas', 'heating', 'thermal', 'fuel', 'thermodynamic', 'optimization', 'performance', 'pump', 'renewable', 'temperature', 'efficiency', 'cycle', 'heat stress', 'cop', 'artificial', 'electricity', 'generation', 'heat transfer', 'turbine', 'solar', 'tank', 'design', 'unit', 'cost', 'artificial neural', 'refrigeration']","['thermo-mechanical optimization of thermoelectric generators using deep learning artificial intelligence algorithms fed with verified finite element simulation data the rising levels of global warming in the environment owing to emissions from fossil-fuel-based engines has increased the search for efficient clean energy systems. thermoelectric generators (tegs) standout as a promising energy conversion device which can directly convert heat to electricity. several optimization studies have been carried out on these devices to improve their power generation rate and efficiencies while guaranteeing long lifespan. however, the limitations of finite element methods (fems) in easily providing optimization guidelines at a fast rate has hindered the manufacture of tegs with high thermo-mechanical performance. this is why this paper presents the first ever artificial intelligence enabled optimization of a teg conducted via deep neural networks (dnns). previous research on this topic completely neglected the mechanical performance and consequently, the service lifetime of the teg when exposed to thermal operating conditions. to fill this gap, the effects of strategic parameters on the power output, efficiency and thermal stress performances of the teg are investigated. these parameters are the hot and cold junction temperatures/heat transfer coefficients, incident heat flux, external load resistance, te leg height, area, and area ratio. the dnn is fed with verified three-dimensional fem simulations carried out on the ansys workbench platform. the fem results exhibit almost perfect correlation with experimental data which establishes the precision of the model. results are that the dnn model is able to provide the necessary optimization guidelines for maximum thermo-mechanical performance in just 8 seconds compared to the 8 hours required by the fem. the tegs that were modelled using the dnn optimization guidelines improved the power, efficiency and mechanical performance of the unoptimized teg by 11.94%, 14.17%, and 91%, respectively. these results are sufficient to provide useful guidelines at a fast rate for the fabrication of high-efficiency tegs that will operate for a long time.', 'techno-environmental assessment and machine learning-based optimization of a novel dual-source multi-generation energy system the utilization of high-temperature hybrid energy systems has a vital and promising role in reducing environmental pollutants and coping with climate change. so, in the present research, a dual-source multigeneration energy system composed of a gas turbine, a supercritical carbon dioxide recompression brayton cycle, an organic rankine cycle, an absorption refrigeration system, and a reverse osmosis desalination unit is designed and analyzed from thermodynamic, environmental and economic perspectives. the system supplies power with a stable load to follow the changes in the demand side which is important for off-grid distributed energy systems. the dual-source operation of the system makes it possible to generate sustainable electricity leading to less utilization of fossil fuels in the gas turbine subsystem and reduction in environmental pollution, and furthermore, malfunctioning of a subsystem will not lead to the failure of the entire plant. three multi-objective optimizations with different objective functions are accomplished using artificial neural network from data learning and genetic and greywolf algorithms to obtain the best-operating conditions. under the base conditions, for the total input energy of 699 mw to the entire system, the energy and exergy efficiencies, the unit exergy cost of products, the carbon dioxide emission index, and the payback period, respectively, were found to be 45 %, 54 %, 15.3 $/gj, 112.2 kg/mwh, and 7.2 years. the net output power of the proposed system was calculated as 288.2 mw. a sensitivity analysis revealed that with a change in the pressure ratio of the supercritical carbon dioxide cycle, the net generated power and overall efficiency take maximum values of 293.9 mw while the unit exergy cost of products and carbon dioxide emission index take minimum values of 15.3 $/gj and 110.1 kg/mwh, respectively. furthermore, increasing the pressure ratio of the gas turbine leads to maximum values of 45 % and 54 % in overall energy and exergy efficiencies, respectively.', 'an artificial intelligence approach for thermodynamic modeling of geothermal based-organic rankine cycle equipped with solar system geothermal energy is a renewable resource that is constantly available. the low geothermal well operating lifetime is the main challenge in using this type of renewable energy. this problem can be covered by the aid of solar system (hybrid system). for complicated renewable energy systems, finding the optimum design parameters and operating conditions require to develop experimental apparatus or sophisticated thermodynamic models. hence, in this study, artificial intelligence (ai) approach is proposed for modeling the geothermal organic rankin cycle (gorc) equipped with solar thermal unit. indeed, the current study depicts how ai methods can meticulously simulate the operation of a complicated renewable energy system. the developed intelligent methods are adaptive neuro-fuzzy inference system (anfis) optimized with particle swarm optimization (pso) algorithm (anfis-pso) and multilayer perceptron (mlp) neural network optimized with pso algorithm (mlp-pso). the models are composed based on the main design parameters of the geothermal system that are solar radiation, well temperature, working fluid mass flow rate, turbine output pressure, surface area of the solar collector and preheater inlet pressure. the intelligent models use the mentioned input variables to predict the net power output, energy efficiency, exergy efficiency and levelized cost of energy (lcoe) of the gorc. energy, exergy and economic analyses are carried out for the low global warming potential (gwp) refrigerants. it was found out that although the intelligent models can meticulously predict the targets, anfis-pso performs better than mlp-pso for modeling the gorc with solar system. root mean square error of this model for prediction of power generation, energy efficiency, exergy efficiency and lcoe was 12.023 (kw), 3.587 ×10-4, 3.278 ×10-4 and 1.332 ×10-4, respectively.']"
59,energy_power,2,62,59_wind_wind power_power_wind speed,"['wind', 'wind power', 'power', 'wind speed', 'speed', 'wind energy', 'energy', 'turbine', 'turbines', 'wind turbines', 'generation', 'power generation', 'wind turbine', 'renewable', 'forecasting', 'renewable energy', 'short term', 'short', 'fatigue', 'offshore wind', 'offshore', 'fault', 'term', 'prediction', 'fossil', 'sources', 'neural', 'proposed', 'hybrid', 'energy sources']","['a comparative study of wind power forecasting techniques - a review article globally, the power demand is increasing very rapidly. to meet this demand, the traditional expansion of conventional and fossil fuels leads to global warming. also the long-term availability of the coal and other conventional energy sources is limited, which is one more reason for choosing renewable sources to generate electrical power. but, the major renewable energy sources especially wind energy is highly uncertain in its nature. in fact the high variability of wind power generation is affecting the power system operation. the integration of the renewable energy systems to grid is also an issue in terms of operation and control. the wind power forecasting has a major role in determining the size of operating reserves to balance the generation with load. to reduce the operating costs and to improve the reliability of the grid integrated to wind power systems, accurate wind power forecasting tools are necessary. this paper discusses the classification, various forecasting techniques and methods, performance evaluation factors etc. in forecasting of wind speed and wind power generation. this survey significantly shows the better performance by hybrid artificial intelligence models in terms of accuracy.', 'applying climate big-data to analysis of the correlation between regional wind speed and wind energy generation in an era of growing concern over climate change, several utility companies originally supplied wholesale and retail power mainly made by burning coal, have started to consider and build the clean-energy power systems for resolving global warming problems. wind power is nowadays regarded as one of the predominant alternative sources of clean energy. in this paper, we discuss our work on utilizing climate big-data associated with wind power, collected from several wind farms over four years, for exploring the correlation between regional wind speed and wind power. once this huge amount of data are analyzed, it can be used to develop policies for siting wind-power facilities, designing smart charging algorithms, or evaluating the capacity of electrical distribution systems to meet the actual requirement of power load. our work started with collecting related climate data for building data model to perform analytics work and experiments using support vector regression (svr) method. also, we observed the correlations between other factors related to wind speed and wind energy from our empirical model. the preliminary experimental results demonstrate that our developed system framework is workable, allowing for detailed analysis of the important wind-power related factors on specific wind farm regions.', 'wind power generation probabilistic modeling using ensemble learning techniques the generation and allocation of sustainable electrical energy in environment friendly, reasonable and socially acceptable manner is the utmost challenge. the primary objective is apparently a transition into an energy system to reduce fossil fuel dependency considering the global increasing energy demand, the scarcity of fossil fuel, and the serious environmental consequences of these fuels. the introduction of renewable energy in the power network is therefore essential. wind power is one of the worlds most extensively utilized sources of energy. the generation of wind power varies from traditional methods of generating electricity because, of the probabilistic nature of wind. therefore, in view of the instability of wind energy production, wind power forecasts play an important role in addressing the complexities of balance supply and demand in any power system. an accurate wind speed forecasts minimize the necessity of auxiliary energy balancing and reserve power to incorporate wind energy. this paper presents an accurate wind speed and wind power prediction methodology using ensemble machine learning algorithms.']"
75,energy_power,2,47,75_reinforcement learning_reinforcement_deep reinforcement_control,"['reinforcement learning', 'reinforcement', 'deep reinforcement', 'control', 'energy', 'agent', 'power', 'drl', 'grid', 'deep', 'optimization', 'strategy', 'systems', 'flexibility', 'optimal', 'multi agent', 'learning', 'dispatch', 'renewable', 'operation', 'cost', 'renewable energy', 'learning drl', 'control strategy', 'based', 'microgrid', 'problem', 'proposed', 'bus', 'ieee']","['data-driven battery operation for energy arbitrage using rainbow deep reinforcement learning as the world seeks to become more sustainable, intelligent solutions are needed to increase the penetration of renewable energy. in this paper, the model-free deep reinforcement learning algorithm rainbow deep q-networks is used to control a battery in a microgrid to perform energy arbitrage and more efficiently utilise solar and wind energy sources. the grid operates with its own demand and renewable generation, as well as dynamic energy pricing from a real wholesale energy market. four scenarios are tested including using demand and price forecasting produced with local weather data. the algorithm and its subcomponents are evaluated against an actor-critic method and a linear programming model with rainbow able to outperform all other methods. this research shows the importance of using the distributional approach for reinforcement learning for complex environments, as well as how it can visualise and contextualise the agents’ behaviour for real-world applications.', 'multi-agent deep reinforcement learning based distributed control architecture for interconnected multi-energy microgrid energy management and optimization environmental and climate change concerns are pushing the rapid development of new energy resources (ders). the energy internet (ei), with the power-sharing functionality introduced by energy routers (ers), offers an appealing alternative for der systems. however, previous centralized control schemes for ei systems that follow a top-down architecture are unreliable for future power systems. this study proposes a distributed control scheme for bottom-up ei architecture. second, model-based distributed control methods are not sufficiently flexible to deal with the complex uncertainties associated with multi-energy demands and ders. a novel model-free/data-driven multiagent deep reinforcement learning (madrl) method is proposed to learn the optimal operation strategy for the bottom-layer microgrid (mg) cluster. unlike existing single-agent deep reinforcement learning methods that rely on homogeneous mg settings, the proposed madrl adopts a form of decentralized execution, in which agents operate independently to meet local customized energy demands while preserving privacy. third, an attention mechanism is added to the centralized critic, which can effectively accelerate the learning speed. considering the bottom-layer power exchange request and the predicted electricity price, model predictive control of the upper layer determines the optimal power dispatching between the ers and main grid. simulations with other alternatives demonstrate the effectiveness of the proposed control scheme.', 'data-driven sustainable distributed energy resources’ control based on multi-agent deep reinforcement learning with the ongoing energy transition, electric power and energy systems are becoming increasingly multi-dimensional and complex with higher levels of uncertainty. obtaining an accurate system model or multiple predictions in such environments is becoming increasingly problematic. such features are challenging aspects to the classical power system control approaches which are essentially model-based and rely on accurate predictions. reinforcement learning is promising a data-driven alternative that can efficiently tackle the raising complexity of controlling such systems with no prior system dynamics modeling or predictions. this work proposes a multi-agent deep reinforcement learning-based control framework for optimally solving multi-dimensional power dispatch problems in systems featuring multiple uncertainties. learned control strategies are based on centralized training decentralized execution to promote an efficient and robust coordination among different dispatchable assets with no communication burden. experimental results of various typical power dispatch scenarios with significant integration of low-carbon generation demonstrate the effectiveness of such control strategies.']"
85,energy_power,2,38,85_appliances_energy_load_consumption,"['appliances', 'energy', 'load', 'consumption', 'fault', 'grid', 'diagnosis', 'disaggregation', 'home', 'smart', 'energy management', 'electricity', 'electrical', 'energy consumption', 'residential', 'power', 'usage', 'intrusive', 'monitoring', 'non intrusive', 'fault diagnosis', 'classification', 'faults', 'svm', 'meters', 'insulation', 'smart grid', 'power quality', 'loads', 'detection']","['feature mapping based deep neural networks for non-intrusive load monitoring of similar appliances in buildings energy management plays an important role in the smart sustainable cities development programme to utilise energy resources in a responsible manner for conserving the environment and improving well-being of the society. building sector is one of the major sectors that consumes more energy in commercial and residential buildings. recently, non-intrusive load monitoring technique (nilm) has become popular among the researchers for its capability in disaggregation of energy at appliance/load level from the measured aggregated energy. appliance signatures are learned using machine learning and deep learning approaches for effective detection of appliance events and energy consumption. however, appliance detection becomes challenging when appliances in the electrical network are similar or same type. therefore, effective feature learning methodologies need to be developed for distinguishing the events of similar loads more accurately. in this paper, we used the open source dataset1 that consists of fundamental electrical features extracted from the four fluorescent lamps having same technical specifications. from the preliminary experiments, it is observed that the baseline system performance with the support vector machine (svm) and deep neural networks (dnn) is not much encouraging due to the overlapping and nonlinear characteristics of similar loads. to overcome this problem, we expresses the original feature vectors in terms of appliance independent basis vectors in a higher dimensional space using a feature mapping technique, locality constrained linear coding (llc) and then used machine learning classifiers for similar load identification. from the experiments and results, it is observed that feature mapping based deep neural networks (llc-dnn) outperforms the baseline, llc-svm and other reported approaches from the literature significantly for similar appliances detection in nilm system.', 'energy disaggregation via clustered regression models: a case study in the convenience store global warming and the depletion of natural resources are two of the most difficult problems we have ever faced. to address this problem, people have begun paying more attention to carbon emission reduction and energy saving. for the residential electricity use, many studies have demonstrated that feedbacks, such as energy consumption of each appliance in the home, can help consumers reduce electricity consumption usage. in this article, we propose a novel framework for the disaggregation of energy consumption, which is looking forward to reaching reducing the number of smart meters installed and providing usage statistics as a feedback for consumers to decrease their energy cost. in our proposed framework, we have a chief meter which measures total energy consumption, and install smart meters at few key appliances. based the energy consumption from these meters, we proposed a clustered regression models for energy disaggregation. more specifically, we first cluster appliances by the correlation between the using behavior of appliances, and select one of them as the key appliance in each cluster. by using the appliance with installed meter, we apply regression model to estimate the energy consumption for other appliances within each cluster. our experimental results confirmed our proposed framework can achieve high accuracy for energy disaggregation while reducing the number of smart meters. © 2013 ieee.', 'non-intrusive load monitoring: a promising path to the society for responsible energy utilization and sustainability responsible use of energy resources, conserving environment and improving well-being of the society are the major goals towards developing smart sustainable cities, and energy management is the inherent part of the smart sustainable cities programme. due to high demand of energy in various sectors, energy management has become essential to utilize the available energy efficiently and understand energy usage behavior of the users in order to avoid possible energy wastage. in recent years, non-intrusive load monitoring (nilm) technique has become popular and emerging approach to monitor events and energy consumption of appliances/electrical utilities in buildings using single energy meter. the information about energy consumption at appliance level would help consumers to understand their appliance usage behavior and take necessary steps for reducing energy consumption. the recent advancements in smart metering and internet of things (iot) have made significant progress in real time implementation of non-intrusive load monitoring (nilm) algorithms in residential and industrial environments. the motivation of this presentation is to address the challenges towards developing more accurate nilm systems and its future perspectives for effective energy management in buildings. nilm can also be used for anomaly detection, predictive maintenance, load forecasting, and energy disaggregation of distributed energy resources in buildings. therefore, creating awareness about the importance of nilm and incorporating its potential applications as the features of nilm system would help achieve responsible energy utilization in residential/industrial environments and conserve energy consumption towards developing smart sustainable cities for the society.']"
86,energy_power,2,116,86_solar_pv_photovoltaic_energy,"['solar', 'pv', 'photovoltaic', 'energy', 'power', 'solar energy', 'renewable', 'generation', 'renewable energy', 'panels', 'solar power', 'irradiance', 'forecasting', 'power generation', 'pv power', 'photovoltaic pv', 'solar irradiance', 'output', 'electricity', 'solar radiation', 'sources', 'radiation', 'energy sources', 'pv systems', 'solar photovoltaic', 'solar panels', 'prediction', 'energy generation', 'energy production', 'learning']","['machine-learned models for the performance of six different solar pv technologies under the tropical environment due to the recent environmental concerns and long-term challenges in energy security, the global energy scenarios are shifting more towards sustainable and renewable energy resources. brunei has planned to increase the use of cleaner energy technologies by contributing 10 percent or 954 gwh of renewable energy in its power generation mix by 2035. out of the available renewable options, solar is the most promising one for brunei, for example, the daily average solar installation is around 5kwh per day [1]. though solar energy is an abundant resource, for optimally designing and successfully managing solar power projects, its availability in different time scales are to be analyzed and understood in a local context. in this paper, we present models for estimating the output of six different solar pv technologies using machine learning methods performance data from the solar pv systems installed at the tenaga suria pv plant in brunei are used to develop the models. influence of relevant environmental parameters, such as irradiance, relative humidity, ambient temperature and wind speed on the power output has been analyzed for optimal feature selection. performance models based on support vector machine (svm) and k-nearest neighbour (knn) were developed and tested for predicting the pv system performance. both the models could estimate the system performance with reasonably high level of accuracy.', 'a novel hybrid spatio-temporal forecasting of multisite solar photovoltaic generation currently, the world is actively responding to climate change problems. there is significant research interest in renewable energy generation, with focused attention on solar photovoltaic (pv) generation. therefore, this study developed an accurate and precise solar pv generation prediction model for several solar pv power plants in various regions of south korea to establish stable supply-and-demand power grid systems. to reflect the spatial and temporal characteristics of solar pv generation, data extracted from satellite images and numerical text data were combined and used. experiments were conducted on solar pv power plants in incheon, busan, and yeongam, and various machine learning algorithms were applied, including the sarimax, which is a traditional statistical time-series analysis method. furthermore, for developing a precise solar pv generation prediction model, the sarimax-lstm model was applied using a stacking ensemble technique that created one prediction model by combining the advantages of several prediction models. con-sequently, an advanced multisite hybrid spatio-temporal solar pv generation prediction model with superior performance was proposed using information that could not be learned in the existing single-site solar pv generation prediction model.', 'machine learning and analytical model hybridization to assess the impact of climate change on solar pv energy production the united nations (un) sustainable development goals (sdgs) agenda has triggered numerous countries to harness solar energy from solar photovoltaic (pv) modules to increase the share of renewable energy in the global energy mix. however, geographical and climatic factors have a significant impact on the electrical performance of solar pv modules. in addition, since solar pv energy production models are the only physics-based approach to transferring ground-measured pv energy production to other locations, the authors developed 294 physical models from six different pv power technologies and validated them for the models adaptability. to facilitate the possible determination of pv electric energy generation in the unique geographical and climatic environment of the experiment site, these models were built using machine learning, gumbels probabilistic approach (gpm), and hybridization of the two. the major challenges in this study are in developing the hybridized machine learning with the gumbel probabilistic functional model, which resides in the mathematical transformation process, which required a great deal of repeated mathematical science knowledge to arrive at the final transformed and efficient model for predicting the potential of solar pv output. with a thorough coefficient of determination (r2) of 0.9998% and a root mean square error (rmse) of 0.0063 kwh, the hybrid model with only the measurable solar radiation parameter is the closest to the measured pv energy production of all technologies. the best hybridized model was used to explore the potential impacts of climate change on the different solar pv technologies. this was achieved by using energy parameters from the australian community climate and global system simulation (access-cm2) in phase 6. on an annual basis, the effects of climate change on various pv technologies have had a small adverse impact (less than 1%) on these renewable energy technologies. it was also found that, compared to other technologies, cigs thin film technology produced the least negative effects on climate change, with 10.94%–36.75% in the best-case, 35.71%–36.36% in the moderate-case, and 33.33–40.00% in the worst-case scenario for shared socioeconomic pathways (ssp126, ssp245, and ssp585) emissions. this suggests the intrinsic properties of copper indium gallium selenide (cigs) thin film modules are more effective at withstanding high temperatures as they contribute 60.00–89.66% of their intrinsic module properties to pv energy production compared to other technologies. however, taking into account the time, resource availability, cost-effectiveness, commercialization, and consumption of various pv technologies studied in this era of global sustainability, poly-crystalline (p-si) technology is highly recommended for harvesting solar pv energy products in alice springs, australia.']"
87,energy_power,2,52,87_energy_renewable_power_renewable energy,"['energy', 'renewable', 'power', 'renewable energy', 'demand', 'generation', 'grid', 'electricity', 'energy systems', 'systems', 'sources', 'sector', 'energy sector', 'market', 'integration', 'smart', 'energy sources', 'power systems', 'ai', 'smart grid', 'technologies', 'new', 'storage', 'integrated', 'trading', 'sustainable', 'article', 'operation', 'development', 'peer']","['using genersys to model electricity generation expansion in an environment of increasing complexity, with new electricity generation technologies, climate change, carbon pricing, and fluctuating fuel prices, electricity generation expansion planners need tools which facilitate the integration and synthesis of a wide range of complex and often uncertain information to support decision making for investment. this paper presents a capacity expansion algorithm integrated in genersys - an agent-based simulation model for electricity and gas markets. the algorithm evaluates different electricity generation technologies for investment in new generating plants in different regions of the market as a response to growing electricity demand, unserved energy and/or high wholesale prices of electricity. it considers capacity factors associated with peak, intermediate and base load generation, technology and fuel costs, carbon price, and distribution of recent electricity prices. as a fully integrated simulation algorithm within genersys, it can be used to assess complex investment scenarios defined by demand models, bidding behaviour of generators, non-scheduled renewable generation, outage models and all other components modelled by this simulation tool. the algorithm creates modular expansions - a single generating unit with predefined generation capacity of the best ranked generation technology in a region will be invested in if the investment criteria are satisfied. generating plants newly created by the capacity expansion algorithm begin to participate in the market, producing and selling electricity. they will have immediate impact on the balance between demand and supply and ultimately will influence the electricity market prices as well. the capacity expansion algorithm can be used to evaluate investment decisions in a mix of generation technologies, including renewable technologies such as wind and big solar generation. in this paper, a case study illustrates the capabilities of the genersys capacity expansion algorithm. genersys successfully models the complexities and uncertainties in an integrated way facilitating better generation expansion planning.', 'artificial intelligence to support the integration of variable renewable energy sources to the power system the power sector is increasingly relying on variable renewable energy sources (vre) whose share in energy production is expected to further increase. a key challenge for adopting these energy sources is their high integration costs. artificial intelligence (ai) solutions and data-intensive technologies are already used in different parts of the electricity value chain and, due to the growing complexity and data generation potential of the future smart grid, have the potential to create significant value in the system. however, different uncertainties or lack of understanding about its impact often hinder the commitment of decision makers to invest in ai and data intensive technologies, also in the energy sector. while previous work has outlined a number of ways ai solutions can be used in the power sector, the goal of this article is to consider the value creation potential of ai in terms of managing vre integration costs. we use an economic model of variable renewable integration cost from the literature to present a systematic review of how ai can decrease substantial integration costs. we review a number of use cases and discuss challenges estimating the value creation of ai solutions in the power sector.', 'an integrated framework to better fit future energy systems-clean energy systems based on smart sector coupling (ensysco) in order to avoid the severe climate changes caused by a large amount of greenhouse gas emissions from bringing destructive impact on global development, most countries are rapidly increasing the proportion of renewable resources in their energy production to accelerate the energytransition and realize the commitment of achieving carbon neutrality by the middle of this century. however, the legacy system did not consider the large-scale integration of renewable resources at the beginning of its formation. therefore, it is often vulnerable to the intermittent, fluctuating and random characteristics of such resources. in this paper, based on the summing upand development of the smart grid and the sector coupling concept of energy utilization raised by germany, and taking into account the widespread adoption of large-scale underground storage and ai-based (artificial intelligence) monitoring, analysis and forecasting, an integrated future framework named clean energy systems based on smart sector coupling (ensysco) is proposed. firstly, ensysco closely couples the three sectors of energys production, consumption and storage through applying the power-to-x-to-power techniques. the introduction of large-scale underground storage on one hand greatly improves the redundancy and flexibility of the system, on the other hand enables the regions to obtain a more abundant and stable energy reserve. secondly, various functional complexes within the future energy system lead to more complex supply-demand relationships, and at the same time, there is an urgent request for proper transport grids. the lightweight ai method driven by the hybrid of physics and data not only endows the governance system powerful analysis, decision-making and feedback capabilities, but also makes the operation of the entire system more efficient, robust and energy-saving. finally, most of the techniques in the ensysco framework, e.g. the pumped-storage hydroelectricity in mines (pshm), are mature enough and can be put into industrial applications immediately. further research on the regenerative enhanced geothermal system (regs) and the ai governance will help ensysco to promote chinas achievement of carbon neutrality more effective.']"
88,energy_power,2,23,88_optimization_dispatch_power_emission,"['optimization', 'dispatch', 'power', 'emission', 'colony', 'algorithm', 'ant', 'objective', 'ant colony', 'problem', 'renewable', 'swarm', 'sizing', 'hybrid', 'microgrid', 'particle', 'multi objective', 'solution', 'solve', 'energy', 'colony optimization', 'cost', 'multi', 'renewable energy', 'load', 'economic', 'particle swarm', 'generation', 'paper', 'costs']","['a novel seeker optimization approach for solving combined economic and emission dispatch over the past 50 years the average global temperature has increased at the fastest rate in record history. the main reason behind this is emission of carbon dioxide from thermal power plants. the u.s only produces 2.5 billion tons every year. this paper solves the problem of emission dispatch including economic dispatch combining as combined economic dispatch and emission dispatch (ceed). economic dispatch gives a solution to reduce the total fuel cost while emission dispatch is to reduce the emission level with the required system constraints. in this paper particle swarm optimization is used to solve ceed on a test system and the results are compared with ant colony optimization (aco) algorithm. © 2013 ieee.', 'an emission constraint economic load dispatch problem solution with microgrid using jaya algorithm in this work, the distributed energy resources (ders) are used in a specific small area which is known a microgrid. microgrid consists of microsources like distribution generator, solar and wind units, etc., and different loads. in the microgrid, the energy management system (ems) having a problem of combined economic emission dispatch (ceed) and it is optimized by meta-heuristic techniques. the ceed is the procedure to scheduling the generating units within their bounds together with minimizing the fuel cost and emission values. the jaya algorithm is applied for the solution of ceed problem in the matlab environment. the minimization of total cost and total emission are obtained for all sources included. the result shows the comparison of jaya algorithm with the gradient method (gm), ant colony optimization (aco) and particle swarm optimizer (pso) technique for the two different cases which are economic load dispatch (eld) without emission and with emission. the results are calculated for different power demand of 24 hours. the results obtained with jaya algorithm gives comparative better cost reduction as compared to gm, aco and pso which shows the effectiveness of the given algorithm. the key objective of this work is to solve the ceed problem to obtained optimal system cost.', 'a multi-objective multi-population ant colony optimization for economic emission dispatch considering power system security with increasing concern about global warming and haze, environmental issue has drawn more attention in daily optimization operation of electric power systems. economic emission dispatch (eed), which aims at reducing the pollution by power generation, has been proposed as a multi-objective, non-convex and non-linear optimization problem. in a practical power system, the problem of eed becomes more complex due to conflict between the objectives of economy and emission, valve-point effect, prohibited operation zones of generating units, and security constraints of transmission networks. to solve this complex problem, an algorithm of a multi-objective multi-population ant colony optimization for continuous domain (mmaco_r) is proposed. mmaco_r reconstructs the pheromone structure of ant colony to extend the original single objective method to multi-objective area. furthermore, to enhance the searching ability and overcome premature convergence, multi-population ant colony is also proposed, which contains ant populations with different searching scope and speed. in addition, a gaussian function based niche search method is proposed to enhance distribution and accuracy of solutions on the pareto optimal front. to verify the performance of mmaco_r in different multi-objective problems, benchmark tests have been conducted. finally, the proposed algorithm is applied to solve eed based on a six-unit system, a ten-unit system and a standard ieee 30-bus system. simulation results demonstrate that mmaco_r is effective in solving economic emission dispatch in practical power systems.']"
89,energy_power,2,23,89_power_energy_pv_solar,"['power', 'energy', 'pv', 'solar', 'renewable', 'photovoltaic', 'controller', 'renewable energy', 'control', 'dc', 'hybrid', 'tracking', 'maximum', 'grid', 'wind', 'sources', 'generation', 'voltage', 'floating', 'generator', 'load', 'point', 'matlab', 'energy using', 'source', 'speed', 'conventional', 'connected', 'energy management', 'simulink']","['cleaner energy for sustainable future using hybrid photovoltaics-thermoelectric generators system under non-static conditions using machine learning based control technique in addition to the load demand, the temperature difference between the hot and cold sides of the thermoelectric generator (teg) module determines the output power for thermoelectric generator systems. maximum power point tracking (mppt) control is needed to track the optimal global power point as operating conditions change. the growing use of electricity and the decline in the use of fossil fuels have sparked interest in photovoltaic-teg system utilization in the energy sector. thermoelectric generation systems are meant to recover waste heat as a green energy supply. concentrated solar can overcome the drawbacks of inefficient power generation. the feasibility of employing a machine learning and metaheuristic-based control strategy to yield maximum power from a hybrid photovoltaic and thermoelectric generator system under various operating situations is examined in this study. the output of both teg and pv modules is affected by the environment; pv panels create heat as a result of shade and wind speed. maximum energy harvesting of pv-teg under non-uniform temperature settings is proposed in this paper using a feed-forward neural network (ffnn) trained by a squirrel search optimization (sqs). teg systems have several local maxima due to this non-uniform state. mppt algorithms based on gradients are unlikely to discover actual gmpp in the majority of cases. the unique sqsffnn is evaluated under non-uniform temperature distribution and variable load and temperature circumstances as a possible answer to this non-linear issue. certain advances are made in this study by addressing concerns of global maximum power point tracking with non-uniform temperature distribution, low efficiency, higher settling and tracking time, and oscillations. particle swarm optimization, cuckoo search optimization (csa), csa-ffnn, and grey wolf optimization algorithms are compared to the outcomes. four experiments are carried out under various meteorological situations. experiments and matlab/simulink are used to validate and prove the results. the experimental results, comparisons with existing techniques, and statistical data show that the suggested sqdffnn technique achieves a greater performance, distinguishing pv-teg as a cleaner source of electrical power generation.', 'maximum power point tracking in a photovoltaic system by optimized fractional nonlinear controller increasing global warming due to using fossil fuels and non-renewable energy for power generation has become one of the most critical problems of this century. solar cells are one of the foremost promising electrical vitality sources, but their efficiency is limited due to different environmental conditions. for making strides productivity, the maximum power point tracking (mppt) strategy is utilized to extricate the most power of the photovoltaic system (pv). to realize the mpp of the solar cell, a novel ant colony optimization-based fractional-order back-stepping sliding mode controller (fobsmc) is proposed in this study. the proposed controller uses the po strategy to control the pv voltage to reach the mpp of pv systems. finally, the efficiency of the suggested strategy is compared with that of a traditional back-stepping sliding mode controller.', 'design and simulation of grid-connected photovoltaic system’s performance analysis with optimal control of maximum power point tracking based on artificial intelligence the research presented in this paper is part of a more significant effort to improve the dynamic and static performance of power generation systems using solar panels under specific climatic conditions. the solar panel can produce the greatest power only at the specified voltage and electric current levels. environmental variables, such as random atmospheric oscillations, have a significant effect on the performance of a solar system connected to the network. irradiation and ambient temperature are the two inputs to a pv (photovoltaic) system. because solar radiation varies in nature, the pv system efficiency is low. to improve the efficiency of a solar pv system, several maximum power point tracking (mppt) approaches are used. the purpose of this paper is to improve the performance of dc/dc chopper controllers and pv inverters in the face of abrupt climate change. to that end, the primary goal of this paper is to compare the following maximum power point search (mppt) algorithms: the incremental conductance (ic) algorithm, the fuzzy logic (fl) algorithm, vsi controller, and the particle swarm optimization (pso) strategy. these algorithms were evaluated in terms of efficiency, stability, and speed. a 100 kw pv system is design using matlab software 2021a version.contribution/originality: the papers primary contribution is in finding the most appropriate model that treats the problem of mppt of grid-connected photovoltaic to achieve maximum power-using four techniques of artificial intelligence.']"
92,energy_power,2,55,92_load_electricity_forecasting_demand,"['load', 'electricity', 'forecasting', 'demand', 'consumption', 'energy', 'power', 'lstm', 'short term', 'term', 'energy consumption', 'short', 'electric', 'electricity consumption', 'recurrent', 'prediction', 'long short', 'memory', 'term memory', 'long', 'electrical', 'deep learning', 'rnn', 'deep', 'time', 'price', 'neural', 'time series', 'error', 'forecast']","['short-term electric load forecasting using an emd-bi-lstm approach for smart grid energy management system electricity is an essential resource for human production and survival. accurately predicting electrical load consumption can help power supply companies make informed decisions, such as peak load shifting, to maintain a reliable power supply and reduce co2 emissions. however, forecasting electricity consumption is challenging due to the nonlinear and nonstationary time series data that is correlated with climate change. to address this challenge, this paper proposes an electricity forecasting method based on empirical mode decomposition (emd) and bidirectional lstm. emd is a solid and robust instrument for time–frequency analysis and signal preprocessing, which separates the time series into components at different resolutions. the proposed model predicts the future 24 h with a resolution of 15 min by creating many stationary component sequences from the original stochastic electricity usage time series data (imfs). to predict each intrinsic mode function, a hybrid model bi-lstm is employed. the results of each components forecast are then merged to give the overall forecast. two comparative studies are conducted to justify the choice of the signal processing method and the prediction algorithm. the proposed model demonstrates a minimal mape of 0.28% and a better r2 close to 1 of 0.84 compared to other papers.', 'electricity consumption forecast based on empirical mode decomposition and gated recurrent unit hybrid model electricity is one of the important needs in human production and life. the prediction of user power consumption can help power supply enterprises to analyze users electricity consumption behavior, provide personalized services for users and formulate effective peak load shifting power supply scheme, which is very important for decision-making and demand response of power management side. as the daily electricity consumption data of users is nonlinear and nonstationary time series data, coupled with its susceptibility to climate change, social activities and other random factors, making electricity consumption forecast is a very challenging demand. at present, many deep learning models, such as recurrent neural network (rnn) and long short-term memory (lstm) have been applied in electricity consumption forecasting and achieved good results. however, the direct use of these models cannot fully take into account the nonstationary characteristics of electricity data, and there is still room for improvement in the prediction effect. in this paper, a hybrid model of empirical mode decomposition (emd) and gated recurrent unit (gru) is proposed to predict user electricity consumption. first, the original nonstationary electricity consumption time series data is decomposed into multiple stationary component sequences through emd, then each component is predicted through a multi-layer gru network, and finally the prediction results of each component are combined to obtain the final forecast results. experimental results show that, compared with the direct use of lstm, the proposed model can effectively reduce the error, achieve a better fitting effect, and improve the training efficiency to a certain extent.', 'medium-term regional electricity load forecasting through machine learning and deep learning due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. to address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (svm) and random forest (rf), and deep learning methods such as non-linear auto-regressive exogenous (narx) neural network and recurrent neural networks (long short-term memory—lstm). a dataset including nine years of historical load demand for bruce county, ontario, canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. the results show that by employing deep learning, the model could predict the load demand more accurately than svm and rf, with an r-squared of about 0.93–0.96 and mean absolute percentage error (mape) of about 4–10%. the model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home-and district-scale renewable energy technologies.']"
96,energy_power,2,40,96_consumption_energy_electricity_electricity consumption,"['consumption', 'energy', 'electricity', 'electricity consumption', 'energy consumption', 'smart', 'usage', 'residential', 'demand', 'energy management', 'smart grid', 'household', 'energy demand', 'grid', 'peak', 'appliances', 'energy usage', 'forecasting', 'homes', 'zones', 'consumers', 'peak load', 'electricity demand', 'load', 'home', 'city', 'use', 'clustering', 'households', 'machine learning']","['smart grid data analytics framework for increasing energy savings in residential buildings human energy consumption has gradually increased greenhouse gas concentrations and is considered the main cause of global warming. currently, the building sector is a major energy consumer, and its share of energy consumption is increasing because of urbanization. this paper presents a framework for smart grid big data analytics and components required for an energy-saving decision-support system. the proposed system has a layered architecture that includes a smart grid, a data collection layer, an analytics bench, and a web-based portal. a smart metering infrastructure was installed in a residential building to conduct an experiment for evaluating the effectiveness of the proposed framework. furthermore, a novel hybrid nature-inspired metaheuristic forecast system and a dynamic optimization algorithm are designed behind the analytics bench for achieving accurate prediction and optimization of future energy consumption. the main contribution of this study is that an innovative framework for the energy-saving decision process is presented; the framework can serve as a basis for the future development of a full-scale smart decision support system (sdss). through the identification of consumer usage patterns, the sdss is expected to enhance energy use efficiency and improve the accuracy of future energy demand estimates. end users can reduce their electricity costs by implementing the optimal operating schedules for appliances, which are provided by the sdss.', 'intelligent data analysis for sustainable smart grids using hybrid classification by genetic algorithm based discretization smart grids, or intelligent electricity grids that utilize modern it/communication/control technologies, become a global trend nowadays. smart grids which enable two-way communication and monitoring between service providers and end-users need novel computational intelligent algorithms for supporting generation of power from wide range of sources, efficient energy distribution, and sustainable consumption. sustainability is of great importance due to increasing demands and limited resources. many problem classes in sustainable energy systems are data mining, optimization, and control tasks. the aim of this paper is to focus on the existing electricity generation infrastructure, electricity consumption behavior of the consumers and the need for smart grid. the various methods that have been concentrated on are that of machine learning and data mining techniques that can be mapped to these smart grid environments. we use publicly available smart grid datasets such as: residential electricity consumption survey (recs) dataset conducted in us; us smart home microgrid dataset; reference energy disaggregation dataset (redd) and almanac of minutely power (ampds) aggregation dataset in our analysis in order to optimize the energy consumption for sustainability. we utilize gaussian process regression with radial basis function (rbf) kernel, best first tree (bftree) and ordered weighted average fuzzy-rough k-nearest neighbor (owaknn) with equal width (ewd) and genetic algorithm based discretization (gad) in our approach to predict and forecast the consumer behavior in electricity consumption. the result obtained in terms of errors will be an ingredient to make effective decisions for developing a sustainable smart grid infrastructure.', 'automated prediction system of household energy consumption in cities using web crawler and optimized artificial intelligence the supply of electrical energy is critical to convenient and comfortable living. however, people consume a large amount of energy, contributing to an energy crisis and global warming, and damaging some ecological cycles. residential electricity consumption has greater elasticity than industrial and business consumption; it therefore has high energy-saving potential. this work establishes an automated platform, which provides information about residential electricity consumption in each city in taiwan. machine learning was used to forecast future residential electricity demand. a nature-inspired optimization method was applied to enhance the accuracy of the best machine learner, yielding an even better hybrid ensemble model. performance measures indicate that the resulting model is accurate and provides effective information for reference. an automatic web-based system based on the model was combined with a web crawler and scheduled to run automatically to provide information on monthly residential electricity consumption in each county and city. by providing energy consumption information across the country, power providers and government can discuss policy and set different goals for energy use. the results of this study can facilitate the early implementation of energy-saving and carbon emission-reducing in cities and aid utility companies in establishing energy conservation guidelines.']"
1,health,3,153,1_disease_health_covid_covid 19,"['disease', 'health', 'covid', 'covid 19', 'cases', 'incidence', 'transmission', 'virus', 'diseases', '19', 'risk', 'infection', 'surveillance', 'patients', 'outbreaks', 'outbreak', 'climate', 'vector', 'distribution', 'factors', 'pandemic', 'heat', 'borne', 'medical', 'temperature', 'infectious', 'ae', 'climatic', 'prediction', 'public health']","['a sustainable advanced artificial intelligence-based framework for analysis of covid-19 spread the idea of sustainability aims to provide a protected operating environment that supports without risking the capacity of coming generations and to satisfy their demands in the future. with the advent of artificial intelligence, big data, and the internet of things, there is a tremendous paradigm transformation in how environmental data are managed and handled for sustainable applications in smart cities and societies. the ongoing covid-19 (coronavirus disease) pandemic maintains a mortifying impact on the world population’s health. a continuous rise in the number of positive cases produced much stress on governing organizations worldwide, and they are finding it challenging to handle the situation. artificial intelligence methods can be extended quite efficiently to monitor the disease, predict the pandemic’s growth, and outline policies and strategies to control its transmission or spread. the combination of healthcare, along with big data, and machine learning methods, can improve the quality of life by providing better care services and creating cost-effective systems. researchers have been using these techniques to fight against the covid-19 pandemic. this paper emphasizes on the analysis of different factors and symptoms and presents a sustainable framework to predict and detect covid-19. firstly, we have collected a data set having different symptoms information of covid-19. then, we have explored various machine learning algorithms or methods: including logistic regression, naive bayes, decision tree, random forest classifier, extreme gradient boost, k-nearest neighbour, and support vector machine to predict and detect covid-19 lab results, using different symptoms information. the model might help to predict and detect the long-term spread of a pandemic and implement advanced proactive measures. the findings show that the logistic regression and support vector machine outperformed from other machine learning algorithms in terms of accuracy; algorithms exhibit 97.66% and 98% results, respectively.', 'european projections of west nile virus transmission under climate change scenarios west nile virus (wnv), a mosquito-borne zoonosis, has emerged as a disease of public health concern in europe. recent outbreaks have been attributed to suitable climatic conditions for its vectors favoring transmission. however, to date, projections of the risk for wnv expansion under climate change scenarios is lacking. here, we estimate the wnv-outbreaks risk for a set of climate change and socioeconomic scenarios. we delineate the potential risk-areas and estimate the growth in the population at risk (par). we used supervised machine learning classifier, xgboost, to estimate the wnv-outbreak risk using an ensemble climate model and multi-scenario approach. the model was trained by collating climatic, socioeconomic, and reported wnv-infections data (2010−22) and the out-of-sample results (1950–2009, 2023–99) were validated using a novel confidence-based performance estimation (cbpe) method. projections of area specific outbreak risk trends, and corresponding population at risk were estimated and compared across scenarios. our results show up to 5-fold increase in west nile virus (wnv) risk for 2040-60 in europe, depending on geographical region and climate scenario, compared to 2000-20. the proportion of disease-reported european land areas could increase from 15% to 23-30%, putting 161 to 244 million people at risk. across scenarios, western europe appears to be facing the largest increase in the outbreak risk of wnv. the increase in the risk is not linear but undergoes periods of sharp changes governed by climatic thresholds associated with ideal conditions for wnv vectors. the increased risk will require a targeted public health response to manage the expansion of wnv with climate change in europe.', 'transformer-based deep learning for covid-19 prediction based on climate variables in indonesia recent research on the effect of climate variables on coronavirus (covid-19) transmission has emerged. climate change can potentially cause new viral outbreaks, illness, and death. this study contributes to covid-19 disease prevention efforts. this study makes two contributions: (1) we investigated the impact of climate variables on the number of covid-19 cases in 34 indonesian provinces, and (2) we developed a transformer-based deep learning model for time series forecasting for the number of positive covid-19 cases the following day based on climate variables in 34 indonesian provinces. we obtained data from march 15, 2020, to july 22, 2021, on the number of positive covid-19 cases and climate change variables (wind, temperature, humidity) in indonesia. to examine the effect of climate change on the number of positive covid-19 cases, we employed 15 scenarios for training. the experiment results of the proposed model show that the combination of wind speed and humidity has a weakly positive correlation with positive covid-19 incidence; however, the temperature has a considerably negative association with positive covid-19 incidences. compared to the other testing scenarios, the transformer-based deep learning model produced the lowest mae of 175.96 and the lowest rmse of 375.81. this study demonstrates that the transformer model works well in several provinces, such as sumatra, java, papua, bali, west nusa tenggara, east nusa tenggara, east kalimantan, and sulawesi, but not in central kalimantan, west sulawesi, south sulawesi, and north sulawesi.']"
78,health,3,24,78_health_care_clinical_health care,"['health', 'care', 'clinical', 'health care', 'healthcare', 'medical', 'individuals', 'digital', 'ai', 'hospital', 'determinants', 'chapter', 'patients', 'barriers', 'implementation', 'public', 'patient', 'intelligence', 'depression', 'artificial intelligence', 'strait', 'treatment', 'technologies', 'medicine', 'ai applications', 'public health', 'recommendation', 'recommendations', 'practice', 'evidence']","['recommendations to overcome barriers to the use of artificial intelligence-driven evidence in health technology assessment background: artificial intelligence (ai) has attracted much attention because of its enormous potential in healthcare, but uptake has been slow. there are substantial barriers that challenge health technology assessment (hta) professionals to use ai-generated evidence for decision-making from large real-world databases (e.g., based on claims data). as part of the european commission-funded htx h2020 (next generation health technology assessment) project, we aimed to put forward recommendations to support healthcare decision-makers in integrating ai into the hta processes. the barriers, addressed by the paper, are particularly focusing on central and eastern european (cee) countries, where the implementation of hta and access to health databases lag behind western european countries. methods: we constructed a survey to rank the barriers to using ai for hta purposes, completed by respondents from cee jurisdictions with expertise in hta. using the results, two members of the htx consortium from cee developed recommendations on the most critical barriers. then these recommendations were discussed in a workshop by a wider group of experts, including hta and reimbursement decision-makers from both cee countries and western european countries, and summarized in a consensus report. results: recommendations have been developed to address the top 15 barriers in areas of (1) human factor-related barriers, focusing on educating hta doers and users, establishing collaborations and best practice sharing; (2) regulatory and policy-related barriers, proposing increasing awareness and political commitment and improving the management of sensitive information for ai use; (3) data-related barriers, suggesting enhancing standardization and collaboration with data networks, managing missing and unstructured data, using analytical and statistical approaches to address bias, using quality assessment tools and quality standards, improving reporting, and developing better conditions for the use of data; and (4) technological barriers, suggesting sustainable development of ai infrastructure. conclusion: in the field of hta, the great potential of ai to support evidence generation and evaluation has not yet been sufficiently explored and realized. raising awareness of the intended and unintended consequences of ai-based methods and encouraging political commitment from policymakers is necessary to upgrade the regulatory and infrastructural environment and knowledge base required to integrate ai into hta-based decision-making processes better.', 'towards relevant health care practices the concept of reasonable, “pertinent”, or “sober” medicine has been developed since 1977. it is now defined as a medical investigation and therapeutic strategy specifically targeted, based on quality, accuracy and safety, according to the best current medical knowledge and adapted to the given patient and his/her environment. high quality medical pertinence requires basic and clinical expertise as well as input from guidelines and recommendations from expert learned bodies. it refers to the evidence based medicine achievements. but appropriate health care organization is also needed, including multidisciplinary collaboration and access to reference centres. although not the only one, the objective is also to contain medical care costs in order to maintain the sustainability of the health system. it is mandatory to evaluate the efficiency of the medical pertinence through specific programs. on the one hand, public authorities should encourage and take action through legislative and regulatory measures in order to help the promotion and development of the relevance of medical procedures. on the other hand, medical teaching as well as training should integrate the global objectives and conditions for pertinent medicine very early, and all along, in the course. however, medical decision will remain an individual responsibility, even in the changing environment of algorithms and artificial intelligence.', 'implementation of artificial intelligence (ai) applications in radiology: hindering and facilitating factors objective: the objective was to identify barriers and facilitators to the implementation of artificial intelligence (ai) applications in clinical radiology in the netherlands. materials and methods: using an embedded multiple case study, an exploratory, qualitative research design was followed. data collection consisted of 24 semi-structured interviews from seven dutch hospitals. the analysis of barriers and facilitators was guided by the recently published non-adoption, abandonment, scale-up, spread, and sustainability (nasss) framework for new medical technologies in healthcare organizations. results: among the most important facilitating factors for implementation were the following: (i) pressure for cost containment in the dutch healthcare system, (ii) high expectations of ai’s potential added value, (iii) presence of hospital-wide innovation strategies, and (iv) presence of a “local champion.” among the most prominent hindering factors were the following: (i) inconsistent technical performance of ai applications, (ii) unstructured implementation processes, (iii) uncertain added value for clinical practice of ai applications, and (iv) large variance in acceptance and trust of direct (the radiologists) and indirect (the referring clinicians) adopters. conclusion: in order for ai applications to contribute to the improvement of the quality and efficiency of clinical radiology, implementation processes need to be carried out in a structured manner, thereby providing evidence on the clinical added value of ai applications. key points: • successful implementation of ai in radiology requires collaboration between radiologists and referring clinicians. • implementation of ai in radiology is facilitated by the presence of a local champion. • evidence on the clinical added value of ai in radiology is needed for successful implementation.']"
7,vegetation_ecosystem,4,35,7_bird_species_acoustic_sound,"['bird', 'species', 'acoustic', 'sound', 'recordings', 'animal', 'monitoring', 'birds', 'audio', 'recognition', 'biodiversity', 'classification', 'speech', 'endangered', 'elephant', 'foraging', 'english', 'identification', 'biodiversity monitoring', 'automated', 'recording', 'signal', 'insects', 'passive', 'speech recognition', 'animals', 'wildlife', 'dataset', 'automatic', 'detection']","['automated bird species identification using audio signal processing and neural network now a days bird population is changing drastically because lots of reasons such as human intervention, climate change, global warming, forest fires or deforestation, etc., with the help of automatic bird species detection using machine learning algorithms, it is now possible to keep a watch on the population of birds as well as their behavior. because manual identification of different bird species takes a lot of time and effort, an automatic bird identification system that does not require physical intervention is developed in this work. to achieve this objective, convolutional neural network is used as compared to traditionally used classifiers such as svm, random forest, smacpy. the foremost goal is to identify the bird species using the dataset including vocals of the different birds. the input dataset will be pre-processed, which will comprise framing, silence removal, reconstruction, and then a spectrogram will be constructed, which will be sent to a convolutional neural network as an input, followed by cnn modification, testing, and classification. the result is compared with pre-trained data and output is generated and birds are classified according to their features (size, colour, species, etc.)', 'raspberry pi based recording system for acoustic monitoring of bird species severe degradation of ecosystems due to human encroachment and climate change call for close monitoring of the ecosystems in order to conserve them. ecosystems have a lot of acoustic data that can be used to study changes taking place in them remotely. in this paper, we present an acoustic system that is based on the raspberry pi and is used to collect audio recordings for use in acoustic monitoring of birds. the system has been designed to work optimally in the field. it has been able to collect good quality acoustic data of several bird species during its pilot deployment. acoustic data collected over a reasonable amount of time will be used to create datasets that will be used in developing machine learning models for automatic classification of bird species. this will offer a tool to provide continuous monitoring of ecosystems.', 'an acoustic detection dataset of birds (aves) in montane forests using a deep learning approach background long-term monitoring is needed to understand the statuses and trends of wildlife communities in montane forests, such as those in yushan national park (ysnp), taiwan. integrating passive acoustic monitoring (pam) with an automated sound identifier, a longterm biodiversity monitoring project containing six pam stations, was launched in ysnp in january 2020 and is currently ongoing. silic, an automated wildlife sound identification model, was used to extract sounds and species information from the recordings collected. animal vocal activity can reflect their breeding status, behaviour, population, movement and distribution, which may be affected by factors, such as habitat loss, climate change and human activity. this massive amount of wildlife vocalisation dataset can provide essential information for the national parks headquarters on resource management and decision-making. it can also be valuable for those studying the effects of climate change on animal distribution and behaviour at a regional or global scale.new information to our best knowledge, this is the first open-access dataset with species occurrence data extracted from sounds in soundscape recordings by artificial intelligence. we obtained seven bird species for the first release, with more bird species and other taxa, such as mammals and frogs, to be updated annually. raw recordings containing over 1.7 million one-minute recordings collected between the years 2020 and 2021 were analysed and silic identified 6,243,820 vocalisations of seven bird species in 439,275 recordings. the automatic detection had a precision of 0.95 and the recall ranged from 0.48 to 0.80. in terms of the balance between precision and recall, we prioritised increasing precision over recall in order to minimise false positive detections. in this dataset, we summarised the count of vocalisations detected per sound class per recording which resulted in 802,670 occurrence records. unlike data from traditional human observation methods, the number of observations in the darwin core organismquantity column refers to the number of vocalisations detected for a specific bird species and cannot be directly linked to the number of individuals. we expect our dataset will be able to help fill the data gaps of fine-scale avian temporal activity patterns in montane forests and contribute to studies concerning the impacts of climate change on montane forest ecosystems on regional or global scales.']"
8,vegetation_ecosystem,4,38,8_pollen_plant_specimens_images,"['pollen', 'plant', 'specimens', 'images', 'phenology', 'phenological', 'species', 'plant species', 'plants', 'flowering', 'leaf', 'reproductive', 'automated', 'deep', 'deep learning', 'computer vision', 'identification', 'image', 'organs', 'flower', 'recognition', 'computer', 'digitized', 'vision', 'collections', 'automatic', 'growth', 'manual', 'segmentation', 'flowers']","['applying machine learning based on multiscale classifiers to detect remote phenology patterns in cerrado savanna trees plant phenology is one of the most reliable indicators of species responses to global climate change, motivating the development of new technologies for phenological monitoring. digital cameras or near remote systems have been efficiently applied as multi-channel imaging sensors, where leaf color information is extracted from the rgb (red, green, and blue) color channels, and the changes in green levels are used to infer leafing patterns of plant species. in this scenario, texture information is a great ally for image analysis that has been little used in phenology studies. we monitored leaf-changing patterns of cerrado savanna vegetation by taking daily digital images. we extract rgb channels from the digital images and correlate them with phenological changes. additionally, we benefit from the inclusion of textural metrics for quantifying spatial heterogeneity. our first goals are: (1) to test if color change information is able to characterize the phenological pattern of a group of species; (2) to test if the temporal variation in image texture is useful to distinguish plant species; and (3) to test if individuals from the same species may be automatically identified using digital images. in this paper, we present a machine learning approach based on multiscale classifiers to detect phenological patterns in the digital images. our results indicate that: (1) extreme hours (morning and afternoon) are the best for identifying plant species; (2) different plant species present a different behavior with respect to the color change information; and (3) texture variation along temporal images is promising information for capturing phenological patterns. based on those results, we suggest that individuals from the same species and functional group might be identified using digital images, and introduce a new tool to help phenology experts in the identification of new individuals from the same species in the image and their location on the ground. © 2013 elsevier b.v.', 'towards automatic airborne pollen monitoring: from commercial devices to operational by mitigating class-imbalance in a deep learning approach allergic diseases have been the epidemic of the century among chronic diseases. particularly for pollen allergies, and in the context of climate change, as airborne pollen seasons have been shifting earlier and abundances have been becoming higher, pollen monitoring plays an important role in generating high-risk allergy alerts. however, this task requires labour-intensive and time-consuming manual classification via optical microscopy. even new-generation, automatic, monitoring devices require manual pollen labelling to increase accuracy and to advance to genuinely operational devices. deep learning-based models have the potential to increase the accuracy of automated pollen monitoring systems. in the current research, transfer learning-based convolutional neural networks were employed to classify pollen grains from microscopic images. given a high imbalance in the dataset, we incorporated class weighted loss, focal loss and weight vector normalisation for class balancing as well as data augmentation and weight penalties for regularisation. airborne pollen has been routinely recorded by a bio-aerosol analyzer (baa500, hund gmbh) located in augsburg, germany. here we utilised a database referring to manually classified airborne pollen images of the whole pollen diversity throughout an annual pollen season. by using the cropped pollen images collected by this device, we achieved an unweighted average f1 score of 93.8% across 15 classes and an unweighted average f1 score of 75.9% across 31 classes. the majority of taxa (9 of 15), being also the most abundant and allergenic, showed a recall of at least 95%, reaching up to a remarkable 100% in pollen from taxus and urticaceae. the recent introduction of novel pollen monitoring devices worldwide has pointed to the necessity for real-time, automatic measurements of airborne pollen and fungal spores. thus, we may improve everyday clinical practice and achieve the most efficient prophylaxis of allergic patients.', 'plant species recognition methods using leaf image: overview plant plays an important role in agricultural, industrial, medicine, environmental and ecological protection. recently, with global warming, biodiversity loss, rapid urban development and environmental damage, people have been seriously destroying the natural environments, which results in that a large number of plant species constantly dying and even dying out every year. it is essential to protect plant species. the first step of protecting plants is to recognize them and understand what they are and where they come from. but there are a large number of plant species that have been named on earth, and many are still unknown yet, it is difficult to identifiy each species. to handle such huge information, develop a quick and efficient classification method has become a significant research. plant species can be recognized by its leaf, flower, skin, fruit and seed, etc. relatively speaking, using leaf to recognize plant species is very simple and convenient, and many leaf based plant species recognition methods have been proposed. in this paper, we mainly summarize the existing leaf based plant species identification methods, including plant leaf characteristic, public databases, feature extraction based methods, subspace learning based methods, sparse representation based methods, and deep learning based methods. the aim is to emphasize the importance of plant species identification, train people to know about plant species, and provide guidance and comprehensive study for the beginners in this field, in turn, to treasure and protect plant species.']"
25,vegetation_ecosystem,4,45,25_wetland_wetlands_classification_mapping,"['wetland', 'wetlands', 'classification', 'mapping', 'restoration', 'coastal', 'remote sensing', 'sensing', 'ecosystems', 'inundation', 'remote', 'vegetation', 'ecosystem', 'map', 'area', 'resolution', 'temporal', 'water', 'using', 'anthropogenic', 'radar', 'kappa', 'mangrove', 'classes', 'change', 'conservation', 'dynamics', 'overall', 'sentinel', 'peat']","['characterizing wetland inundation and vegetation dynamics in the arctic coastal plain using recent satellite data and field photos arctic wetlands play a critical role in the global carbon cycle and are experiencing disproportionate impacts from climate change. even though alaska hosts 65% of u.s. wetlands, less than half of the wetlands in alaska have been mapped by the u.s. fish and wildlife service national wetlands inventory (nwi) or other high-resolution wetlands protocols. the availability of time series satellite data and the development of machine learning algorithms have enabled the characterization of arctic wetland inundation dynamics and vegetation types with limited ground data input. in this study, we built a semi-automatic process to generate sub-pixel water fraction (swf) maps across the coastal plain of the arctic national wildlife refuge (anwr) in alaska using random forest regression and 139 sentinel-2 images taken in ice-free seasons from 2016 to 2019. with this, we characterized the seasonal dynamics of wetland inundation and explored their potential usage in determining nwi water regimes. the highest levels of surface water expression were detected in june, resulting from seasonal active layer thaw and snowmelt. inundation was most variable in riverbeds, lake and pond margins, and depressional wetlands, where water levels fluctuate substantially between dry and wet seasons. nwi water regimes that indicate frequent inundation, such as permanently flooded wetlands, had high swf values (swf ≥ 90%), while those with infrequent inundation, such as temporarily flooded wetlands, had low swf values (swf < 10%). vegetation types were also classified through the synergistic use of a vegetation index, water regimes, synthetic-aperture radar (sar) data, topographic data, and a random forest classifier. the random forest classification algorithms demonstrated good performance in classifying arctic wetland vegetation types, with an overall accuracy of 0.87. compared with nwi data produced in the 1980s, scrub-shrub wetlands appear to have increased from 91 to 258 km2 over the last three decades, which is the largest percentage change (182%) among all vegetation types. however, additional field data are needed to confirm this shift in vegetation type. this study demonstrates the potential of using time series satellite data and machine learning algorithms in characterizing inundation dynamics and vegetation types of arctic wetlands. this approach could aid in the creation and maintenance of wetland inventories, including the nwi, in arctic regions and enable an improved understanding of long-term wetland dynamics.', 'long-term spatio-temporal changes of wetlands in tibetan plateau and their response to climate change understanding the changes of wetlands on the tibetan plateau (tp) is important for action to ensure ecosystem resilience in asia. however, mapping long-term changes of wetlands at high resolutions remains challenging. here, we quantify the spatio-temporal changes of tp wetlands from 1990 to 2019, by combining landsat imagery with deep learning to map tp wetlands. the deep learning model combined with transfer learning strategies achieves high classification performance using a few class samples. the validation results show that the users accuracy is 95.5% and the producers accuracy is 90.1% for wetland extraction, satisfying with subsequent analysis of wetland spatio-temporal changes. based on the wetland extraction model, we have created annual wetland map in the tp for the first time. we find that the areal extent of tp wetlands has increased by 31.2 ± 6.6 % over the past 30 years. the growth is particularly noticeable (by 22.5 ± 6.2 %) during 2015–2019. spatially, the wetland areal extent on the qiangtang plateau (in the inner part of tp and as habitats of various birds and rare wild animals) and the source region of yangtze river show the largest expansions by 55.3 ± 9.3 % and 44.0 ± 8.9 %, respectively. such rapid wetland expansions are associated with increasing rainfall and temperature which have heterogeneous influences on wetland changes across the tp. our findings provide evidence for the impact of climate change on wetland area. the marked wetland changes highlight that climate mitigation is a priority for high-latitude ecosystems.', 'review of global studies on the remote sensing of wetlands from 1975 to 2020 sustainable ecosystem management requires considerable wetland spatial information given the evident climate change impacts and human disturbances on wetlands. remote sensing of wetlands, as an important interdiscipline, has increasing publications. here, we searched for published papers in the past 50 years from the “web of science core collection” database. we summarized the changes in the number of publications and citations and the development process and trend in remote sensing of wetlands. we divided the development history into three research periods including potential exploration phase, framework emerging phase, and rapid growth phase. based on the development history over the past 50 years and facing the background of wetland ecosystem protection demand in the era of big data, studies on remote sensing of wetlands have developed in the direction of fast, multisource, and fine, such as wetland intelligent classification, remote sensing inversion of large-scale wetland vegetation ecological parameters, and wetland ecosystem health assessment. however, the spectral and backscattering characteristics of wetlands are complex due to the interaction of water, vegetation, and soil, and their annual/inter-annual variation characteristics are notable, aggravating the difficulty of remote sensing detection of wetlands. this condition is a key issue that requires resolution at present. thus, the multimodal remote sensing experiments of wetlands should be strengthened. we also concluded the main research topics and data sources in different phases and analyzed the hotspots in remote sensing of wetlands by the extracted keywords from 500 latest and top-cited papers. three outlook bullets were presented from the wetland classification and landscape dynamics in the era of big data, the fine remote sensing observations in wetland ecological variables, and the spatial decision support for sustainable wetland management. based on cloud platforms (such as gee), carrying out large-scale and long-term wetland mapping and landscape dynamic analysis by means of time series remotely sensed data (i.e., landsat and sentinel), investigating the application potential of diverse machine learning algorithms (i.e., random forest and deep learning) for wetland ecological parameter inversion at different geographic scales, establishing a scientific indicator system, and fully applying the multisource and multiplatform remote sensing observation technology to solve the actual ecological environment problems are important development trends and research hotspots of future wetland remote sensing studies. we hope that this review not only provides a glimpse, but also a framework understanding of wetland remote sensing research. with the improvement on the awareness of the importance of wetland ecosystem, the number of scholars engaged in research of remote sensing of wetlands increases. the review is expected to be beneficial for understanding the development history and international frontiers for studies in remote sensing of wetlands and to support their layout domestically and abroad.']"
26,vegetation_ecosystem,4,20,26_mangrove_mangroves_carbon_stocks,"['mangrove', 'mangroves', 'carbon', 'stocks', 'soil carbon', 'carbon stocks', 'mg ha', 'mg', 'soc', 'ha', 'biomass', 'soil', 'ecosystems', 'coastal', 'wetlands', 'agb', 'wetland', 'sea level', 'sentinel', 'level rise', 'marshes', 'slr', 'stock', 'ecosystem', 'blue', 'svr', 'estuaries', 'rmse', 'aboveground', 'upscaling']","['mangrove species mapping and above-ground biomass estimation in suriname based on fused sentinel-1 and sentinel-2 imagery and national forest inventory data obtaining state-of-the art data on the mangrove cover extent is important to monitor possible responses to environmental changes such as land use change and mangrove ecosystem degradation caused by climate change. in this study, we examined the possibility of speciesspecific mapping within the mangrove area in suriname based on the fusion of sentinel-1 and sentinel-2 data using the google earth engine platform and a random forest classifier. to do this, a 2- level classification scheme was developed. in the first level, the mangrove cover was discriminated from mangrove graveyards and other land cover classes (kappa index of 80.65% ). in the second level, the dominating mangrove species were successfully classified within the living mangrove cover (kappa index of 75.21%). secondly mangrove above-ground biomass (agb) was estimated on a national scale, based on fused sentinel-1 and sentinel-2 data and national mangrove forest inventory data by using a support vector regression (svr) machine learning technique, resulting in a root mean square error (rmse) of 32.181 mg.ha-1 and a r2of 0.542.', 'a global map of mangrove forest soil carbon at 30 m spatial resolution with the growing recognition that effective action on climate change will require a combination of emissions reductions and carbon sequestration, protecting, enhancing and restoring natural carbon sinks have become political priorities. mangrove forests are considered some of the most carbon-dense ecosystems in the world with most of the carbon stored in the soil. in order for mangrove forests to be included in climate mitigation efforts, knowledge of the spatial distribution of mangrove soil carbon stocks are critical. current global estimates do not capture enough of the finer scale variability that would be required to inform local decisions on siting protection and restoration projects. to close this knowledge gap, we have compiled a large georeferenced database of mangrove soil carbon measurements and developed a novel machine-learning based statistical model of the distribution of carbon density using spatially comprehensive data at a 30 m resolution. this model, which included a prior estimate of soil carbon from the global soilgrids 250 m model, was able to capture 63% of the vertical and horizontal variability in soil organic carbon density (rmse of 10.9 kg m-3). of the local variables, total suspended sediment load and landsat imagery were the most important variable explaining soil carbon density. projecting this model across the global mangrove forest distribution for the year 2000 yielded an estimate of 6.4 pg c for the top meter of soil with an 86-729 mg c ha-1 range across all pixels. by utilizing remotely-sensed mangrove forest cover change data, loss of soil carbon due to mangrove habitat loss between 2000 and 2015 was 30-122 tg c with >75% of this loss attributable to indonesia, malaysia and myanmar. the resulting map products from this work are intended to serve nations seeking to include mangrove habitats in payment-for- ecosystem services projects and in designing effective mangrove conservation strategies.', 'a global predictive model of carbon in mangrove soils mangroves are among the most threatened and rapidly vanishing natural environments worldwide. they provide a wide range of ecosystem services and have recently become known for their exceptional capacity to store carbon. research shows that mangrove conservation may be a low-cost means of reducing co<inf>2</inf> emissions. accordingly, there is growing interest in developing market mechanisms to credit mangrove conservation projects for associated co<inf>2</inf> emissions reductions. these efforts depend on robust and readily applicable, but currently unavailable, localized estimates of soil carbon. here, we use over 900 soil carbon measurements, collected in 28 countries by 61 independent studies, to develop a global predictive model for mangrove soil carbon. using climatological and locational data as predictors, we explore several predictive modeling alternatives, including machine-learning methods. with our predictive model, we construct a global dataset of estimated soil carbon concentrations and stocks on a high-resolution grid (5 arc min). we estimate that the global mangrove soil carbon stock is 5.00±0.94 pg c (assuming a 1 meter soil depth) and find this stock is highly variable over space. the amount of carbon per hectare in the worlds most carbon-rich mangroves (approximately 703±38 mg c ha<sup>-1</sup>) is roughly a 2.6±0.14 times the amount of carbon per hectare in the worlds most carbon-poor mangroves (approximately 272±49 mg c ha<sup>-1</sup>). considerable within country variation in mangrove soil carbon also exists. in indonesia, the country with the largest mangrove soil carbon stock, we estimate that the most carbon-rich mangroves contain 1.5±0.12 times as much carbon per hectare as the most carbon-poor mangroves. our results can aid in evaluating benefits from mangrove conservation and designing mangrove conservation policy. additionally, the results can be used to project changes in mangrove soil carbon stocks based on changing climatological predictors, e.g. to assess the impacts of climate change on mangrove soil carbon stocks.']"
34,vegetation_ecosystem,4,28,34_erosion_soil erosion_soil_land,"['erosion', 'soil erosion', 'soil', 'land', 'susceptibility', 'auc', 'area', 'soil loss', 'factors', 'land degradation', 'drained', 'encroachment', 'curve', 'dem', 'degradation', 'respectively', 'subsidence', 'land subsidence', 'river', 'basin', 'land use', 'slope', 'loss', 'cover', 'mlp', 'map', 'lands', 'machine', 'wind', 'receiver operating']","['susceptibility mapping of soil water erosion using machine learning models soil erosion is a serious threat to sustainable agriculture, food production, and environmental security. the advancement of accurate models for soil erosion susceptibility and hazard assessment is of utmost importance for enhancing mitigation policies and laws. this paper proposes novel machine learning (ml) models for the susceptibility mapping of the water erosion of soil. the weighted subspace random forest (wsrf), gaussian process with a radial basis function kernel (gaussprradial), and naive bayes (nb) ml methods were used in the prediction of the soil erosion susceptibility. data included 227 samples of erosion and non-erosion locations through field surveys to advance models of the spatial distribution using predictive factors. in this study, 19 effective factors of soil erosion were considered. the critical factors were selected using simulated annealing feature selection (safs). the critical factors included aspect, curvature, slope length, flow accumulation, rainfall erosivity factor, distance from the stream, drainage density, fault density, normalized difference vegetation index (ndvi), hydrologic soil group, soil texture, and lithology. the dataset cells of samples (70% for training and 30% for testing) were randomly prepared to assess the robustness of the different models. the functional relevance between soil erosion and effective factors was computed using the ml models. the ml models were evaluated using different metrics, including accuracy, the kappa coefficient, and the probability of detection (pod). the accuracies of the wsrf, gaussprradial, and nb methods were 0.91, 0.88, and 0.85, respectively, for the testing data; 0.82, 0.76, and 0.71, respectively, for the kappa coefficient; and 0.94, 0.94, and 0.94, respectively, for pod. however, the ml models, especially the wsrf, had an acceptable performance regarding producing soil erosion susceptibility maps. maps produced with the most robust models can be a useful tool for sustainable management, watershed conservation, and the reduction of soil and water loss.', 'evaluation of soil erosion using 3s techniques: a case study of cangxi county in the jialing river basin, yangtze river, china soil erosion not only has a negative impact on the ecological environment, but also restricts the sustainable development. therefore, its significant to develop green ecology, evaluate soil erosion in large river valley, and propose feasible suggestions of soil and water conservation. this paper uses 3s technology to evaluate soil erosion of cangxi county in the jialing river basin, yangtze river, china. the main steps include: (1)image pre-processing: using zy-3 and gf-1 satellite data, both geometric error and radiometric error are corrected so that image accuracy is improved; (2)field investigation and indoor interpretation of remote sensing: field interpretation marks are established by gps and rs; mainly image interpretation is carried out by machine learning or image classification, then results are checked by visual interpretation and confirmed by field review to improve interpretation accuracy; (3)spatial analysis: soil erosion intensity is classified and soil erosion intensity map can be obtained through standards for classification and gradation of soil erosion(sl190-2007) based on three factors(land-use types, vegetation coverage and slope); (4)soil erosion evaluation: soil erosion is evaluated by soil erosion intensity map. the results show that: (1)the intensity of soil erosion in the study area is concentrated on slight and moderate soil erosion; the area with severe soil erosion is few; (2)the soil erosion of townships are mainly concentrated on the north and east of the county, while soil erosion is relatively slight in the northwest and southwest; (3)under the influence of different factors, the soil erosion intensity is quite different and is often affected by many factors(exposed soil, human activities). finally, this paper proposes some specific measures from the perspective of green ecology, which may provide a view to a certain reference for soil and water conservation in the study area and evaluation of soil erosion in other regions in yangtze river, china.', 'assessing and mapping soil erosion risk zone in ratlam district, central india evaluation of physical and quantitative data of soil erosion is crucial to the sustainable development of the environment. the extreme form of land degradation through different forms of erosion is one of the major problems in the sub-tropical monsoon-dominated region. in india, tackling soil erosion is one of the major geo-environmental issues for its environment. thus, identifying soil erosion risk zones and taking preventative actions are vital for crop production management. soil erosion is induced by climate change, topographic conditions, soil texture, agricultural systems, and land management. in this research, the soil erosion risk zones of ratlam district was determined by employing the geographic information system (gis), revised universal soil loss equation (rusle), analytic hierarchy process (ahp), and machine learning algorithms (random forest and reduced error pruning (rep) tree). rusle measured the rainfall eosivity (r), soil erodibility (k), length of slope and steepness (ls), land cover and management (c), and support practices (p) factors. kappa statistic was used to configure model reliability and it was found that random forest and ahp have higher reliability than other models. about 14.73% (715.94 km2) of the study area has very low risk to soil erosion, with an average soil erosion rate of 0.00–7.00 × 103 kg/(hm2·a), while about 7.46% (362.52 km2) of the study area has very high risk to soil erosion, with an average soil erosion rate of 30.00 × 103–48.00 × 103 kg/(hm2·a). slope, elevation, stream density, stream power index (spi), rainfall, and land use and land cover (lulc) all affect soil erosion. the current study could help the government and non-government agencies to employ developmental projects and policies accordingly. however, the outcomes of the present research also could be used to prevent, monitor, and control soil erosion in the study area by employing restoration measures.']"
100,vegetation_ecosystem,4,39,100_deforestation_tree_forest_trees,"['deforestation', 'tree', 'forest', 'trees', 'images', 'detection', 'deep learning', 'amazon', 'deep', 'segmentation', 'imagery', 'urban', 'mapping', 'convolutional', 'resolution', 'high resolution', 'canopy', 'image', 'urban tree', 'semantic segmentation', 'palm', 'forests', 'cover', 'rgb', 'convolutional neural', 'semantic', 'areas', 'satellite', 'learning', 'cnn']","['amazon forest cover change mapping based on semantic segmentation by u-nets deforestation remains a major concern with regard to climate change and the maintenance of biodiversity. meanwhile, the development of new image processing techniques and the broad availability of high spatiotemporal resolution satellite imagery provide an unprecedented setup for the development of effective and scalable forest cover change monitoring systems. this is especially relevant in regions with large forested areas with high rates of deforestation, such as in the region of the brazilian amazon rainforest. in this context, existing forest cover change monitoring methods are based on a combination of visual inspection, spectral profiles, statistics, and machine learning techniques, which offer alternative backbones to deal with deforestation monitoring. given the recent advances in the field of image processing by fully convolutional neural networks (fcns), the objective of this study is to evaluate the performance of the u-net architecture for the mapping of forest cover aimed at identifying deforestation polygons in multi-temporal satellite imagery. to this end, 10-m resolution imagery from the sentinel-2 satellite covering portions of the legal amazon region were employed. the u-net could identify and draw polygons of forest areas and forest fragments with high accuracy (0.9470), precision (0.9356), recall (0.9676), and f1-score (0.9513), thus outperforming largely applied and well-know supervised and unsupervised image classification methods. the results indicate and we further discuss that u-nets have the potential to run as the backbone for efficient forest cover change monitoring initiatives and support the deployment of near real-time deforestation warning systems.', 'evaluation of semantic segmentation methods for deforestation detection in the amazon deforestation is a wide-reaching problem, responsible for serious environmental issues, such as biodiversity loss and global climate change. containing approximately ten percent of all biomass on the planet and home to one tenth of the known species, the amazon biome has faced important deforestation pressure in the last decades. devising efficient deforestation detection methods is, therefore, key to combat illegal deforestation and to aid in the conception of public policies directed to promote sustainable development in the amazon. in this work, we implement and evaluate a deforestation detection approach which is based on a fully convolutional, deep learning (dl) model: the deeplabv3+. we compare the results obtained with the devised approach to those obtained with previously proposed dl-based methods (early fusion and siamese convolutional network) using landsat oli-8 images acquired at different dates, covering a region of the amazon forest. in order to evaluate the sensitivity of the methods to the amount of training data, we also evaluate them using varying training sample set sizes. the results show that all tested variants of the proposed method significantly outperform the other dl-based methods in terms of overall accuracy and f1-score. the gains in performance were even more substantial when limited amounts of samples were used in training the evaluated methods.', 'change detection of deforestation in the brazilian amazon using landsat data and convolutional neural networks mapping deforestation is an essential step in the process of managing tropical rainforests. it lets us understand and monitor both legal and illegal deforestation and its implications, which include the effect deforestation may have on climate change through greenhouse gas emissions. given that there is ample room for improvements when it comes to mapping deforestation using satellite imagery, in this study, we aimed to test and evaluate the use of algorithms belonging to the growing field of deep learning (dl), particularly convolutional neural networks (cnns), to this end. although studies have been using dl algorithms for a variety of remote sensing tasks for the past few years, they are still relatively unexplored for deforestation mapping. we attempted to map the deforestation between images approximately one year apart, specifically between 2017 and 2018 and between 2018 and 2019. three cnn architectures that are available in the literature-sharpmask, u-net, and resunet-were used to classify the change between years and were then compared to two classic machine learning (ml) algorithms-random forest (rf) and multilayer perceptron (mlp)-as points of reference. after validation, we found that the dl models were better in most performance metrics including the kappa index, f1 score, and mean intersection over union (miou) measure, while the resunet model achieved the best overall results with a value of 0.94 in all three measures in both time sequences. visually, the dl models also provided classifications with better defined deforestation patches and did not need any sort of post-processing to remove noise, unlike the ml models, which needed some noise removal to improve results.']"
103,vegetation_ecosystem,4,55,103_vegetation_grassland_agb_plateau,"['vegetation', 'grassland', 'agb', 'plateau', 'ndvi', 'desertification', 'alpine', 'grasslands', 'qinghai tibet', 'tibet', 'tibet plateau', 'qinghai', 'precipitation', 'tibetan', 'rocky', 'tibetan plateau', 'changes', 'china', 'meadow', 'vegetation index', 'human activities', 'factors', 'xinjiang', 'ecological', 'desert', 'land', 'index', 'temperature', 'temperature precipitation', 'area']","['estimation of alpine grassland above-ground biomass and its response to climate on the qinghai-tibet plateau during 2001 to 2019 the qinghai-tibet plateau (qtp) grassland is a critical part of the carbon pool of terrestrial ecosystems and provides important animal husbandry resources. to embody the stability of grassland ecology and the response of grassland above-ground biomass (agb) to climate change on the qtp, four estimations models (partial least squares regression (plsr), random forest (rf), back-propagation neural network (bpnn) and deep belief network (dbn)), which are based on statistics, machine learning and deep learning regression methods, respectively, were established to estimate the alpine meadow and alpine steppe agb from 2001–2019. the results showed that: (1) the rf model performs well on the agb estimation with the highest accuracy (r2 =0.84, rmse=8.51gc/m2, mae=6.46gc/m2) and stability (r2 =0.76, rmse=9.24gc/m2, mae =8.30gc/m2); (2) in spatial pattern, the agb decreased from southeast to northwest on the qtp, and present a significant increasing with a rate at 0.19gc/m2a and 0.06gc/m2a on alpine meadow and alpine steppe during 2001–2019, respectively; (3) in the past 19 years, the agb variations on the qtp showed a relatively stability with the average cv of 0.1; (4) the sustainability of the agb show a weak anti-persistence (hurst = 0.43), and it indicates that the future trend of the agb is opposite to the current trend and the agb is likely to be decreased in the future; (5) temperature and precipitation have a positive promotion on the most alpine meadow and alpine steppe vegetation growth on the qtp, while warming and wetting have negative promotion effects on the northwest of the plateau. these research results can provide useful reference materials for the study of the impact of climate change on alpine meadow and alpine steppe on the qtp.', 'vegetation survey and mapping on the qinghai-tibet plateau background & aim: surveying and mapping has always been an important part of vegetation research on the qinghai-tibet plateau. historically, china has performed several vegetation surveys of the qinghai-tibet plateau and compiled a series of important vegetation maps. in this research, we aim to review the history of vegetation surveys and mapping on the qinghai-tibet plateau and evaluate the consistency across maps, and to investigate the consistency between vegetation types in several maps and vegetation survey points provided by the second qinghai-tibet plateau scientific expedition and research program. results: (1) the vegetation surveys of the qinghai-tibet plateau have a long history, with systematic and scientific vegetation surveys of the qinghai-tibet plateau starting after the founding of the people’s republic of china. in vegetation mapping, the vegetation map of china (1: 4,000,000), the grassland resource map of china (1: 1,000,000), and the vegetation map of china (1: 1,000,000) were the most widely used vegetation maps that cover the qinghai-tibet plateau. however, they are quite different in vegetation classification systems. (2) the comparison between different vegetation maps shows that the vegetation formation groups covering a large area, such as forest and grassland, have high consistency between different vegetation maps, while the vegetation formation groups covering a small area, such as aquatic and agricultural vegetation, have low consistency between different vegetation maps. furthermore, we select carex parvula meadow, stipa purpurea grassland, carex moorcroftii grassland, stipa glareosa grassland, carex alatauensis meadow, artemisia wellbyi grassland, stipa roborowskyi grassland, and orinus thoroldii grassland to investigate the differences in the areas and spatial distribution patterns between the vegetation map of china (1: 1,000,000) and the grassland resource map of china (1: 1,000,000). the results show that there are significant differences in the area and spatial distribution pattern between the two maps for these selected vegetation types. (3) the comparison of vegetation types between vegetation survey points and the vegetation map of china (1: 1,000,000), the grassland resource map of china (1: 1,000,000), and the current vegetation map of qinghai-tibet plateau (2019−2020) indicates that 45.05%, 21.02%, and 50.83% of the vegetation survey points, respectively, are not consistent with the three vegetation maps at the vegetation formation group level. (4) in the past 30 years, due to the influence of climate change and human activities, the vegetation distribution pattern of the qinghai-tibet plateau has changed significantly. conclusion: vegetation survey and mapping technology has undergone great progress in recent decades. remote imaging with high spatial, temporal, and spectral resolution and deep learning technology have been widely used in vegetation identification and mapping. it is the right time to produce new vegetation maps of the qinghai-tibet plateau at medium or large scales, which will provide more detailed and fundamental data for ecosystem management, and ecological restoration projects on the qinghai-tibet plateau.', 'spatiotemporal dynamics of grassland aboveground biomass and its driving factors in north china over the past 20 years although remote sensing has enabled rapid monitoring of grassland aboveground biomass (agb) at a regional scale, it is still a difficult challenge to construct an accurate estimation model of grassland agb in a vast region to support the agb dynamics analysis over a long time series. in this study, extensive grassland agb measurements (collected in north china during the grassland growing season of 2000–2019), modis data, and environmental factors (climate, topography and soil) were employed to construct the grassland agb models using four machine learning algorithms (random forest, support vector machine, artificial neural network and extreme learning machine) combined with four variable selections. the spatial distributions of annual grassland agb from 2000 to 2019 were simulated based on the optimal agb model. the temporal change and future trend of agb series from 2000 to 2019 were comprehensively analyzed by the slope model and hurst exponent. the influences of natural and anthropogenic factors on grassland agb dynamics were explored quantitatively using the geodetector model. the results showed that (1) the random forest model constructed from the variables selected by the successive projections algorithm is the optimal grassland agb model. (2) the 20-year average grassland agb in north china showed an overall spatial distribution of being low in the central and western parts and high in the southeastern part. (3) the annual maximum grassland agb in most regions (82.71%) showed an increasing trend during 2000–2019; and most of the grasslands with a decreasing trend of agb were located in regions with low agb values and arid climates. (4) the future trend of grassland agb after the study period may be optimistic, as reflected by more grassland agb was predicted to increase rather than decrease (70.38% vs. 29.62%). (5) the main driving factors of spatiotemporal dynamics of grassland agb were precipitation, soil type, and livestock density; the interactive influence of two drivers on agb showed mutual enhancement.']"
104,vegetation_ecosystem,4,45,104_deforestation_land_amazon_loss,"['deforestation', 'land', 'amazon', 'loss', 'cover', 'brazilian', 'forest', 'land use', 'land cover', 'degradation', 'lulc', 'brazil', 'areas', 'changes', 'biodiversity', 'use land', 'use', 'natural', 'region', 'ecosystem', 'dynamics', 'change', 'acoustic', 'machine', 'machine learning', 'biome', 'period', 'km2', 'conservation', 'expansion']","['spatial near future modeling of land use and land cover changes in the temperate forests of mexico the loss of temperate forests of mexico has continued in recent decades despite wide recognition of their importance to maintaining biodiversity. this study analyzes land use/land cover change scenarios, using satellite images from the landsat sensor. images corresponded to the years 1990, 2005 and 2017. the scenarios were applied for the temperate forests with the aim of getting a better understanding of the patterns in land use/land cover changes. the support vector machine (svm) multispectral classification technique served to determine the land use/land cover types, which were validated through the kappa index. for the simulation of land use/land cover dynamics, a model developed in dinamica-ego was used, which uses stochastic models of markov chains, cellular automata and weight of evidences. for the study, a stationary, an optimistic and a pessimistic scenario were proposed. the projections based on the three scenarios were simulated for the year 2050. five types of land use/land cover were identified and evaluated. they were primary forest, secondary forest, human settlements, areas without vegetation and water bodies. results from the land use/land cover change analysis show a substantial gain for the secondary forest. the surface area of the primary forest was reduced from 55.8% in 1990 to 37.7% in 2017. moreover, the three projected scenarios estimate further losses of the surface are for the primary forest, especially under the stationary and pessimistic scenarios. this highlights the importance and probably urgent implementation of conservation and protection measures to preserve these ecosystems and their services. based on the accuracy obtained and on the models generated, results from these methodologies can serve as a decision tool to contribute to the sustainable management of the natural resources of a region.', 'achieving brazils deforestation target will reduce fire and deliver air quality and public health benefits climate, deforestation, and forest fires are closely coupled in the amazon, but models of fire that include these interactions are lacking. we trained machine learning models on temperature, rainfall, deforestation, land-use, and fire data to show that spatial and temporal patterns of fire in the amazon are strongly modified by deforestation. we find that fire count across the brazilian amazon increases by 0.44 percentage points for each percentage point increase in deforestation rate. we used the model to predict that the increased deforestation rate in the brazilian amazon from 2013 to 2020 caused a 42% increase in fire counts in 2020. we predict that if brazil had achieved the deforestation target under the national policy on climate change, there would have been 32% fewer fire counts across the brazilian amazon in 2020. using a regional chemistry-climate model and exposure-response associations, we estimate that the improved air quality due to reduced smoke emission under this scenario would have resulted in 2,300 fewer deaths due to reduced exposure to fine particulate matter. our analysis demonstrates the air quality and public health benefits that would accrue from reducing deforestation in the brazilian amazon.', 'land use/land cover (lulc) analysis (2009–2019) with google earth engine and 2030 prediction using markov-ca in the rondônia state, brazil the amazonian biome is important not only for south america but also for the entire planet, providing essential environmental services. the state of rondônia ranks third in deforestation rates in the brazilian legal amazon (bla) political division. this study aims to evaluate the land use/land cover (lulc) changes over the past ten years (2009–2019), as well as, to predict the lulc in the next 10 years, using terrset 18.3 software, in the state of rondônia, brazil. the machine learning algorithms within the google earth engine cloud-based platform employed a random forest classifier in image classifications. the markov-ca deep learning algorithm predicted future lulc changes by comparing scenarios of one and three transitions. the results showed a reduction in forested areas of about 15.7% between 2009 and 2019 in the rondônia state. according to the predictive model, by 2030, around 30% of the remaining forests will be logged, most likely converted into occupied areas. the results reinforce the importance of measures and policies integrated with investments in research and satellite monitoring to reduce deforestation in the brazilian amazon and ensure the continuity of the amazonian role in halting climate change.']"
107,vegetation_ecosystem,4,62,107_forest_tree_uav_vegetation,"['forest', 'tree', 'uav', 'vegetation', 'tree species', 'cover', 'classification', 'forests', 'species', 'sentinel', 'canopy', 'sensing', 'mapping', 'remote', 'remote sensing', 'images', 'structure', 'hyperspectral', 'satellite', 'imagery', 'lidar', 'forestry', 'forest structure', 'biodiversity', 'resolution', 'monitoring', 'airborne', 'trees', 'maps', 'diversity']","['mapping canopy heights in dense tropical forests using low-cost uav-derived photogrammetric point clouds and machine learning approaches tropical forests are a key component of the global carbon cycle and climate change mitigation. field-or lidar-based approaches enable reliable measurements of the structure and above-ground biomass (agb) of tropical forests. data derived from digital aerial photogrammetry (dap) on the unmanned aerial vehicle (uav) platform offer several advantages over field-and lidar-based approaches in terms of scale and efficiency, and dap has been presented as a viable and economical alternative in boreal or deciduous forests. however, detecting with dap the ground in dense tropical forests, which is required for the estimation of canopy height, is currently considered highly challenging. to address this issue, we present a generally applicable method that is based on machine learning methods to identify the forest floor in dap-derived point clouds of dense tropical forests. we capitalize on the dap-derived high-resolution vertical forest structure to inform ground detection. we conducted uav-dap surveys combined with field inventories in the tropical forest of the congo basin. using airborne lidar (als) for ground truthing, we present a canopy height model (chm) generation workflow that constitutes the detection, classification and interpolation of ground points using a combination of local minima filters, supervised machine learning algorithms and tin densification for classifying ground points using spectral and geometrical features from the uav-based 3d data. we demonstrate that our dap-based method provides estimates of tree heights that are identical to lidar-based approaches (conservatively estimated nse = 0.88, rmse = 1.6 m). an external validation shows that our method is capable of providing accurate and precise estimates of tree heights and agb in dense tropical forests (dap vs. field inventories of old forest: r2 = 0.913, rmse = 31.93 mg ha−1). overall, this study demonstrates that the application of cheap and easily deployable uav-dap platforms can be deployed without expert knowledge to generate biophysical information and advance the study and monitoring of dense tropical forests.', 'mapping tree species in natural and planted forests using sentinel-2 images using remote-sensing technology to accurately map the composition and distribution of tree species is vital for sustainable forest resource management. sentinel-2 data with the dense time-series observations enable to identify tree species. however, few studies clarify the differences in classification using sentinel-2 images in natural forest and planted forest. two study areas with different forest environments (planted forest and natural forest) were selected to evaluate the potential of sentinel-2 imagery. our results show that red-edge band, short-wave infrared (swir) band and vegetation indices (vis), such as red-edge normalized difference vegetation index (rendvi), soil-adjusted vegetation index (savi), and enhanced vegetation index (evi), have important effects on tree species classification, especially in the growing season. by using two machine learning algorithms (support vector machine [svm] and random forest [rf]), the results show that the classification accuracy for planted forests (91.27% in svm and 88.35% in rf) significantly exceeds that of natural forests (84.34% in svm and 81.03% in rf). this accuracy difference may be related to the spatial heterogeneity inside the forest and the surrounding environmental implications. although the multi-temporal sentinel-2 images produce satisfactory accuracy for classifying tree species, further research is needed to improve the classification accuracy.', 'application of machine learning to tree species classification using active and passive remote sensing: a case study of the duraer forestry zone the technology of remote sensing-assisted tree species classification is increasingly developing, but the rapid refinement of tree species classification on a large scale is still challenging. as one of the treasures of ecological resources in china, arxan has 80% forest cover, and tree species classification surveys guarantee ecological environment management and sustainable development. in this study, we identified tree species in three samples within the arxan duraer forestry zone based on the spectral, textural, and topographic features of unmanned aerial vehicle (uav) multispectral remote sensing imagery and light detection and ranging (lidar) point cloud data as classification variables to distinguish among birch, larch, and nonforest areas. the best extracted classification variables were combined to compare the accuracy of the random forest (rf), support vector machine (svm), and classification and regression tree (cart) methodologies for classifying species into three sample strips in the arxan duraer forestry zone. furthermore, the effect on the overall classification results of adding a canopy height model (chm) was investigated based on spectral and texture feature classification combined with field measurement data to improve the accuracy. the results showed that the overall accuracy of the rf was 79%, and the kappa coefficient was 0.63. after adding the chm extracted from the point cloud data, the overall accuracy was improved by 7%, and the kappa coefficient increased to 0.75. the overall accuracy of the cart model was 78%, and the kappa coefficient was 0.63; the overall accuracy of the svm was 81%, and the kappa coefficient was 0.67; and the overall accuracy of the rf was 86%, and the kappa coefficient was 0.75. to verify whether the above results can be applied to a large area, google earth engine was used to write code to extract the features required for classification from sentinel-2 multispectral and radar topographic data (create equivalent conditions), and six tree species and one nonforest in the study area were classified using rf, with an overall accuracy of 0.98, and a kappa coefficient of 0.97. in this paper, we mainly integrate active and passive remote sensing data for forest surveying and add vertical data to a two-dimensional image to form a three-dimensional scene. the main goal of the research is not only to find schemes to improve the accuracy of tree species classification, but also to apply the results to large-scale areas. this is necessary to improve the time-consuming and labor-intensive traditional forest survey methods and to ensure the accuracy and reliability of survey data.']"
110,vegetation_ecosystem,4,78,110_land_cover_land cover_classification,"['land', 'cover', 'land cover', 'classification', 'land use', 'lulc', 'mapping', 'remote', 'remote sensing', 'svm', 'sensing', 'earth', 'use', 'use land', 'landsat', 'google earth', 'imagery', 'earth engine', 'google', 'classes', 'maps', 'cover mapping', 'overall', 'engine', 'spectral', 'lc', 'classifiers', 'change detection', 'change', 'machine']","['analysis of land use and land cover using machine learning algorithms on google earth engine for munneru river basin, india the growing human population accelerates alterations in land use and land cover (lulc) over time, putting tremendous strain on natural resources. monitoring and assessing lulc change over large areas is critical in a variety of fields, including natural resource management and climate change research. lulc change has emerged as a critical concern for policymakers and environmentalists. as the need for the reliable estimation of lulc maps from remote sensing data grows, it is critical to comprehend how different machine learning classifiers perform. the primary goal of the present study was to classify lulc on the google earth engine platform using three different machine learning algorithms—namely, support vector machine (svm), random forest (rf), and classification and regression trees (cart)—and to compare their performance using accuracy assessments. the lulc of the study area was classified via supervised classification. for improved classification accuracy, ndvi (normalized difference vegetation index) and ndwi (normalized difference water index) indices were also derived and included. for the years 2016, 2018, and 2020, multitemporal sentinel-2 and landsat-8 data with spatial resolutions of 10 m and 30 m were used for the lulc classification. ‘water bodies’, ‘forest’, ‘barren land’, ‘vegetation’, and ‘built-up’ were the major land use classes. the average overall accuracy of svm, rf, and cart classifiers for landsat-8 images was 90.88%, 94.85%, and 82.88%, respectively, and 93.8%, 95.8%, and 86.4% for sentinel-2 images. these results indicate that rf classifiers outperform both svm and cart classifiers in terms of accuracy.', 'sentinel-1 image classification using machine learning algorithms based on the support vector machine and random forest due to concerns of recent earth climate changes such as an increase of earth surface temperature and monitoring its effect on the earths surface, environmental monitoring is a necessity. environmental change monitoring in earth sciences needs land use land cover change (lulcc) modeling to investigate the impact of climate change phenomena such as droughts and floods on earth surface land cover. as land cover has a direct effect on land surface temperature (lst), the land cover mapping is an essential part of climate change modeling. in this paper, for land use land cover mapping (lulcm), image classification of sentinel-1a synthetic aperture radar (sar) ground range detected (grd) data using two machine learning algorithms including support vector machine (svm) and random forest (rf) are implemented in r programming language and compared in terms of overall accuracy for image classification. with seven different scenarios defined in this research, rf and svm classification methods show their best performance with overall accuracies of 75.30 and 75.35 percent, respectively.', 'sentinel-1 image classification for city extraction based on the support vector machine and random forest algorithms environmental change monitoring in earth sciences needs land use land cover change (lulcc) modeling to investigate the impact of climate change phenomena such as droughts and floods on earth surface land cover. as land cover has a direct impact on land surface temperature (lst), the land cover mapping is an essential part of climate change modeling. in this paper, for land use land cover mapping (lulcm), image classification of sentinel-1a synthetic aperture radar (sar) ground range detected (grd) data using two machine learning algorithms including support vector machine (svm) and random forest (rf) are implemented in r programming language and compared in terms of overall accuracy for image classification. considering eight different scenarios defined in this research, rf and svm classification methods show their best performance with overall accuracies of 90.81 and 92.09 percent respectively.']"
111,vegetation_ecosystem,4,67,111_classification_image_remote sensing_remote,"['classification', 'image', 'remote sensing', 'remote', 'sensing', 'land cover', 'images', 'land', 'cover', 'deep', 'spectral', 'features', 'cnn', 'satellite', 'feature', 'deep learning', 'registration', 'convolutional neural', 'convolutional', 'resolution', 'cover classification', 'imagery', 'multispectral', 'network', 'hyperspectral', 'neural', 'proposed', 'spatial', 'learning', 'satellite images']","['consistent registration of remote sensing images in parametric synthesized spatial transformation network objective: remote sensing image registration is a process of matching and superimposing multiple sets of images. it plays an important role in many fields such as climate change, urban change and crustal movement. currently, most remote sensing registration methods can be generally divided into two categories: traditional based methods and deep learning based methods. the traditional remote sensing registration algorithms can be labor-cost and weaken adaptive learning to cause time-consuming registration. even though the remote sensing image registration algorithms based on deep learning reduce the labor cost and improve the ability of model adaptive learning, the accuracy and the running time still need to be improved. a parametric synthesized spatial transformation network has been proposed that can be probably used for bidirectional consistent registration of remote sensing images. methods an end-to-end method is proposed for registration, which mainly includes feature extraction, feature matching and parameter regression. first, the feature extraction network has been designated based on the spatial transformation network model: the local network in the context of spatial transformation network has been more deepening via jumping connection. four sets of full convolution modules are added, each of which is composed of four full convolution layers. meanwhile, every two sets of four full convolution layers in each module are connected based on the same jumping connection structure. in order to ensure the integrity of data transmission, the beginning and the ending of each module are connected by jumping structure as well. then two parameters have been synthesized which are regressed by local network. following the process of grid generator and sampler, the input images are transformed to generate two saliency images with the same region based on affine transformation. thus, fine-tuning residual structure has been used for feature extraction to obtain the targeted feature map. next, a feature matching structure is designed to conduct bidirectional consistent matching. a matching branch is added to obtain the correlation from the source image to the target image and the correlation originated from the target image to the source image via pearson correlation coefficient. the parameter regression network with two regression parameters have been leaked out based on the regression of matching relationship in two directions to maintain the consistency of registration. at last, the grid loss function has been iterated in consistency. the optimized bidirectional consistency parameters have been calculated via weighted and synthesized regression. the final registration is completed after sampling. result the experimental results have been compared with two classical methods, which are scale-invariant feature transform (sift) and speeded up robust features(surf).simultaneously the latest methods proposed in recent three years have been compared as well, such as convolutional neural network architecture for geometric matching (cnngeo), cnn-registration (multi-temporal remote sensing image registration) and robust matching network (rmnet). registration results have illustrated that our research is qualified in qualitative visual effects and has good results in quantitative evaluation indexes. based on the aerial image dataset, the percentage of correct key points compared with the above five methods have been implemented, and the accuracy is increased by 36.2%, 75.9%, 53.6%, 29.9% and 1.7%, respectively. registration time is reduced by 9.24 s, 7.16 s, 48.29 s, 1.06 s and 4.06 s. since the gap between cnngeo method, rmnet method and the method proposed, it cannot be clearly identified via the percentage of correct keypoints(pck) evaluation index, the grid loss and the average grid loss for further comparison. compared with the above two methods, the grid loss in this research has been increased by 3.48% and 2.66%, the average grid loss has been increased by 2.67% and 0.2% respectively. the gradient of the research method and rmnet method has decreased fastest in the grid loss line chart and average grid loss line chart. it has demonstrated that the accuracy of proposed method is higher via the histogram comparison between the method proposed and the rmnet method. the improved feature extraction network has been used to replace the feature extraction network in the cnngeo method, and the pck index is increased by 4.6% compared with the original benchmark network (cnngeo). the improved matching relationship is replaced via the matching relationship in the cnngeo method. the pck index is improved by 3.9% compared with the original benchmark network. bidirectional parameter of weighted synthesis has been further improved. the pck index is increased by 14.1% compared with the original benchmark network. the experimental results have shown that the method proposed has its advantages in accuracy and efficient operation. conclusion: the registration method is applicable for three types of remote sensing image registration applications, such as temporal variation (multi-temporal), visual diversity (multi-viewpoints) and different sensors (multi-modal). the proposed algorithm has illustrated more qualified registration accuracy and registration efficiency.', 'classification of land cover usage from satellite images using deep learning algorithms earths environment and its evolution can be seen through satellite images in near real time. through satellite imagery, remote sensing data provides crucial information that can be used for a variety of applications, including image fusion, change detection, land cover classification, agriculture, mining, disaster mitigation, and monitoring climate change. the objective of this project is to propose a method for classifying satellite images according to multiple predefined land cover classes. the proposed approach involves collecting data in image format. the data is then preprocessed using data preprocessing techniques. the processed data is fed into the proposed algorithm and the obtained result is analyzed. some of the algorithms used in satellite imagery classification are u-net, random forest, deep labv3, cnn (convolutional neural network), ann(artificial neural network), resnet etc. in this project, deeplabv3 (atrous convolution) algorithm is used for land cover classification. the dataset used is the deep globe land cover classification dataset. deeplabv3 is a semantic segmentation system that uses atrous convolution to capture multi-scale context by adopting multiple atrous rates in cascade or in parallel to determine the scale of segments.', 'automatic deep learning land cover classification methods of high-resolution remotely sensed images land cover change refers to the areal change and type transformation between vegetation cover and non-vegetation cover caused by climate change and human activities, which is closely related to human survival and development, ecological environment evolution, and material energy cycle. accurate classification is the basis of land cover change while land cover change is the core of global change research. the rapid development of high-resolution remote sensing technology poses a dual challenge to the speed and accuracy of land surface classification. in recent years, the development of new artificial intelligence technology has realized the automatic segmentation of natural scene images. intelligent image processing technology has become an important force to promote the improvement of remote sensing information service level in the era of big data. the deep learning method represented by the convolution neural network also has significant advantages in the field of remote sensing image classification. to compare the impacts of deep learning model design on the classification results of high-resolution remote sensing images, this paper takes gaofen-1 images of zhengzhou city in henan province in 2019 as an example. this paper compares and studies the differences of four diverse deep learning network models, improved based on unet model, in the application of automatic land cover classification of high-resolution images. furthermore, this paper discusses the influence mechanism of encoder and decoder architecture settings, such as residual network, multi-scale loss function, skip-layer connection, and attention mechanism module, on classification accuracy. the results show that the ms-efficientunet model with multi-scale loss function, skip-layer connection, and attention mechanism module is the best for zhengzhou city land cover classification, with an overall classification accuracy, based on pixel evaluation, of 0.7981. by introducing multi-scale loss function into the decoder, the classification accuracy of the forest, water, and other categories can be effectively improved. moreover, by improving the encoder, adding skip-layer connection and attention mechanism, the classification accuracy of grassland, water, and other categories can be further improved. the results show that the powerful generalization ability of deep learning technology can effectively break through the spectral and time- phased differences between images, and realize the feature adaptive enhancement and intelligent decision- making, which has great potential in the field of high- resolution remote sensing image segmentation. however, further improvement of classification accuracy and multi-level and largescale fine classification method are still the focus of the next step. at the same time, the unification of image sequence and the expansion of training samples are also the key factors to further improve the classification accuracy.']"
112,vegetation_ecosystem,4,41,112_impervious_semantic_extraction_railway,"['impervious', 'semantic', 'extraction', 'railway', 'urban', 'green space', 'semantic segmentation', 'deep learning', 'deep', 'images', 'remote sensing', 'segmentation', 'remote', 'sensing', 'image', 'urban green', 'lcz', 'extract', 'inspection', 'automatic', 'network', 'heritage', 'detection', 'imagery', 'building', 'fusion', 'land', 'space', 'subsidence', 'green']","['automatic extraction of urban impervious surface based on sah-unet increases in the area of impervious surfaces have occurred with urbanization. such surfaces are an important indicator of urban expansion and the natural environment. the automatic extraction of impervious surface data can provide useful information for urban and regional management and planning and can contribute to the realization of the united nations sustainable development goal 11—sustainable cities and communities. this paper uses google earth engine (gee) high-resolution remote sensing images and openstreetmap (osm) data for chengdu, a typical city in china, to establish an impervious surface dataset for deep learning. to improve the extraction accuracy, the small attention hybrid unet (sah-unet) model is proposed. it is based on the unet architecture but with attention modules and a multi-scale feature fusion mechanism. finally, depthwise-separable convolutions are used to reduce the number of model parameters. the results show that, compared with other classical semantic segmentation networks, the sah-unet network has superior precision and accuracy. the final scores on the test set were as follows: accuracy = 0.9159, miou = 0.8467, f-score = 0.9117, recall = 0.9199, precision = 0.9042. this study provides support for urban sustainable development by improving the extraction of impervious surface information from remote sensing images.', 'extraction of urban impervious surface from high-resolution remote sensing imagery based on deep learning impervious surface is an important indicator of urban ecological environment, which is of great significance for urbanization and environmental quality assessment. the complexity of urban land use and the diversity of impervious surface materials make it a challenge to extract impervious surface directly from high-resolution remote sensing imagery. to meet the requirement of impervious surface extraction from high-resolution remote sensing imagery at the urban scale, a model of impervious surface extraction based on deep learning was proposed in this paper. firstly, deep convolution neural network was used to extract image features. in extracting impervious surface of complex cities, a convolution layer and a pool layer were retained. while the void convolution was introduced to increase the field of receptivity and reduce the loss of information, so that each convolution output contained a larger range of information. secondly, a probabilistic graph learning model was constructed according to its neighborhood relationship, and high-order semantic information was introduced to optimize the features to achieve accurate extraction of impervious surfaces. this paper choosed wuhan as the experimental area, and took gaofen-2 satellite remote sensing imagery as the data source to implement the proposed model for the extraction of impervious surface thematic information. the automatic extraction accuracy was 89.02% in the construction area and 95.55% in the urban-rural junction. compared with the traditional machine learning algorithms such as random forest and support vector machine, the efficiency and accuracy of the proposed deep learning method were better. statistics and analysis of the impervious surface information of the main administrative regions in wuhan showed that the proportion of impervious surface in the whole territory of wuhan was 11.43%, and the proportion of impervious surface in the core main urban area was close to 70%. additionally, the present situation and development planning characteristics of wuhan were analyzed and discussed. the impervious surface can be used as a link between urban development level and environmental quality. the distribution of impervious surface in wuhan development planning of various administrative districts is closely related to the sustainable development of the city. our findings suggest that the deep learning method is effective for the extraction of impervious surfaces from high-resolution remote sensing imagery, and can provide technical support and data reference for the construction of sponge city and ecological city.', 'an automatic extraction architecture of urban green space based on deeplabv3plus semantic segmentation model urban green space plays an important role in maintaining the balance of ecological environment and the sustainable development of the city. extracting urban green space from remote sensing imagery can provide fast and accurate reference for urban planning and management. deep learning semantic segmentation is a new exploration for image processing including remote sensing (rs) images these years. this paper describes a multilevel architecture which targets urban green space extraction from gf-2 imagery. the pillar of the architecture is semantic segmentation model (deeplabv3plus) that is used for satellite imagery classification. in this paper, we take 19687 256 pixels × 256 pixels slices from gaofen-2(gf-2) satellite images of three different cities and ground truth as training samples, and data of another city in hebei province is taken for model verification. then, five comparative methods are carried out for monitoring urban green space distribution, including ml (maximum likelihood), svm (support vector machine), object-oriented method, fcn and u-net. the accuracy indexes of each method are obtained by calculating the difference between the extraction result and the ground truth. the results shows that the architecture with deeplabv3plus outperforms the other five methods allowing us to better extract urban green space, in particular eliminating interference from farmland pixels. the architecture allows us to reach the target extraction accuracy of 89.46%. this paper is also an exploration of applying artificial intelligence technology to remote sensing image processing.']"
113,vegetation_ecosystem,4,25,113_carbon_china_gpp_carbon sink,"['carbon', 'china', 'gpp', 'carbon sink', 'sink', 'vegetation', 'terrestrial', 'carbon sequestration', 'sequestration', 'park', 'carbon sinks', 'trend', 'sinks', 'increase', 'accumulated', 'spatial', 'forest', 'carbon storage', 'stock', 'annual', 'satellite remote', 'chinas', 'npp', 'xco2', 'soc', 'biomass', 'yr', 'spatial distribution', 'vegetation carbon', 'vegetation growth']","['accelerated increase in vegetation carbon sequestration in china after 2010: a turning point resulting from climate and human interaction china has increased its vegetation coverage and enhanced its terrestrial carbon sink through ecological restoration since the end of the 20th century. however, the temporal variation in vegetation carbon sequestration remains unclear, and the relative effects of climate change and ecological restoration efforts are under debate. by integrating remote sensing and machine learning with a modelling approach, we explored the biological and physical pathways by which both climate change and human activities (e.g., ecological restoration, cropland expansion, and urbanization) have altered chinese terrestrial ecosystem structures and functions, including vegetation cover, surface heat fluxes, water flux, and vegetation carbon sequestration (defined by gross and net primary production, gpp and npp). our study indicated that during 2001–2018, gpp in china increased significantly at a rate of 49.1–53.1\xa0tgc/yr2, and the climatic and anthropogenic contributions to gpp gains were comparable (48%–56% and 44%–52%, respectively). spatially, afforestation was the dominant mechanism behind forest cover expansions in the farming-pastoral ecotone in northern china, on the loess plateau and in the southwest karst region, whereas climate change promoted vegetation cover in most parts of southeastern china. at the same time, the increasing trend in npp (22.4–24.9\xa0tgc/yr2) during 2001–2018 was highly attributed to human activities (71%–81%), particularly in southern, eastern, and northeastern china. both gpp and npp showed accelerated increases after 2010 because the anthropogenic npp gains during 2001–2010 were generally offset by the climate-induced npp losses in southern china. however, after 2010, the climatic influence reversed, thus highlighting the vegetation carbon sequestration that occurs with ecological restoration.', 'spatial patterns of chinas carbon sinks estimated from the fusion of remote sensing and field-observed net primary productivity and heterotrophic respiration accurately assessing the carbon sink and spatial distribution pattern of chinas terrestrial ecosystems is of great significance to the implementation of climate change and carbon neutrality strategy. however, the views of various studies are still very controversial due to the differences in carbon sink estimation methods and data sources. in this study, vegetation net primary productivity (npp) and ecosystem heterotrophic respiration (rh) estimation models were constructed based on machine learning methods by fusing multisource data, such as remote sensing and ground observation data. the magnitude and spatial pattern of carbon sink in china from 2000 to 2018 were then revealed, and the carbon sink capacity of various ecosystems was quantitatively assessed. the main conclusions include the following: (1) the use of scale-matched carbon input and output data can help reduce the system error in carbon sink estimation. (2) chinas terrestrial ecosystem carbon sink since the twenty-first century is approximately 0.458 pg c/yr, which is equivalent to 22.72% of chinas anthropogenic carbon emissions. (3) deciduous forest has a higher carbon sink capacity than evergreen forest, while coniferous forest has a more stable carbon sink capacity than broad-leaved forest. the magnitude and spatial distribution of carbon sink in china reported in this study provides a scientific reference for achieving carbon neutrality and sustainable development.', 'ecological engineering induced carbon sinks shifting from decreasing to increasing during 1981–2019 in china substantial evidence shows that most of chinas terrestrial ecosystems are important carbon sinks. however, the nonlinear trend of the carbon sinks and their nonlinear response to driving factors are unclear. taking the net ecosystem productivity (nep) as a proxy for the ecosystem carbon sink, the nonlinear relationships between the monotonically increasing trends and decreasing to increasing shifts in the carbon sink to climate change and ecological engineering were investigated based on ensemble empirical mode decomposition (eemd) and machine learning algorithm (boosted regression tree model, brt). the results suggest that 16.75 % of the carbon sinks in china experienced a monotonic increase. additionally, 20.55 % of the carbon sinks shifted from decreasing to increasing trends, primarily after 1995, and these carbon sinks were located in the key ecological engineering areas, such as the middle reaches of the yellow river shelterbelt program area, the liaohe shelterbelt program area, the grain to green program area, and the three-north forest shelterbelt program area. moreover, carbon sinks exhibited strong spatial autocorrelation with low-low clustering in the north and high-high clustering in the south. the increase in co2 (slope of co2 < 1.8 g/m2/s/y) and solar radiation (slope of radiation >1 w/m2/y) promoted the monotonic increase in the carbon sinks in the center of china. the increase in the areas of forest and grassland shifted the carbon sink trend from decreasing to increasing in the key ecological engineering program areas, and economic development reversed the carbon sink reduction in the pearl river shelterbelt program area. these findings highlight the positive effect of ecological engineering on carbon sinks and provide adaptation strategies and guidance for china to achieve the “carbon neutrality” target.']"
114,vegetation_ecosystem,4,155,114_soil_soc_organic_soil organic,"['soil', 'soc', 'organic', 'soil organic', 'organic carbon', 'carbon', 'soils', 'stocks', 'content', 'soc stock', 'stock', 'topsoil', 'properties', 'carbon soc', 'mapping', 'spatial', 'soil properties', 'som', 'cm', 'soil carbon', 'rf', 'covariates', 'salinity', 'land', 'kg', 'total', 'rmse', 'global', 'sequestration', '30']","['spatial prediction of soil organic carbon stock in the moroccan high atlas using machine learning soil organic carbon (soc) is an essential component, which soil quality depends on. thus, understanding the spatial distribution and controlling factors of soc is paramount to achieving sustainable soil management. in this study, soc prediction for the ourika watershed in morocco was done using four machine learning (ml) algorithms: cubist, random forest (rf), support vector machine (svm), and gradient boosting machine (gbm). a total of 420 soil samples were collected at three different depths (0–10 cm, 10–20 cm, and 20–30 cm) from which soc concentration and bulk density (bd) were measured, and consequently soc stock (socs) was determined. modeling data included 88 variables incorporating environmental covariates, including soil properties, climate, topography, and remote sensing variables used as predictors. the results showed that rf (r2 = 0.79, rmse = 1.2%) and cubist (r2 = 0.77, rmse = 1.2%) were the most accurate models for predicting soc, while none of the models were satisfactory in predicting bd across the watershed. as with soc, cubist (r2 = 0.86, rmse = 11.62 t/ha) and rf (r2 = 0.79, rmse = 13.26 t/ha) exhibited the highest predictive power for socs. land use/land cover (lu/lc) was the most critical factor in predicting soc and socs, followed by soil properties and bioclimatic variables. both combinations of bioclimatic–topographic variables and soil properties–remote sensing variables were shown to improve prediction performance. our findings show that ml algorithms can be a viable tool for spatial modeling of soc in mountainous mediterranean regions, such as the study area.', 'mapping soil organic carbon stocks and trends with satellite-driven high resolution maps over south africa estimation and monitoring of soil organic carbon (soc) stocks is important for maintaining soil productivity and meeting climate change mitigation targets. current global soc maps do not provide enough detail for landscape-scale decision making, and do not allow for tracking carbon sequestration or loss over time. using an optical satellite-driven machine learning workflow, we mapped soc stocks (topsoil; 0 to 30 cm) under natural vegetation (86% of land area) over south africa at 30 m spatial resolution between 1984 and 2019. we estimate a total topsoil soc stock of 5.6 pg c with a median soc density of 6 kg c m−2 (iqr: interquartile range 2.9 kg c m−2). over 35 years, predicted soc underwent a net increase of 0.3% (relative to long-term mean) with the greatest net increases (1.7%) and decreases (−0.6%) occurring in the grassland and nama karoo biomes, respectively. at the landscape scale, soc changes of up to 25% were evident in some locations, as evidenced from fence-line contrasts, and were likely due to local management effects (e.g. woody encroachment associated with increased soc and overgrazing associated with decreased soc). our soc mapping approach exhibited lower uncertainty (r2 = 0.64; rmse = 2.5 kg c m−2) and less bias compared to previous low-resolution (250–1000 m) national soc mapping efforts (average r2 = 0.24; rmse = 3.7 kg c m−2). our trend map remains an estimate, pending repeated measures of soil samples in the same location (time-series); a global priority for tracking soc changes. while high resolution soc maps can inform land management decisions aimed at climate mitigation (natural climate solutions), potential increases in soc are likely limited by local climate and soils. it is also important that climate mitigation efforts such as planting trees balance trade-offs between carbon, biodiversity and overall ecosystem function.', 'soil organic carbon stocks and their determining factors in the dano catchment (southwest burkina faso) although the evaluation of soil organic carbon (soc) stocks across different types of land use and major reference soil groups is essential for mitigating climate change, there remains, to date, limited comprehensive understanding of whole tropical soil profiles. therefore, this study aimed to explain the amount of soc stocks in different land-use systems and across various soil groups, as well as its spatial pattern in the topsoil (0–30 cm) and subsoil (30–100 cm) within the savannah zone of burkina faso. roughly 70 soil profiles were considered along with additional auger sampling to account for spatial variation in both cropland (cr) and savannah (sa). the machine learning technique random forest regression (rfr) and multiple linear regression (mlr) were used for modeling the surface and subsurface soc stocks. for model calibration, covariates including land use, topographic, texture, and climatic data were considered as surrogate for soil forming factors. the prediction maps produced by the calibrated models were validated by an independent dataset. the results indicated that about 53% of the soc stock over 1 m depth was held in the upper 30 cm. only a marginal difference was recorded between the topsoil soc stock in sa (41.4 t c ha−1) and cr (39.1 t c ha−1) soils. for the subsoil, a significant difference (p < 0.05) was observed between the soc stock of cr soils recording about 40.2 t c ha−1 and sa soils with 26.3 t c ha−1. among the reference soil groups, the gleysols located at lower elevation positions revealed the highest soc stocks over 0–30 cm (44 t c ha−1) and 100 cm depth (86.6 t c ha−1). the stagnosols (45.2 t c ha−1) followed by the gleysols (42.7 t c ha−1) recorded the highest soc stocks over 30–100 cm. the variability of soc stock in the topsoil was primarily related to site-specific elements, such as particle-size fraction and wetness index, while its distribution in the subsoil was mainly associated with the topographical orientation represented by the slope aspect. compared to the mlr, rfr estimated mean top- and subsoil soc stocks of the catchment fairly well, along with lower statistical error metrics, though extreme values were not covered. nevertheless, the findings on soc stocks reinforce the view that the semi-arid ecosystems of west africa still offer a significant opportunity for carbon sequestration for both topsoil and subsoil, and these results represent a baseline for future modeling of soc dynamics in the region.']"
115,vegetation_ecosystem,4,67,115_gpp_fluxes_carbon_photosynthesis,"['gpp', 'fluxes', 'carbon', 'photosynthesis', 'ecosystem', 'flux', 'terrestrial', 'vegetation', 'global', 'ecosystems', 'co2', 'carbon cycle', 'gross', 'gross primary', 'ch4', 'drought', 'terrestrial ecosystems', 'eddy covariance', 'carbon fluxes', 'uncertainty', 'cycle', 'eddy', 'r2', 'gap filling', 'covariance', 'filling', 'sites', 'respiration', 'lai', 'climate']","['assessing and modeling ecosystem carbon exchange and water vapor flux of a pasture ecosystem in the temperate climate-transition zone the rising frequency of extreme weather events and global warming are greatly challeng-ing pastoral ecosystem productivity, particularly in the temperate climate-transition regions. while this could cause greater gross primary production (gpp) mainly contributed by the warm-season vegetation, the consequences for the dynamics of net ecosystem exchange (nee) and hydrological responses (e.g., evapotranspiration, et) on an ecosystem level are poorly known. here, we in-vestigated the evolution of plant phenology, nutritive value, energy balance, and carbon/water budgets of a cool-season dominated pastoral ecosystem in the temperate zone; integrating both eddy covariance (ec) flux measurement and simulation modeling-based uncertainty analysis. throughout the two-year duration (2017–2018) of this study, the entire pasture ecosystem remained a strong carbon sink (nee = −1.23 and −1.95 kg c m−2, respectively) with 74% and 62% of available energy loss explained by ec fluxes, respectively. the cumulative et was 735.8 and 796.8 mm, respectively; and the overall ecosystem water use efficiency (ewue) were calculated as 6.5 g c kg−1 water across both growing seasons. the above-ground biomass yield agreed with the cumulative gpp and was inversely correlated with grass nutritive value. the uncertainty analysis indicated that accurate ec flux gap-filling models could be constructed using support vector machine trained time-series models (nee, r2 = 0.77, rmse = 11.8; et, r2 = 0.90, rmse = 73.8). the performance benchmarking tests indicated that reddyproc-based gap-filling performance was very limiting and highly variable (nee, r2 = 0.21–0.64; et, r2 = 0.79–0.87), particularly for estimating nee. overall, the warm-season vegetation encroachment greatly filled the production gap of cool-season grasses, leading to greater cumulative nee and ewue on a system level, compared with those from many other reported field-crop or grassland studies using ec approaches. the complex and dynamic nature of grassland ecosystems greatly challenged the conventional reddyproc-based ec flux gap-filling performance. however, accurate machine learning models could be constructed for error/uncertainty control purposes and, thus, should be encouraged in future studies.', 'partitioning net carbon dioxide fluxes into photosynthesis and respiration using neural networks the eddy covariance (ec) technique is used to measure the net ecosystem exchange (nee) of co2 between ecosystems and the atmosphere, offering a unique opportunity to study ecosystem responses to climate change. nee is the difference between the total co2 release due to all respiration processes (reco), and the gross carbon uptake by photosynthesis (gpp). these two gross co2 fluxes are derived from ec measurements by applying partitioning methods that rely on physiologically based functional relationships with a limited number of environmental drivers. however, the partitioning methods applied in the global fluxnet network of ec observations do not account for the multiple co-acting factors that modulate gpp and reco flux dynamics. to overcome this limitation, we developed a hybrid data-driven approach based on combined neural networks (nnc-part). nnc-part incorporates process knowledge by introducing a photosynthetic response based on the light-use efficiency (lue) concept, and uses a comprehensive dataset of soil and micrometeorological variables as fluxes drivers. we applied the method to 36 sites from the fluxnet2015 dataset and found a high consistency in the results with those derived from other standard partitioning methods for both gpp (r2\xa0>.94) and reco (r2\xa0>.8). high consistency was also found for (a) the diurnal and seasonal patterns of fluxes and (b) the ecosystem functional responses. nnc-part performed more realistic than the traditional methods for predicting additional patterns of gross co2 fluxes, such as: (a) the gpp response to vpd, (b) direct effects of air temperature on gpp dynamics, (c) hysteresis in the diel cycle of gross co2 fluxes, (d) the sensitivity of lue to the diffuse to direct radiation ratio, and (e) the post rain respiration pulse after a long dry period. in conclusion, nnc-part is a valid data-driven approach to provide gpp and reco estimates and complementary to the existing partitioning methods.', 'tracking the seasonal and inter-annual variations of global gross primary production during last four decades using satellite near-infrared reflectance data terrestrial vegetation absorbs approximately 30% of the anthropogenic carbon dioxide (co2) emitted into the atmosphere through photosynthesis (represented by gross primary productivity, gpp) and thus effectively mitigates global warming. however, large uncertainties still remain in the global gpp estimations and their long-term trends. here we used the satellite-based near-infrared reflectance (nirv) as the proxy of gpp and generated a global long-term (1982–2018) gpp datasets (hereafter gppnirv). analysis at the site-level showed that nirv could accurately capture both the monthly and annual variations in gpp (r2 = 0.71 and 0.74 respectively) at 104 flux sites. upscaling the relationships between nirv and gpp to the global scale, the global annual gpp was estimated to be 128.3 ± 4.0 pg c yr−1 during the last four decades, which fell between the estimations from the machine-learning upscaling approach, light-use-efficiency (lue) models and processed-based models. the seasonal variation of gppnirv was also consistent with those from flux sites and models. more importantly, the inter-annual trends in gppnirv during the last four decades were consistent with those from processed-based models across latitudes, which outperformed the other gpp products. this evidence suggested that the long-term gpp datasets derived from nirv have better abilities to capture the seasonal and inter-annual variations of terrestrial gpp at the global scale. the long-term gppnirv product could be beneficial for the estimation of terrestrial carbon fluxes and for the projection of future climates.']"
116,vegetation_ecosystem,4,37,116_species_distribution_pest_bee,"['species', 'distribution', 'pest', 'bee', 'bees', 'insect', 'suitable', 'habitat', 'coffee', 'future', 'species distribution', 'genetic', 'maxent', 'occurrence', 'potential', 'suitable areas', 'outbreak', 'potential distribution', 'outbreaks', 'invasive', 'insects', 'tss', 'suitability', 'interactions', 'habitat suitability', 'niche', 'areas', 'pollination', 'climate', 'genetic diversity']","['coupling of pollination services and coffee suitability under climate change climate change will cause geographic range shifts for pollinators and major crops, with global implications for food security and rural livelihoods. however, little is known about the potential for coupled impacts of climate change on pollinators and crops. coffee production exemplifies this issue, because large losses in areas suitable for coffee production have been projected due to climate change and because coffee production is dependent on bee pollination. we modeled the potential distributions of coffee and coffee pollinators under current and future climates in latin america to understand whether future coffee-suitable areas will also be suitable for pollinators. our results suggest that coffee-suitable areas will be reduced 73–88% by 2050 across warming scenarios, a decline 46–76% greater than estimated by global assessments. mean bee richness will decline 8–18% within future coffee-suitable areas, but all are predicted to contain at least 5 bee species, and 46–59% of future coffee-suitable areas will contain 10 or more species. in our models, coffee suitability and bee richness each increase (i.e., positive coupling) in 10–22% of future coffee-suitable areas. diminished coffee suitability and bee richness (i.e., negative coupling), however, occur in 34–51% of other areas. finally, in 31–33% of the future coffee distribution areas, bee richness decreases and coffee suitability increases. assessing coupled effects of climate change on crop suitability and pollination can help target appropriate management practices, including forest conservation, shade adjustment, crop rotation, or status quo, in different regions.', 'predicting the habitat suitability of the invasive white mango scale, aulacaspis tubercularis; newstead, 1906 (hemiptera: diaspididae) using bioclimatic variables background: the white mango scale, aulacaspis tubercularis (hemiptera: diaspididae), is an invasive pest that threatens the production of several crops of commercial value including mango. though it is an important pest, little is known about its biology and ecology. specifically, information on habitat suitability of a. tubercularis occurrence and potential distribution under climate change is largely unknown. in this study, we used four ecological niche models, namely maximum entropy, random forest, generalized additive models, and classification and regression trees to predict the habitat suitability of a. tubercularis under current and future [representative concentration pathways (rcps): rcp4.5 and rcp8.5 of the year 2070] climatic scenarios, using bioclimatic variables. models performance was evaluated using the true skill statistic (tss), the area under the curve (auc), correlation (cor), and the deviance. results: all models sufficiently predicted the occurrence of a. tubercularis with high accuracy (auc ≥ 0.93, tss ≥ 0.81 and cor ≥ 0.77). the random forest algorithm had the highest accuracy among the four models (auc\xa0=\xa00.99, tss\xa0=\xa00.93, cor\xa0=\xa00.90, deviance\xa0=\xa00.26). temperature seasonality (bio4), mean temperature of the driest quarter (bio9), and precipitation seasonality (bio15) were the most important variables influencing a. tubercularis occurrence. models predictions showed that countries in east, south, and west africa are highly suitable for a. tubercularis establishment under current conditions. similarly, mexico, brazil, india, myanmar, bangladesh, thailand, laos, vietnam, and cambodia are also highly suitable for the pest to thrive. under future conditions, the suitable areas might slightly decrease in many countries of sub-saharan africa under both rcps. however, the range of expansion of a. tubercularis is projected to be higher in australia, brazil, spain, italy, and portugal under the future climatic scenarios. conclusion: the results reported here will be useful for guiding decision-making, developing an effective management strategy, and serving as an early warning tool to prevent further spread toward new areas. © 2022 society of chemical industry.', 'the use of multisource spatial data for determining the proliferation of stingless bees in kenya stingless/meliponine bees are eusocial insects whose polylactic nature enables interaction with a wide variety of wild plants and crops that enhance pollination and, hence, support ecosystem services. however, their true potential regarding pollination services and honey production is yet to be fully recognized. worldwide, there are over 800 species of meliponine bees, with over 20 species documented on the african continent. out of these, only 12 species have been well documented in kenya. moreover, interest on meliponine bees has increased amid climate change, agricultural intensification, and other anthropogenic effects. generally, stingless bees are under-researched, with no previous documented evidence of their ecological niche (en) distribution in most african countries. hence, this study sought to establish the influence of bioclimatic, topographic, and vegetation phenology on their spatial distribution and change patterns. stingless response variables from 490 sample points were collected and used in conjunction with 11 non-conflating features to build stingless ecological niche models. six machine learning-based en models were used to predict the distribution of seven stingless bees’ species combined. the results from the en models showed that annual precipitation was the most influential variable to stingless bee distribution (contributing 43.09% logit), while potential evapotranspiration and temperature seasonality contributed 21.18% of the information needed to predict the spatial distribution of stingless bees. vegetation phenology (21.36%) and topography (14.36%) had moderate effect on stingless bees’ distribution. on the other hand, high seasonality in precipitation and temperature indicated high stingless niche variability in the future (i.e. 2055). the performance of six en algorithms used to predict distribution of stingless bees was found to be “excellent” for random forest (true skills statistics (tss)\xa0=\xa00.91) and ranger (tss\xa0=\xa00.90) and “good” for generalized additive models (tss\xa0=\xa00.87), multivariate adaptive regression spline (tss\xa0=\xa00.80), and boosted regression trees (tss\xa0=\xa00.80), while they were “fair” for recursive portioning and regression trees (tss\xa0=\xa00.79). these en models could be utilized to inform stingless bee farming and insects pollinated crops by highlighting regions that provide highly suitable conditions for stingless bees. additionally, the findings could be harnessed to increase both bee and agricultural productivity and forest conservation efforts through supplementary pollination services.']"
121,vegetation_ecosystem,4,64,121_agb_forest_biomass_aboveground,"['agb', 'forest', 'biomass', 'aboveground', 'forests', 'ha', 'mg ha', 'lidar', 'mg', 'canopy', 'carbon', 'estimation', 'aboveground biomass', 'height', 'r2', 'remote sensing', 'remote', 'rmse', 'vegetation', 'sentinel', 'estimates', 'sensing', 'canopy height', 'forest aboveground', 'optical', 'landsat', 'biomass agb', 'radar', 'random forest', 'satellite']","['improving aboveground biomass maps of tropical dry forests by integrating lidar, alos palsar, climate and field data background: reliable information about the spatial distribution of aboveground biomass (agb) in tropical forests is fundamental for climate change mitigation and for maintaining carbon stocks. recent agb maps at continental and national scales have shown large uncertainties, particularly in tropical areas with high agb values. errors in agb maps are linked to the quality of plot data used to calibrate remote sensing products, and the ability of radar data to map high agb forest. here we suggest an approach to improve the accuracy of agb maps and test this approach with a case study of the tropical forests of the yucatan peninsula, where the accuracy of agb mapping is lower than other forest types in mexico. to reduce the errors in field data, national forest inventory (nfi) plots were corrected to consider small trees. temporal differences between nfi plots and imagery acquisition were addressed by considering biomass changes over time. to overcome issues related to saturation of radar backscatter, we incorporate radar texture metrics and climate data to improve the accuracy of agb maps. finally, we increased the number of sampling plots using biomass estimates derived from lidar data to assess if increasing sample size could improve the accuracy of agb estimates. results: correcting nfi plot data for both small trees and temporal differences between field and remotely sensed measurements reduced the relative error of biomass estimates by 12.2%. using a machine learning algorithm, random forest, with corrected field plot data, backscatter and surface texture from the l-band synthetic aperture radar (palsar) installed on the on the advanced land observing satellite-1 (alos), and climatic water deficit data improved the accuracy of the maps obtained in this study as compared to previous studies (r2 = 0.44 vs r2 = 0.32). however, using sample plots derived from lidar data to increase sample size did not improve accuracy of agb maps (r2 = 0.26). conclusions: this study reveals that the suggested approach has the potential to improve agb maps of tropical dry forests and shows predictors of agb that should be considered in future studies. our results highlight the importance of using ecological knowledge to correct errors associated with both the plot-level biomass estimates and the mismatch between field and remotely sensed data.', 'estimation of national forest aboveground biomass from multi-source remotely sensed dataset with machine learning algorithms in china forests are the largest terrestrial ecosystem carbon pool and provide the most important nature-based climate mitigation pathway. compared with belowground biomass (bgb) and soil carbon, aboveground biomass (agb) is more sensitive to human disturbance and climate change. therefore, accurate forest agb mapping will help us better assess the mitigation potential of forests against climate change. here, we developed six models to estimate national forest agb using six machine learning algorithms based on 52,415 spaceborne light detection and ranging (lidar) footprints and 22 environmental features for china in 2007. the results showed that the ensemble model generated by the stacking algorithm performed best with a determination coefficient (r2) of 0.76 and a root mean square error (rmse) of 22.40 mg/ha. the verifications at pixel level (r2 = 0.78, rmse = 16.08 mg/ha) and provincial level (r2 = 0.53, rmse = 14.05 mg/ha) indicated the accuracy of the estimated forest agb map is satisfactory. the forest agb density of china was estimated to be 53.16 ± 1.63 mg/ha, with a total of 11.00 ± 0.34 pg. net primary productivity (npp), normalized difference vegetation index (ndvi), enhanced vegetation index (evi), average annual rainfall, and annual temperature anomaly are the five most important environmental factors for forest agb estimation. the forest agb map we produced is expected to reduce the uncertainty of forest carbon source and sink estimations.', 'estimation of aboveground biomass in agroforestry systems over three climatic regions in west africa using sentinel-1, sentinel-2, alos, and gedi data agroforestry systems (afs) offer viable solutions for climate change because of the aboveground biomass (agb) that is maintained by the tree component. therefore, spatially explicit estimation of their agb is crucial for reporting emission reduction efforts, which can be enabled using remote sensing (rs) data and methods. however, multiple factors including the spatial distributions within the afs, their structure, their composition, and their variable extents hinder an accurate rs-assisted estimation of the agb across afs. the aim of this study is to (i) evaluate the potential of spaceborne optical, sar and lidar data for agb estimations in afs and (ii) estimate the agb of different afs in various climatic regions. the study was carried out in three climatic regions covering côte d’ivoire and burkina faso. two agb reference data sources were assessed: (i) agb estimations derived from field measurements using allometric equations and (ii) agb predictions from the gedi level 4a (l4a) product. vegetation indices and texture parameters were generated from optical (sentinel-2) and sar data (sentinel-1 and alos-2) respectively and were used as predictors. machine learning regression models were trained and evaluated by means of the coefficient of determination (r2) and the rmse. it was found that the prediction error was reduced by 31.2% after the stratification based on the climatic conditions. for the agb prediction, the combination of random forest algorithm and sentinel-1 and -2 data returned the best score. the gedi l4a product was applicable only in the guineo-congolian region, but the prediction error was approx. nine times higher than the ground truth. moreover, the agb level varied across afs including cocoa (7.51 ± 0.6 mg ha−1) and rubber (7.33 ± 0.33 mg ha−1) in the guineo-congolian region, cashew (13.78 ± 0.98 mg ha−1) and mango (12.82 ± 0.65 mg ha−1) in the guinean region. the afs farms in the sudanian region showed the highest agb level (6.59 to 82.11 mg ha−1). agb in an afs was mainly determined by the diameter (r2 = 0.45), the height (r2 = 0.13) and the tree density (r2 = 0.10). nevertheless, rs-based estimation of agb remain challenging because of the spectral similarities between afs. therefore, spatial assessment of the prediction uncertainties should complement agb maps in afs.']"
122,vegetation_ecosystem,4,50,122_forest_biomass_forests_agb,"['forest', 'biomass', 'forests', 'agb', 'tree', 'stand', 'r2', 'trees', 'dbh', 'carbon', 'mean', 'diameter', 'productivity', 'regression', 'machine', 'rf', 'forest biomass', 'machine learning', 'rmse', 'predict', 'svm', 'leaf', 'plantations', 'prediction', 'random', 'height', 'plantation', 'learning', 'species', 'climate']","['predicting eucalyptus spp. stand volume in zululand, south africa: an analysis using a stochastic gradient boosting regression ensemble with multi-source data sets accurate, reliable, and up-to-date forest stand volume information is a prerequisite for a detailed evaluation of commercial forest resources and their sustainable management. commercial forest responses to global climate change remain uncertain, and hence the mapping of stand volume as carbon sinks is fundamentally important in understanding the role of forests in stabilizing climate change effects. the aim of this study was to examine the utility of stochastic gradient boosting (sgb) and multi-source data to predict stand volume of a eucalyptus plantation in south africa. the sgb ensemble, random forest (rf), and stepwise multiple-linear regression (smlr) were used to predict eucalyptus stand volume and other related tree-structural attributes such as mean tree height and mean diameter at breast height (dbh). multi-source data consisted of spot-5 raw spectral features (four bands), 14 spectral vegetation indices, rainfall data, and stand age. when all variables were used, the sgb algorithm showed that stand volume can be accurately estimated (r2\xa0=\xa00.78 and rmse\xa0=\xa033.16\xa0m3\xa0ha−1 (23.01% of the mean)). the competing rf ensemble produced an r2 value of 0.76 and a rmse value of 37.28\xa0m3\xa0ha−1 (38.28% of the mean). smlr on the other hand, produced an r2 value of 0.65 and an rmse value of 42.50\xa0m3\xa0ha−1 (42.50% of the mean). our study further showed that eucalyptus mean tree height (r2\xa0=\xa00.83 and rmse\xa0=\xa01.63\xa0m (9.08% of the mean)) and mean diameter at breast height (r2\xa0=\xa00.74 and rmse\xa0=\xa01.06 (7.89% of the mean)) can also be reasonably predicted using sgb and multi-source data. furthermore, when the most important sgb model-selected variables were used for prediction, the predictive accuracies improved significantly for mean dbh (r2\xa0=\xa00.81 and rmse\xa0=\xa01.21\xa0cm (6.12% of the mean)), mean tree height (r2\xa0=\xa00.86 and rmse\xa0=\xa01.39\xa0m (7.02% of the mean)), and stand volume (r2\xa0=\xa00.83 and rmse\xa0=\xa029.58\xa0m3\xa0ha−1 (17.63% of the mean)). these results underscore the importance of integrating multi-source data with remotely sensed data for predicting eucalyptus stand volume and related tree-structural attributes.', 'deep learning models for improved reliability of tree aboveground biomass prediction in the tropical evergreen broadleaf forests aboveground biomass (agb) and carbon uptake of a forest are key ecological indicators for various technical and scientific applications and sustainable forest management. deep learning (dl) methods have been considered as alternative to regression techniques to increase the reliability of tree agb prediction. the objectives were to develop dl models to predict agb in the tropical evergreen broadleaf forests and compare dl models with traditional regression equations for their reliability in agb prediction. a total of 968 individual trees were destructively sampled from fourteen 1-ha and twenty-six 0.2-ha plots distributed across five ecoregions of viet nam to get a dataset of tree predictors of diameter at breast height (dbh), tree height (h), wood density (wd) and the response variable of agb along with forest stand factors of basal area (ba) and density (n); ecological and environmental variables such as ecoregion, slope, altitude, soil type, averaged annual temperature (t), averaged annual rainfall (p) and averaged dry season length. the dl models were developed using different combinations of variables selected by factor analysis for mixed data and compared with traditional regression equations by using cross-validation. trees agb in tropical rainforest predicted by dl models had significantly higher reliability than the conventional regression equations when both had the same input variables. of the 16 developed dl models with 1 to 9 predictors, the model with 9 predictors (dbh, h, ecoregion, altitude, p, t, soil type, n and wd) was the best dl model which reduced root mean square percent error (rmspe) and mean absolute percent error (mape) by up to 7.7% and 6.1%, respectively, compared to traditional allometric equations. the dl models created in this study should be applied for measured tree data following factors of the forest stand, ecology, and environment in sampled plots to predict the tree agb and total agb, carbon on a large scale with variation in the value of these factors. thus, we recommend that the dl models apply for the measurement, reporting, and verification (mrv) system of the reducing emissions from deforestation and forest degradation (redd+) program at a large regional level, national or territorial level scale.', 'quantifying the effects of stand and climate variables on biomass of larch plantations using random forests and national forest inventory data in north and northeast china the accurate estimation of forest biomass is crucial for supporting climate change mitigation efforts such as sustainable forest management. although traditional regression models have been widely used to link stand biomass with biotic and abiotic predictors, this approach has several disadvantages, including the difficulty in dealing with data autocorrelation, model selection, and convergence. while machine learning can overcome these challenges, the application remains limited, particularly at a large scale with consideration of climate variables. this study used the random forests (rf) algorithm to estimate stand aboveground biomass (agb) and total biomass (tb) of larch (larix spp.) plantations in north and northeast china and quantified the contributions of different predictors. the data for modelling biomass were collected from 445 sample plots of the national forest inventory (nfi). a total of 22 independent variables (6 stand and 16 climate variables) were used to develop and train climate-sensitive stand biomass models. optimization of hyper parameters was implemented using grid search and 10-fold cross-validation. the coefficient of determination (r2) and root mean square error (rmse) of the rf models were 0.9845 and 3.8008 t ha−1 for agb, and 0.9836 and 5.1963 t ha−1 for tb. the cumulative contributions of stand and climate factors to stand biomass were >98% and <2%, respectively. the most crucial stand and climate variables were stand volume and annual heat-moisture index (ahm), with relative importance values of >60% and ~0.25%, respectively. the partial dependence plots illustrated the complicated relationships between climate factors and stand biomass. this study illustrated the power of rf for estimating stand biomass and understanding the effects of stand and climate factors on forest biomass. the application of rf can be useful for mapping of large-scale carbon stock.']"
123,vegetation_ecosystem,4,44,123_tree_forest_growth_forests,"['tree', 'forest', 'growth', 'forests', 'site', 'species', 'tree growth', 'tree species', 'phenological', 'stand', 'trees', 'climate', 'stands', 'vegetation', 'climatic', 'height', 'basal area', 'productivity', 'deciduous', 'sup', 'basal', 'spring', 'temperature', 'nep', 'conditions', 'diameter', 'modelling', 'fir', 'phenology', 'index']","['using machine learning to synthesize spatiotemporal data for modelling dbh-height and dbh-height-age relationships in boreal forests sustainable forest management requires the ability to accurately model forest dynamics under a changing environment, which is difficult using conventional statistical methods as many factors that interactively affect forest growth must be considered. as well, statistical model development is often limited by the lack of broad-scale repeated forest measurements needed to capture changes in 1 or more variables and the corresponding changes in forest dynamics (e.g., growth in diameter and height), while assuming other variables do not change, or their changes do not significantly affect the forest dynamics of interest. in many forested countries, comprehensive monitoring programs have amassed large amounts of diverse forest measurement data. here we propose a new approach for using artificial neural network-based machine learning to synthesize spatiotemporal tree measurement data collected over a vast area of boreal forest in central canada to model diameter at breast height (dbh)-height and dbh-height-age relationships for 6 dominant tree species. more than 30 potentially important stand structure, site, and climate variables were considered. we used an individual-based modelling approach by considering each individual tree measurement as an instance of the complex relationships modelled; together, broad-scale long-term monitoring data contain many such instances, representing considerable spatial and temporal scale variation in forest growth and growing conditions. using this approach, we significantly improved dbh-height and dbh-height-age models. and the models developed allowed us to analyze the effects of environmental conditions or changes in these conditions on forest growth. this may be the first attempt at applying this type of approach, which can be used to more accurately model, for example, forest growth, mortality, and how they are affected by changing climate in a variety of forest types.', 'modeling tree growth responses to climate change: a case study in natural deciduous mountain forests climate change has significant effects on forest ecosystems around the world. since tree diameter increment determines forest volume increment and ultimately forest production, an accurate estimate of this variable under future climate change is of great importance for sustainable forest management. in this study, we modeled tree diameter increment under the effects of current and expected future climate change, using multilayer perceptron (mlp) artificial neural networks and linear mixed-effect model in two sites of the hyrcanian forest, northern iran. using 573 monitoring fixed-area (0.1 ha) plots, we measured and calculated biotic and abiotic factors (i.e., diameter at breast height (dbh), basal area in the largest trees (bal), basal area (ba), elevation, aspect, slope, precipitation, and temperature). we investigated the effect of climate change in the year 2070 under two reference scenarios; rcp 4.5 (an intermediate scenario) and rcp 8.5 (an extreme scenario) due to the uncertainty caused by the general circulation models. according to the scenarios of climate change, the amount of annual precipitation and temperature during the study period will increase by 12.18 mm and 1.77 °c, respectively. further, the results showed that the impact of predicted climate change was not very noticeable and the growth at the end of the period decreased by only about 7% annually. the effect of precipitation and temperature on the growth rate, in fact, neutralize each other, and therefore, the growth rate does not change significantly at the end of the period compared to the beginning. based on the models’ predictions, the mlp model performed better compared to the linear mixed-effect model in predicting tree diameter increment.', 'a novel modelling approach for predicting forest growth and yield under climate change global climate is changing due to increasing anthropogenic emissions of greenhouse gases. forest managers need growth and yield models that can be used to predict future forest dynamics during the transition period of present-day forests under a changing climatic regime. in this study, we developed a forest growth and yield model that can be used to predict individual-tree growth under current and projected future climatic conditions. the model was constructed by integrating historical tree growth records with predictions from an ecological process-based model using neural networks. the new model predicts basal area (ba) and volume growth for individual trees in pure or mixed species forests. for model development, tree-growth data under current climatic conditions were obtained using over 3000 permanent sample plots from the province of nova scotia, canada. data to reflect tree growth under a changing climatic regime were projected with jabowa-3 (an ecological process-based model). model validation with designated data produced model efficiencies of 0.82 and 0.89 in predicting individual-tree ba and volume growth. model efficiency is a relative index of model performance, where 1 indicates an ideal fit, while values lower than zero means the predictions are no better than the average of the observations. overall mean prediction error (bias) of basal area and volume growth predictions was nominal (i.e., for ba: -0.0177 cm<sup>2</sup> 5-year-<sup>1</sup> and volume: 0.0008 m<sup>3</sup> 5-year-<sup>1</sup>). model variability described by root mean squared error (rmse) in basal area prediction was 40.53 cm<sup>2</sup> 5-year-<sup>1</sup> and 0.0393 m<sup>3</sup> 5-year-<sup>1</sup> in volume prediction. the new modelling approach has potential to reduce uncertainties in growth and yield predictions under different climate change scenarios. this novel approach provides an avenue for forest managers to generate required information for the management of forests in transitional periods of climate change. artificial intelligence technology has substantial potential in forest modelling.']"
124,vegetation_ecosystem,4,54,124_forest_forests_forest management_species,"['forest', 'forests', 'forest management', 'species', 'bark', 'bark beetle', 'beetle', 'regeneration', 'climate', 'forestry', 'management', 'change', 'disturbance', 'climate change', 'tree', 'pine', 'stand', 'disturbances', 'resilience', 'stands', 'reforestation', 'decision support', 'storm', 'succession', 'landscape', 'europe', 'ecosystem services', 'future', 'wood', 'decision']","['do bark beetle outbreaks amplify or dampen future bark beetle disturbances in central europe? bark beetle outbreaks have intensified in many forests around the globe in recent years. yet, the legacy of these disturbances for future forest development remains unclear. bark beetle disturbances are expected to increase further because of climate change. consequently, feedbacks within the disturbance regime are of growing interest, for example, whether bark beetle outbreaks are amplifying future bark beetle activity (through the initiation of an even-aged cohort of trees) or dampening it (through increased structural and compositional diversity). we studied bark beetle–vegetation–climate interactions in the bavarian forest national park (germany), an area characterised by unprecedented bark beetle activity in the recent past. we simulated the effect of future bark beetle outbreaks on forest structure and composition and analysed how disturbance-mediated forest dynamics influence future bark beetle activity under different scenarios of climate change. we used process-based simulation modelling in combination with machine learning to disentangle the long-term interactions between vegetation, climate and bark beetles at the landscape scale. disturbances by the european spruce bark beetle were strongly amplified by climate change, increasing between 59% and 221% compared to reference climate. bark beetle outbreaks reduced the dominance of norway spruce (picea abies (l.) karst.) on the landscape, increasing compositional diversity. disturbances decreased structural diversity within stands (α diversity) and increased structural diversity between stands (β diversity). overall, disturbance-mediated changes in forest structure and composition dampened future disturbance activity (a reduction of up to −67%), but were not able to fully compensate for the amplifying effect of climate change. synthesis. our findings indicate that the recent disturbance episode at the bavarian forest national park was caused by a convergence of highly susceptible forest structures with climatic conditions favourable for bark beetle outbreaks. while future climate is increasingly conducive to massive outbreaks, the emerging landscape structure is less and less likely to support them. this study improves our understanding of the long-term legacies of ongoing bark beetle disturbances in central europe. it indicates that increased diversity provides an important dampening feedback, and suggests that preventing disturbances or homogenizing post-disturbance forests could elevate the future susceptibility to large-scale bark beetle outbreaks.', 'implementing climate change and associated future timber price trends in a decision support system designed for irish forest management and applied to irelands western peatland forests research highlights: predicting impacts on forest management of climate change (cc) and dynamic timber prices by incorporating these external factors in a forest management decision support system (fmdss). background and objectives: forest managers must comply with sustainable forest management (sfm) practices, including considering the long-term impacts that cc and the bioeconomy may have on their forests and their management. the aims of this study are: (1) incorporate the effects of cc and dynamic prices (dp) in a fmdss that was developed for irelands peatland forests, (2) analyse the impact of global climate and market scenarios on forest management and forest composition at the landscape level. materials and methods: remsoftwoodstock is a strategic planning decision support system that is widely used for forest management around the world. a linear programming model was developed for irelands western peatland forests while using woodstock. data from climadapt, which is an expert-based decision support system that was developed in ireland, were used to include cc effects on forest productivity and species suitability. dynamic market prices were also included to reflect the changing demands for wood fibre as part of the european union (eu) and global effort to mitigate cc. results: dp will likely have more impact on harvest patterns, volumes, and net present value than cc. higher assortment prices, especially for pulpwood, stimulate the harvesting of forests on marginal sites and off-set some of the negative cc growth impacts on forest profitability. conclusions: incorporating cc and bioeconomy prices in a forest decision support system is feasible and recommendable. foresters should incorporate the expected global changes in their long-term management planning to mitigate the negative effects that un-informed management decisions can have on the sustainability of their forests.', 'spatial pattern of climate change effects on lithuanian forestry research highlights: validating modelling approach which combines global framework conditions in the form of climate and policy scenarios with the use of forest decision support system to assess climate change impacts on the sustainability of forest management. background and objectives: forests and forestry have been confirmed to be sensitive to climate. on the other hand, human efforts to mitigate climate change influence forests and forest management. to facilitate the evaluation of future sustainability of forest management, decision support systems are applied. our aims are to: (1) adopt and validate decision support tool to incorporate climate change and its mitigation impacts on forest growth, global timber demands and prices for simulating future trends of forest ecosystem services in lithuania, (2) determine the magnitude and spatial patterns of climate change effects on lithuanian forests and forest management in the future, supposing that current forestry practices are continued. materials and methods: upgraded version of lithuanian forestry simulator kupolis was used to model the development of all forests in the country until 2120 under management conditions of three climate change scenarios. selected stand-level forest and forest management characteristics were aggregated to the level of regional branches of the state forest enterprise and analyzed for the spatial and temporal patterns of climate change effects. results: increased forest growth under a warmer future climate resulted in larger tree dimensions, volumes of growing stock, naturally dying trees, harvested assortments, and also higher profits from forestry activities. negative impacts were detected for the share of broadleaved tree species in the standing volume and the tree species diversity. climate change effects resulted in spatially clustered patterns-increasing stand productivity, and amounts of harvested timber were concentrated in the regions with dominating coniferous species, while the same areas were exposed to negative dynamics of biodiversity-related forest attributes. current forest characteristics explained 70% or more of the variance of climate change effects on key forest and forest management attributes. conclusions: using forest decision support systems, climate change scenarios and considering the balance of delivered ecosystem services is suggested as a methodological framework for validating forest management alternatives aiming for more adaptiveness in lithuanian forestry.']"
126,vegetation_ecosystem,4,60,126_species_distribution_habitat_species distribution,"['species', 'distribution', 'habitat', 'species distribution', 'suitability', 'suitable', 'future', 'maxent', 'potential distribution', 'climate', 'current', 'climate change', 'scenarios', 'change', 'niche', 'predicted', 'potential', 'distributions', 'current future', 'habitats', 'diversity', 'vegetation', 'areas', 'plant', 'invasive', 'change scenarios', 'range', 'plant species', '2070', 'habitat suitability']","['modeling species distribution of shorea guiso (blanco) blume and parashorea malaanonan (blanco) merr in mount makiling forest reserve using maxent climate change is regarded as one of the most significant drivers of biodiversity loss and altered forest ecosystems. this study aimed to model the current species distribution of two dipterocarp species in mount makiling forest reserve as well as the future distribution under different climate emission scenarios and global climate models. a machine-learning algorithm based on the principle of maximum entropy (maxent) was used to generate the potential distributions of two dipterocarp species-shorea guiso and parashorea malaanonan. the species occurrence records of these species and sets of bioclimatic and physical variables were used in maxent to predict the current and future distribution of these dipterocarp species. the variables were initially reduced and selected using principal component analysis (pca). moreover, two global climate models (gcms) and climate emission scenarios (rcp4.5 and rcp8.5) projected to 2050 and 2070 were utilized in the study. the maxent models predict that suitable areas for p. malaanonan will decline by 2050 and 2070 under rcp4.5 and rcp 8.5. on the other hand, s. guiso was found to benefit from future climate with increasing suitable areas. the findings of this study will provide initial understanding on how climate change affects the distribution of threatened species such as dipterocarps. it can also be used to aid decision-making process to better conserve the potential habitat of these species in current and future climate scenarios.', 'species distribution modelling under climate change scenarios for maritime pine (pinus pinaster aiton) in portugal to date, a variety of species potential distribution mapping approaches have been used, and the agreement in maps produced with different methodological approaches should be assessed. the aims of this study were: (1) to model maritime pine potential distributions for the present and for the future under two climate change scenarios using the machine learning maximum entropy algorithm (maxent); (2) to update the species ecological envelope maps using the same environmental data set and climate change scenarios; and (3) to perform an agreement analysis for the species distribution maps produced with both methodological approaches. the species distribution maps produced by each of the methodological approaches under study were reclassified into presence–absence binary maps of species to perform the agreement analysis. the results showed that the maxent-predicted map for the present matched well the species’ current distribution, but the species ecological envelope map, also for the present, was closer to the species’ empiric potential distribution. climate change impacts on the species’ future distributions maps using the maxent were moderate, but areas were relocated. the 47.3% suitability area (regular-medium-high), in the present, increased in future climate change scenarios to 48.7%–48.3%. conversely, the impacts in species ecological envelopes maps were higher and with greater future losses than the latter. the 76.5% suitability area (regular-favourable-optimum), in the present, decreased in future climate change scenarios to 58.2%–51.6%. the two approaches combination resulted in a 44% concordance for the species occupancy in the present, decreasing around 30%–35% in the future under the climate change scenarios. both methodologies proved to be complementary to set species’ best suitability areas, which are key as support decision tools for planning afforestation and forest management to attain fire-resilient landscapes, enhanced forest ecosystems biodiversity, functionality and productivity.', 'species ecological envelopes under climate change scenarios: a case study for the main two wood-production forest species in portugal species ecological envelope maps were obtained for the two main portuguese wood-production species (eucalyptus globulus labill. and pinus pinaster aiton) and projected future climate change scenarios. a machine learning approach was used to understand the most influential environmental variables that may explain current species distribution and productivity. background and objectives: the aims of the study were: (1) to map species potential suitability areas using ecological envelopes in the present and to project them in the future under climate change scenarios; (2) to map species current distributions; (3) to map species current productivity; and (4) to explore the most influential environmental variables on species current distribution and productivity. materials and methods: climate, elevation data, and soil data sets were used to obtain present and future species ecological envelopes under two climate change scenarios. the official land cover maps were used to map species distributions. forest inventory data were used to map the species productivity by geostatistical techniques. a bayesian machine learning approach, supported by species distributions and productivity data, was used to explore the most influential environmental variables on species distribution and productivity and to validate species ecological envelopes. results: the species ecological envelope methodology was found to be robust. species ecological envelopes showed a high potential for both species afforestation. in the future, a decrease in the countrys area potentiality was forecasted for both species. the distribution of maritime pine was found to be mainly determined by precipitation-related variables, but the elevation and temperature-related variables were very important to differentiate species productivity. for eucalypts, species distribution was mainly explained by temperature-related variables, as well as the species productivity. conclusions: these findings are key to support recommendations for future afforestation and will bring value to policy-makers and environmental authorities in policy formulation under climate change scenarios.']"
127,vegetation_ecosystem,4,91,127_species_habitat_conservation_distribution,"['species', 'habitat', 'conservation', 'distribution', 'species distribution', 'distributions', 'range', 'niche', 'biodiversity', 'protected', 'suitability', 'ecological', 'climate', 'habitat suitability', 'sdms', 'change', 'protected areas', 'sdm', 'shifts', 'suitable', 'presence', 'climate change', 'areas', 'habitats', 'pseudo', 'migratory', 'birds', 'future', 'suitable habitat', 'modelling']","['what will remain? predicting the representation in protected areas of suitable habitat for endangered tropical avifauna in borneo under a combined climate-and land-use change scenario the responses of threatened tropical avian species to projected climate change and land-use change are important for evaluating the ability of the existing protected areas to provide habitat to these species under future scenarios in biodiversity hotspots. this study uses maxent, a species distribution model that employs a maximum entropy machine learning approach to map the spatial distributions of habitats suitable for the international union for conservation of nature threatened birds under present and future climate and land-use change in borneo. we find that the existing protected areas provide very low coverage of the threatened bird species’ suitable habitat areas (95%ci = 9.3–15.4%). analysis of habitat suitability projections for 18 species of threatened birds suggests that in 2050, under special report on emissions scenarios a1b and b1, avian species with currently little suitable habitat may gain area but lose in the proportion of this that is protected. large-ranged species are likely to lose habitat area and this will inflate the proportion of this remaining in protected areas. the present availability of suitable habitat was the most important determinant of future habitat availability under both the scenarios. threat level, as measured by the international union for conservation of nature and the habitat preferences considered here, lowland or lowland–montane, are poor predictors of the amount of habitat contraction or expansion undergone by the species.', 'species distribution modelling for conservation planning in victoria of australia biodiversity and natural ecosystems are under pressure from a large range of drivers. in south eastern australia these include periods of drought and occasional floods, increasing urbanisation pressures, changing land use patterns, altered fire frequency and regime, habitat loss and degradation, impacts of invasive species, etc. detailed and reliable information about the spatial distribution of species provides critical information for effective conservation planning. in victoria, australia, this responsibility is held by the department of sustainability and environment, which maintains and curates a database of site records for species: the victorian biodiversity atlas (vba). although this database is comprehensive and contains many records for numerous species, the information is provided in point form. this point data can be useful for some planning and management tasks, but does not provide an adequate view of distribution for all management purposes. by integrating known occurrences of species with environmental gis data layers using statistical or machine learning algorithms, it is possible to build models and to predict the likelihood that the species may occur at particular locations across large landscapes. we have used the vba to build species distribution models (sdms) for 523 vertebrate fauna species across the whole state, providing predictive maps that are available for use in various conservation planning activities. in this paper, we introduce and discuss the methods we developed and implemented for producing these models. modelling requires a number of steps: gathering species occurrence data, vetting or mining this data for useful records and winnowing out potentially misleading records, collating existing and / or developing new relevant environmental variables, selecting an appropriate modelling algorithm, training and evaluating models, mapping predictions to the whole state scale, and selecting a threshold to transform the continuous model result into binary (i.e. presence/absence) mapped outputs. all the occurrence data for the 523 fauna species were derived from the vba. this database contains presence data for fauna species, but for the majority of survey techniques information on absences cannot be known or inferred. twenty four explanatory variables were selected for use in developing models including 12 bioclimatic, terrain and soil radiometric (btsr) variables, two coordinates (latitude and longitude), and 10 remotely sensed variables primarily derived from a time series of landsat imagery. for each species, three versions of models (v1, v2 and v3 models) reflecting different constraint were built. v1 models used 12 btsr variables; v2 models used 12 btsr variables and two coordinates; and v3 models used all the 24 variables. since true absence data was not available, pseudo-absences were created by filtering random points by a profile model based on mahalanobis distance method. using both pseudo-absences and the true presences data sdms were developed with a machine learning algorithm, random forest, which was implemented in the statistical computing environment r. the models were evaluated using six accuracy measures, which have been demonstrated suitable for evaluating presence-only models. these metrics include aul (area under the lift curve), mvdl (the maximum vertical distance from lift curve to the diagonal line), mpa (minimal predicted area with pre-specified true positive rate), vdl (vertical distance from lift curve to the diagonal line), lift and se (sensitivity). these measures can be used for both within-species and among-species model comparisons. a threshold was set for each model with a method based on maximising the sum of sensitivity and specificity to provide binary outputs or maps of species distributions.', 'multi-scale habitat modelling and predicting change in the distribution of tiger and leopard using random forest algorithm tigers and leopards have experienced considerable declines in their population due to habitat loss and fragmentation across their historical ranges. multi-scale habitat suitability models (hsm) can inform forest managers to aim their conservation efforts at increasing the suitable habitat for tigers by providing information regarding the scale-dependent habitat-species relationships. however the current gap of knowledge about ecological relationships driving species distribution reduces the applicability of traditional and classical statistical approaches such as generalized linear models (glms), or occupancy surveys to produce accurate predictive maps. this study investigates the multi-scale habitat relationships of tigers and leopards and the impacts of future climate change on their distribution using a machine-learning algorithm random forest (rf). the recent advancements in the machine-learning algorithms provide a powerful tool for building accurate predictive models of species distribution and their habitat relationships even when little ecological knowledge is available about the species. we collected species occurrence data using camera traps and indirect evidence of animal presences (scats) in the field over 2\xa0years of rigorous sampling and used a machine-learning algorithm random forest (rf) to predict the habitat suitability maps of tiger and leopard under current and future climatic scenarios. we developed niche overlap models based on the recently developed statistical approaches to assess the patterns of niche similarity between tigers and leopards. tiger and leopard utilized habitat resources at the broadest spatial scales (28,000\xa0m). our model predicted a 23% loss in the suitable habitat of tigers under the rcp 8.5 scenario (2050). our study of multi-scale habitat suitability modeling provides valuable information on the species habitat relationships in disturbed and human-dominated landscapes concerning two large felid species of conservation importance. these areas may act as refugee habitats for large carnivores in the future and thus should be the focus of conservation importance. this study may also provide a methodological framework for similar multi-scale and multi-species monitoring programs using robust and more accurate machine learning algorithms such as random forest.']"
13,agriculture,5,32,13_manure_caps_production_straw,"['manure', 'caps', 'production', 'straw', 'environmental', 'farm', 'agricultural', 'environmental impacts', 'neural', 'ann', 'management', 'soybean', 'cycle assessment', 'agriculture', 'cultivation', 'impacts', 'artificial', 'different', 'artificial neural', 'fertilizer', 'application', 'human health', 'greenhouse', 'life cycle', 'anfis', 'paddy', 'nitrogen', 'output', 'decomposition', 'scenario']","['a comparative of modeling techniques and life cycle assessment for prediction of output energy, economic profit, and global warming potential for wheat farms uncertainty about the energy use efficiency, lack of knowledge about economic outcomes, and its environmental consequences have always take risks in changing cultivation patterns and moving towards the optimal path. accordingly, this study provided mathematical, artificial neural networks (anns), adaptive neuro-fuzzy inference system (anfis) methods to predict output energy, economic profit, and global warming potential (gwp) of wheat production. for this purpose, 75 wheat farms located in the central area of hamadan province, iran, were selected randomly, and data were gathered through oral interviews. after collecting input and output energies data, the averages of inputs and outputs energies were obtained about 43055 mj ha−1 and 117407 mj ha−1, respectively. economic analysis has performed in the next step. its results revealed that the benefit-to-cost ratio and net return were computed about 2.33 and 488.29 $ per ha for wheat production. then, life cycle assessment (lca) was utilized to specify the environmental effects of wheat cultivation, and its results demonstrated that gwp is the most important environmental impact which caused 624.29 kg co2eq. during 1 ton of wheat production. modeling results illustrated r2 was varied between 0.264 and 0.978 in the linear regression, 0.313 and 954 in the best structure of ann with two hidden layers, and 0.520 and 0.962 in the anfis with three-level structure. modeling comparison indicated that generally, anfis model with considering all uncertainty items can be offered better prediction models among all and after that ann with considering non-linear parameters is in the next rank.', 'sustainable systems engineering using life cycle assessment: application of artificial intelligence for predicting agro-environmental footprint the increase in population has increased the need for agricultural and food products, and thus agricultural production should be increased. this goal may cause increases in emissions and environmental impacts by increasing the consumption of agricultural inputs. the prediction of environmental impacts plays an important role in evaluating pollutant emissions in crop production. this study employed two artificial intelligence (ai) methods: the adaptive neuro-fuzzy inference system–fuzzy c-means (anfis–fcm) algorithm as a novel computational method, and an artificial neural network (ann) as a conventional computational method to predict the environmental impacts of soybean production in different scenarios (i.e., soybean cultivation after rapeseed (r-s), wheat (w-s), and fallow (f-s)). the life cycle of soybean production was assessed in terms of environmental impacts through the impact2002+ method in simapro. in the present study, the production of one ton of soybeans was considered the functional unit, and the boundary of the system was considered the gate of the field. according to the results, the production of each ton of soybean in the defined scenarios resulted in 0.0009 to 0.0016 daly, 5476.18 to 8799.80 mj primary, 1033.68 to 1840.70 pdf × m2 × yr, and 563.55 to 880.61 kg co2-eq damage to human health, resources, ecosystem quality, and climate change, respectively. moreover, the weighted analysis indicated that various soybean production scenarios led to 293.87–503.73 mpt damage to the environment, in which the r-s scenario had the best environmental performance. according to the results, the anfis–fcm algorithm acted as the best prediction model of environmental indicators for soybean cultivation in all cases related to the ann. the range of calculated r2 for the anfis-fcm and ann models were between 0.9967 to 0.9989 and 0.9269 to 0.9870, respectively. it can be concluded that the proposed anfis–fcm model is an efficient technique for obtaining accurate environmental prediction parameters of soybean cultivation.', 'integration of artificial intelligence methods and life cycle assessment to predict energy output and environmental impacts of paddy production prediction of agricultural energy output and environmental impacts play important role in energy management and conservation of environment as it can help us to evaluate agricultural energy efficiency, conduct crops production system commissioning, and detect and diagnose faults of crop production system. agricultural energy output and environmental impacts can be readily predicted by artificial intelligence (ai), owing to the ease of use and adaptability to seek optimal solutions in a rapid manner as well as the use of historical data to predict future agricultural energy use pattern under constraints. this paper conducts energy output and environmental impact prediction of paddy production in guilan province, iran based on two ai methods, artificial neural networks (anns), and adaptive neuro fuzzy inference system (anfis). the amounts of energy input and output are 51,585.61 mj kg−1 and 66,112.94 mj kg−1, respectively, in paddy production. life cycle assessment (lca) is used to evaluate environmental impacts of paddy production. results show that, in paddy production, in-farm emission is a hotspot in global warming, acidification and eutrophication impact categories. ann model with 12-6-8-1 structure is selected as the best one for predicting energy output. the correlation coefficient (r) varies from 0.524 to 0.999 in training for energy input and environmental impacts in ann models. anfis model is developed based on a hybrid learning algorithm, with r for predicting output energy being 0.860 and, for environmental impacts, varying from 0.944 to 0.997. results indicate that the multi-level anfis is a useful tool to managers for large-scale planning in forecasting energy output and environmental indices of agricultural production systems owing to its higher speed of computation processes compared to ann model, despite anns higher accuracy.']"
14,agriculture,5,58,14_crop_rice_remote sensing_sensing,"['crop', 'rice', 'remote sensing', 'sensing', 'remote', 'sentinel', 'crops', 'classification', 'winter', 'wheat', 'winter wheat', 'yield', 'hyperspectral', 'vegetation', 'imagery', 'mapping', 'crop types', 'spectral', 'food', 'bands', 'palm', 'agricultural', 'uav', 'planting', 'using', 'field', 'resolution', 'landsat', 'growth stages', 'phenotyping']","['modeling of winter wheat fapar by integrating unmanned aircraft vehicle-based optical, structural and thermal measurement the fraction of absorbed photosynthetically active radiation (fapar) is a critical biophysical parameter for crop growth monitoring and yield estimation. remote sensing provides an efficient way for measuring fapar over large areas, compared with the time-consuming and labor-intensive field measurements. however, the optical remote sensing signals usually saturate over dense vegetation (e.g., leaf area index (lai) > 5 or fapar > 0.7), limiting the performance of optical remote sensing in modeling fapar. multi-source remote sensing data fusion has proven to be a feasible method to overcome the saturation problem of optical remote sensing in vegetation monitoring, but little is known about the performance of optical, structural and thermal features fusion for modeling winter wheat fapar. also, the modeling powers of optical, structural, and thermal features for fapar estimation have seldom been compared. to fill in these knowledge gaps, the very high spatial resolution rgb-optical and thermal imagery collected by unmanned aircraft vehicle (uav) were used to quantify the powers of rgb-derived vegetation indices (vis), structural indices (sis, crop height/canopy cover), and canopy temperature (ct) and their combinations in modeling winter wheat fapar in this study. the modeling powers of different remote sensing features were compared with the commonly used hyperspectral vegetation indices (hvis) from field spectrometer measurements. results showed that (1) multi-source data fusion that integrates optical, structural, and thermal features provided the best model in winter wheat fapar mapping (r2 = 0.907 and rmse = 0.041); (2) the rgb imagery-derived optical (i.e., rgb vis) and structural features (i.e., rgb sis) were important preditors for winter wheat fapar modeling, and their combination can steadily improve the modeling accuracy (~2% improvement in r2 compared to optical-only model); (3) the thermal feature alone performed the worst among all experiments, but it still can complement other types of remote sensing features (i.e., rgb vis&sis) and further improve the modeling accuracy within the framework of data fusion (~3% improvement in r2 compared to optical-only model). in general, this study indicates that the framework of multi-source remote sensing data fusion can provide more accurate, efficient measurements of winter wheat fapar for crop management in precision agriculture, which can help improve resource utilization efficiency (e.g., determine where and when to apply nitrogen fertilizer) and ensure food security in the face of climate change.', 'mapping a cloud-free rice growth stages using the integration of proba-v and sentinel-1 and its temporal correlation with sub-district statistics monitoring rice production is essential for securing food security against climate change threats, such as drought and flood events becoming more intense and frequent. the current practice to survey an area of rice production manually and in near real-time is expensive and involves a high workload for local statisticians. remote sensing technology with satellite-based sensors has grown in popularity in recent decades as an alternative approach, reducing the cost and time required for spatial analysis over a wide area. however, cloud-free pixels of optical imagery are required to pro-duce accurate outputs for agriculture applications. thus, in this study, we propose an integration of optical (proba-v) and radar (sentinel-1) imagery for temporal mapping of rice growth stages, including bare land, vegetative, reproductive, and ripening stages. we have built classification models for both sensors and combined them into 12-day periodical rice growth-stage maps from january 2017 to september 2018 at the sub-district level over java island, the top rice production area in indonesia. the accuracy measurement was based on the test dataset and the predicted cross-correlated with monthly local statistics. the overall accuracy of the rice growth-stage model of proba-v was 83.87%, and the sentinel-1 model was 71.74% with the support vector machine clas-sifier. the temporal maps were comparable with local statistics, with an average correlation between the vegetative area (remote sensing) and harvested area (local statistics) is 0.50, and lag time 89.5 days (n = 91). this result was similar to local statistics data, which correlate planting and the harvested area at 0.61, and the lag time as 90.4 days, respectively. moreover, the cross-correlation between the predicted rice growth stage was also consistent with rice development in the area (r > 0.52, p < 0.01). this novel method is straightforward, easy to replicate and apply to other areas, and can be scaled up to the national and regional level to be used by stakeholders to support improved agricultural policies for sustainable rice production.', 'automatic mapping of rice growth stages using the integration of sentinel-2, mod13q1, and sentinel-1 rice (oryza sativa l.) is a staple food crop for more than half of the world’s population. rice production is facing a myriad of problems, including water shortage, climate, and land-use change. accurate maps of rice growth stages are critical for monitoring rice production and assessing its impacts on national and global food security. rice growth stages are typically monitored by coarse-resolution satellite imagery. however, it is difficult to accurately map due to the occurrence of mixed pixels in fragmented and patchy rice fields, as well as cloud cover, particularly in tropical countries. to solve these problems, we developed an automated mapping workflow to produce near real-time multi-temporal maps of rice growth stages at a 10-m spatial resolution using multisource remote sensing data (sentinel-2, mod13q1, and sentinel-1). this study was investigated between 1 june and 29 september 2018 in two (wet and dry) areas of java island in indonesia. first, we built prediction models based on sentinel-2, and fusion of mod13q1/sentinel-1 using the ground truth information. second, we applied the prediction models on all images in area and time and separation between the non-rice planting class and rice planting class over the cropping pattern. moreover, the model’s consistency on the multitemporal map with a 5–30-day lag was investigated. the result indicates that the sentinel-2 based model classification gives a high overall accuracy of 90.6% and the fusion model mod13q1/sentinel-1 shows 78.3%. the performance of multitemporal maps was consistent between time lags with an accuracy of 83.27–90.39% for sentinel-2 and 84.15% for the integration of sentinel-2/mod13q1/sentinel-1. the results from this study show that it is possible to integrate multisource remote sensing for regular monitoring of rice phenology, thereby generating spatial information to support local-, national-, and regional-scale food security applications.']"
16,agriculture,5,34,16_genomic_breeding_stress_genome,"['genomic', 'breeding', 'stress', 'genome', 'plant', 'genetic', 'traits', 'abiotic', 'food', 'selection', 'tolerance', 'genotypes', 'cultivars', 'gene', 'wheat', 'drought', 'crops', 'genes', 'cattle', 'barley', 'plants', 'protein', 'crop', 'yield', 'candidate', 'genomics', 'resistance', 'tf', 'horticultural', 'regulatory']","['canopy spectral reflectance indices correlate with yield traits variability in bread wheat genotypes under drought stress drought stress is a major issue impacting wheat growth and yield worldwide, and it is getting worse as the world’s climate changes. thus, selection for drought-adaptive traits and drought-tolerant genotypes are essential components in wheat breeding programs. the goal of this study was to explore how spectral reflectance indices (sris) and yield traits in wheat genotypes changed in irrigated and water-limited environments. in two wheat-growing seasons, we evaluated 56 preselected wheat genotypes for sris, stay green (sg), canopy temperature depression (ctd), biological yield (by), grain yield (gy), and yield contributing traits under control and drought stress, and the sris and yield traits exhibited higher heritability (h2) across the growing years. diverse sris associated with sg, pigment content, hydration status, and aboveground biomass demonstrated a consistent response to drought and a strong association with gy. under drought stress, gy had stronger phenotypic correlations with sg, ctd, and yield components than in control conditions. three primary clusters emerged from the hierarchical cluster analysis, with cluster i (15 genotypes) showing minimal changes in sris and yield traits, indicating a relatively higher level of drought tolerance than clusters ii (26 genotypes) and iii (15 genotypes). the genotypes were appropriately assigned to distinct clusters, and linear discriminant analysis (lda) demonstrated that the clusters differed significantly. it was found that the top five components explained 73% of the variation in traits in the principal component analysis, and that vegetation and water-based indices, as well as yield traits, were the most important factors in explaining genotypic drought tolerance variation. based on the current study’s findings, it can be concluded that proximal canopy reflectance sensing could be used to screen wheat genotypes for drought tolerance in water-starved environments.', 'harness the power of genomic selection and the potential of germplasm in crop breeding for global food security in the era with rapid climate change crop genetic improvements catalysed population growth, which in turn has increased the pressure for food security. we need to produce 70% more food to meet the demands of 9.5 billion people by 2050. climate changes have posed challenges for global food supply, while the narrow genetic base of elite crop cultivars has further limited our capacity to increase genetic gain through conventional breeding. the effective utilization of genetic resources in germplasm collections for crop improvement is crucial to increasing genetic gain to address challenges in the global food supply. genomic selection (gs) uses genome-wide markers and phenotype information from observed populations to establish associations, followed by genome-wide markers to predict phenotypic values in test populations. characterizing an extensive germplasm collection can serve a dual purpose in gs, as a reference population for predicting model, and mining desirable genetic variants for incorporation into elite cultivars. new technologies, such as high-throughput genotyping and phenotyping, machine learning, and gene editing, have great potential to contribute to genome-assisted breeding. breeding programmes integrating germplasm characterization, gs and emerging technologies offer promise for accelerating the development of cultivars with improved yield and enhanced resistance and tolerance to biotic and abiotic stresses. finally, scientifically informed regulations on new breeding technologies, and increased sharing of genetic resources, genomic data, and bioinformatics expertise between developed and developing economies will be the key to meeting the challenges of the rapidly changing climate and increased demand for food.', 'the application of pangenomics and machine learning in genomic selection in plants genomic selection approaches have increased the speed of plant breeding, leading to growing crop yields over the last decade. however, climate change is impacting current and future yields, resulting in the need to further accelerate breeding efforts to cope with these changing conditions. here we present approaches to accelerate plant breeding and incorporate nonadditive effects in genomic selection by applying state-of-the-art machine learning approaches. these approaches are made more powerful by the inclusion of pangenomes, which represent the entire genome content of a species. understanding the strengths and limitations of machine learning methods, compared with more traditional genomic selection efforts, is paramount to the successful application of these methods in crop breeding. we describe examples of genomic selection and pangenome-based approaches in crop breeding, discuss machine learning-specific challenges, and highlight the potential for the application of machine learning in genomic selection. we believe that careful implementation of machine learning approaches will support crop improvement to help counter the adverse outcomes of climate change on crop production.']"
19,agriculture,5,22,19_migration_insecurity_food_household,"['migration', 'insecurity', 'food', 'household', 'food insecurity', 'conflict', 'child', 'vulnerability', 'food security', 'farming systems', 'poverty', 'conflicts', 'farming', 'security', 'predictors', 'factors', 'farmers', 'households', 'livelihood', 'dryland', 'youth', 'machine learning', 'shocks', 'childhood', 'bangladesh', 'armed conflict', 'machine', 'armed', 'humanitarian', 'interventions']","['random forest analysis of two household surveys can identify important predictors of migration in bangladesh the decision to migrate is complex and is often influenced by a combination of economic, social, political, and environmental pressures. household survey instruments can capture detailed information about migration histories and their contexts, but it can be challenging to identify important predictors from large numbers of covariates with standard statistical methods, such as regression analyses. machine learning techniques are well suited to pattern identification and can identify important covariates from large datasets. we report on the application of machine learning approaches to two large surveys collected from a total of more than 2800 households in southwestern bangladesh. we applied random forest classification and regression models to identify significant covariates with the greatest predictive power for household migration decisions. the results show that random forest models are able to identify nuances in predictors of different types of migration and migration in different communities. random forests also outperform logistic regression and support vector machines in predicting migration in all cases analyzed. therefore, random forest models and other machine learning methods can be useful for improving the predictive accuracy of migration models and identifying patterns in complex social datasets. future work should continue to explore the potential of machine learning techniques applied to questions of environmental migration.', 'forecasting transitions in the state of food security with machine learning using transferable features food insecurity is a growing concern due to man-made conflicts, climate change, and economic downturns. forecasting the state of food insecurity is essential to be able to trigger early actions, for example, by humanitarian actors. to measure the actual state of food insecurity, expert and consensus-based approaches and surveys are currently used. both require substantial manpower, time, and budget. this paper introduces an extreme gradient-boosting machine learning model to forecast monthly transitions in the state of food security in ethiopia, at a spatial granularity of livelihood zones, and for lead times of one to 12 months, using open-source data. the transition in the state of food security, hereafter referred to as predictand, is represented by the integrated food security phase classification data. from 19 categories of datasets, 130 variables were derived and used as predictors of the transition in the state of food security. the predictors represent changes in climate and land, market, conflict, infrastructure, demographics and livelihood zone characteristics. the most relevant predictors are found to be food security history and surface soil moisture. overall, the model performs best for forecasting deteriorations and improvements in the state of food security compared to the baselines. the proposed method performs (f1 macro score) at least twice as well as the best baseline (a dummy classifier) for a deterioration. the model performs better when forecasting long-term (7 months; f1 macro average = 0.61) compared to short-term (3 months; f1 macro average = 0.51). combining machine learning, integrated phase classification (ipc) ratings from monitoring systems, and open data can add value to existing consensus-based forecasting approaches as this combination provides longer lead times and more regular updates. our approach can also be transferred to other countries as most of the data on the predictors are openly available from global data repositories.', 'food security analysis and forecasting: a machine learning case study in southern malawi chronic food insecurity remains a challenge globally, exacerbated by climate change-driven shocks such as droughts and floods. forecasting food insecurity levels and targeting vulnerable households is apriority for humanitarian programming to ensure timely delivery of assistance. in this study, we propose to harness a machine learning approach trained on high-frequency household survey data to infer the predictors of food insecurity and forecast household level outcomes in near real-time. our empirical analyses leverage the measurement indicators for resilience analysis (mira) data collection protocol implemented by catholic relief services (crs) in southern malawi, a series of sentinel sites collecting household data monthly. when focusing on predictors of community-level vulnerability, we show that a random forest model outperforms other algorithms and that location and self-reported welfare are the best predictors of food insecurity. we also show performance results across several neural networks and classical models for various data modeling scenarios to forecast food security. we pose that problem as binary classification via dichotomization of the food security score based on two different thresholds, which results in two different positive class to negative class ratios. our best performing model has an f1 of 81% and an accuracy of 83% in predicting food security outcomes when the outcome is dichotomized based on threshold 16 and predictor features consist of historical food security score along with 20 variables selected by artificial intelligence explainability frameworks. these results showcase the value of combining high-frequency sentinel site data with machine learning algorithms to predict future food insecurity outcomes.']"
20,agriculture,5,19,20_food_food supply_foods_agri food,"['food', 'food supply', 'foods', 'agri food', 'supply', 'food safety', 'chain', 'agri', 'supply chain', 'safety', 'bioeconomy', 'dairy', 'digital', 'resilient', 'technologies', 'products', 'food industry', 'sustainable food', 'industry', 'intake', 'health', 'security', 'infant', 'food security', 'plant based', 'feeds', 'digital technologies', 'intelligence', 'nutrition', 'project']","['the u.s. bioeconomy: charting a course for a resilient and competitive future in the nearly 50 years since the first genetic engineering experiments, the united states has become the world’s biotechnology powerhouse, with the resulting biobased and bio-enabled economy—the bioeconomy—generating at least 5.1% of u.s. gdp or more,1 with more than half of the total generated outside the biomedical sector, including the agricultural and industrial biotechnology sectors. within the next two decades, a well-developed bioeconomy will transform manufacturing processes to use the more than a billion tons of sustainable biomass and other sources of biogenic carbon in the united states rather than petroleum to make the products of modern society. doing so will reduce the nation’s dependence on fossil fuels, revitalize u.s. manufacturing and employment across the nation, create a more resilient supply chain, address concerns regarding national competitiveness and national security, improve the nation’s health and environment, and contribute significantly to the goal of creating a net-zero carbon economy. if fully utilized, those billion tons could be used by a thriving bioeconomy to generate 25% of the nation’s liquid transportation fuels and 50 billion pounds of biobased chemicals, as well as cut carbon dioxide emissions by 450 million tons and support 1.1 million u.s. jobs. indeed, the world will transition to a bioeconomy within the next two decades, and the question is whether the united states will lead the way or relinquish its current leadership position. however, decentralized leadership and a corresponding lack of a strategic vision, inadequate talent development, insufficient investment in both fundamental research and the activities that turn discovery into public benefits, and international competition put the united states at risk of forfeiting its world-leading position and squandering the entrepreneurial drive and capital market interest that is trying to expand the bioeconomy. without concrete action to address these concerns, the nation’s economy, its national security, the health of its residents, and its opportunity to move to a net-zero carbon economy that creates goodpaying jobs and keeps them in the country are in peril. schmidt futures, a philanthropic initiative of eric and wendy schmidt, convened a task force to chart a course for achieving the promise of platform technologies such as engineering biology and artificial intelligence to contribute to what has recently been projected to become a future bioeconomy worth somewhere between $4 trillion and $30 trillion dollars globally, according to the most recent projections.2 the task force deliberated on the roadblocks and focused on identifying opportunities for translating basic science research into products for the general public by enabling large-scale production of exciting bioeconomy products. this document makes recommendations for public and private action that fall into four broad categories: addressing foundational science and technology challenges; building a national infrastructure for bioproduction scale-up capacity; developing a well-trained workforce to power the bioeconomy; and enabling centralized leadership and a policy environment that incentivizes and supports a circular bioeconomy. bringing everything together, this document presents a strategy composed of three parts: a rationale and recommendations; a set of strategic actions that if implemented would enact the key aspects of the recommendations; and a series of case studies to provide additional context to the concepts discussed (fig. 1). collectively, the recommendations and strategy could be implemented in ways that do not harm the environment on a net basis and also advance equity in society, particularly as it relates to improving economic competitiveness and revitalizing underserved communities. in addition, the recommendations and strategy address the fact that most life sciences research funded by the federal government today in the united states is curiosity and discovery driven rather than application driven. as a result, the ‘‘non-academic’’ challenges, which arise in the transition of discovery to application and limit the ability to realize bioproduction goals are underfunded, underexplored, and underdeveloped in the united states. in addition, because other countries are investing in solving these challenges and developing the necessary workforce and regulations that support these activities, u.s. companies are taking their technologies overseas for production and commercialization, a situation that if continued, promises to yield the same ‘‘innovate here, produce there’’ outcome that did so much damage to the u.s. manufacturing sector in the 1980s and the people it employed..', 'artificial intelligence in extended agri-food supply chain: a short review based on bibliometric analysis climate change and population growth are triggering a digital transformation in agriculture. consequently, agri-food supply chains are becoming more intelligent, producing vast amounts of data and pushing the boundaries of the traditional food lifecycle. however, artificial intelligence (ai) for the extended agri-food supply chain is only beginning to emerge. this paper presents a short literature review of eighteen papers on the intelligent agri-food supply chain. the bibliometric analysis reveals key research clusters and current trends in the ai-enabled stages of food production, distribution, and sustainable consumption. the important advances of ai in traditional stages of production need to be expanded with intelligent planning for demand uncertainty and personalized needs of end-customers, storage optimization, waste reduction in the post-production phase (e.g., distribution and recycling), and boundary-spanning analytics. for theory, this work highlights mature areas for ai adoption in agri-food and identifies opportunities for future research in the extended agri-food supply chain. for practice, the review findings can inspire startups interested in extended agri-food ecosystems and incumbents in their pilot projects for the intelligent and sustainable digital transformation of agri-food. ai techniques can contribute to close the loop of sustainable agri-food supply chains.', 'building a resilient, sustainable, and healthier food supply through innovation and technology the modern food supply faces many challenges. the global population continues to grow and people are becoming wealthier, so the food production system must respond by creating enough high-quality food to feed everyone with minimal damage to our environment. the number of people suffering or dying from diet-related chronic diseases, such as obesity, diabetes, heart disease, stroke, and cancer, continues to rise, which is partly linked to overconsumption of highly processed foods, especially high-calorie or rapidly digestible foods. after falling for many years, the number of people suffering from starvation or malnutrition is rising, and thishas been exacerbated by the global covid-19 pandemic. the highly integrated food supply chains that spread around the world are susceptible to disruptions due to policy changes, economic stresses, and natural disasters, as highlighted by the recent pandemic. in this perspective article, written by members of the editorial committee of the annual review of food science and technology, we highlight some of the major challenges confronting the modern food supply chain as well as how innovations in policy and technology can be used to address them. pertinent technological innovations include robotics, machine learning, artificial intelligence, advanced diagnostics, nanotechnology, biotechnology, gene editing, vertical farming, and soft matter physics. many of these technologies are already being employed across the food chain by farmers, distributors, manufacturers, and consumers to improve the quality, nutrition, safety, and sustainability of the food supply. these innovations are required to stimulate the development and implementation of new technologies to ensure a more equitable, resilient, and efficient food production system. where appropriate, these technologies should be carefully tested before widespread implementation so that proper risk-benefit analyses can be carried out. they can then be employed without causing unforeseen adverse consequences. finally, it is important to actively engage all stakeholders involved in the food supply chain throughout the development and testing of these new technologies to support their adoption if proven safe and effective.']"
21,agriculture,5,81,21_yield_crop_crop yield_yield prediction,"['yield', 'crop', 'crop yield', 'yield prediction', 'lstm', 'deep', 'prediction', 'agricultural', 'irrigation', 'deep learning', 'rice', 'neural', 'wheat', 'yields', 'network', 'learning', 'agriculture', 'neural network', 'production', 'long short', 'short term', 'term memory', 'soil', 'food', 'memory', 'weather', 'short', 'forecasting', 'time', 'long']","['multi-parametric multiple kernel deep neural network for crop yield prediction a rapid, precise and credible prediction of crop yield at a wider scale is more important than ever for crop management, the measurement of food production, food trading and policymaking to address the challenges of environmental change, increased population and food demand. deep learning (dl) models are recently well-known for predicting crop yields. multi-parametric deep neural network (mdnn) is a dl model employed to estimate the crop yield concerning multiple parameters such as climate and soil. a growing-degree day (gdd) has been used to determine the impact of climate change on crop yield. the determined values along with the climate and soil parameters have been learned by the dnn to estimate the yield quality. the mdnn performs well with the huge volume of data and it is not suitable for the medium scale of data. the learning ability of mdnn is improved in this paper by proposing a multi-parametric multiple kernel dnn (mmkdnn) to provide better crop yield prediction for medium-scale data. the effectiveness of the model built in the last hidden layer is solely determined by the intermediate representations of the input. the intermediate representation of the input in a neural network is combined through multiple kernel learning. the mmkdnn with increasing complexity representations is preserved in this way, but the output calculation, i.e. crop yield prediction, is free to use all of the networks knowledge. for five different types of crops, the experiments are conducted to assess the efficiency of the mmkdnn.', 'crop yield prediction using deep reinforcement learning model for sustainable agrarian applications predicting crop yield based on the environmental, soil, water and crop parameters has been a potential research topic. deep-learning-based models are broadly used to extract significant crop features for prediction. though these methods could resolve the yield prediction problem there exist the following inadequacies: unable to create a direct non-linear or linear mapping between the raw data and crop yield values; and the performance of those models highly relies on the quality of the extracted features. deep reinforcement learning provides direction and motivation for the aforementioned shortcomings. combining the intelligence of reinforcement learning and deep learning, deep reinforcement learning builds a complete crop yield prediction framework that can map the raw data to the crop prediction values. the proposed work constructs a deep recurrent q-network model which is a recurrent neural network deep learning algorithm over the q-learning reinforcement learning algorithm to forecast the crop yield. the sequentially stacked layers of recurrent neural network is fed by the data parameters. the q-learning network constructs a crop yield prediction environment based on the input parameters. a linear layer maps the recurrent neural network output values to the q-values. the reinforcement learning agent incorporates a combination of parametric features with the threshold that assist in predicting crop yield. finally, the agent receives an aggregate score for the actions performed by minimizing the error and maximizing the forecast accuracy. the proposed model efficiently predicts the crop yield outperforming existing models by preserving the original data distribution with an accuracy of 93.7%.', 'a new framework for winter wheat yield prediction integrating deep learning and bayesian optimization early prediction of winter wheat yield at the regional scale is essential for food policy making and food security, especially in the context of population growth and climate change. agricultural big data and artificial intelligence (ai) are key technologies for smart agriculture, bringing cost-effective solutions to the agricultural sector. deep learning-based crop yield forecast has currently emerged as one of the key methods for guiding agricultural production. in this study, we proposed a bayesian optimization-based long- and short-term memory model (bo-lstm) to construct a multi-source data fusion-driven crop growth feature extraction algorithm for winter wheat yield prediction. the yield prediction performance of bo-lstm, support vector machine (svm), and least absolute shrinkage and selection operator (lasso) was then compared with multi-source data as input variables. the results showed that effective deep learning hyperparameter optimization is made possible by bayesian optimization. the bo-lstm (rmse = 177.84 kg/ha, r2 = 0.82) model had the highest accuracy of yield prediction with the input combination of “gpp + climate + lai + vis”. bo-lstm and svm (rmse = 185.7 kg/ha, r2 = 0.80) methods outperformed linear regression lasso (rmse = 214.5 kg/ha, r2 = 0.76) for winter wheat yield estimation. there were also differences between machine learning and deep learning, bo-lstm outperformed svm. indicating that the bo-lstm model was more effective at capturing data correlations. in order to further verify the robustness of the bo-lstm method, we explored the performance estimation performance of bo-lstm in different regions. the results demonstrated that the bo-lstm model could obtain higher estimation accuracy in regions with concentrated distribution of winter wheat cultivation and less influence of human factors. the approach used in this study can be expected to forecast crop yields, both in regions with a deficit of data and globally; it can also simply and effectively forecast winter wheat yields in a timely way utilizing publicly available multi-source data.']"
24,agriculture,5,76,24_disease_diseases_plant_leaf,"['disease', 'diseases', 'plant', 'leaf', 'disease detection', 'leaves', 'detection', 'plant diseases', 'plants', 'images', 'tomato', 'classification', 'image', 'identification', 'skin', 'crop', 'deep', 'diagnosis', 'deep learning', 'early', 'farmers', 'crops', 'cancer', 'image processing', 'learning', 'agriculture', 'using', 'cnn', 'pathogens', 'dataset']","['deep learning-based segmentation for disease identification plant diseases have recently increased and exacerbated due to several factors such as climate change, chemicals’ misuse and pollution. they represent a severe threat for both economy and global food security. recently, several researches have been proposed for plant disease identification through modern image-based recognition systems based on deep learning. however, several challenges still require further investigation. one is related to the high variety of leaf diseases/ species along with constraints related to the collection and annotation of real-world datasets. other challenges are related to the study of leaf disease in uncontrolled environment. compared to major existing researches, we propose in this article a new perspective to handle the problem with two main differences: first, while most approach aims to identify simultaneously a pair of species-disease, we propose to identify diseases independently of leaf species. this helps to recognize new species holding diseases that were previously learnt. moreover, instead of using the global leaf image, we directly predict disease on the basis of the local disease symptom features. we believe that this may decrease the bias related to common context and/or background and enables to build a more generalised model for disease classification. in particular, we propose an hybrid system that combines strengths of deep learning-based semantic segmentation with classification capabilities to respectively extract infected regions and determine their identity. for that, an extensive experimentation including a comparison of different semantic segmentation and classification cnns has been conducted on plantvillage dataset (leaves within homogeneous background) in order to study the extent of use of local disease symptoms features to identify diseases. specifically, a particular enhancement of disease identification accuracy has been demonstrated in ipm and bing datasets (leaves within uncontrolled background).', 'corn leaf disease detection with pertinent feature selection model using machine learning technique with efficient spot tagging model crop diseases constitute a substantial threat to food safety but, due to the lack of a critical basis, their rapid identification in many parts of the world is challenging. the development of accurate techniques in the field of image categorization based on leaves produced excellent results. plant phenotyping for plant growth monitoring is an important aspect of plant characterization. early detection of leaf diseases is crucial for efficient crop output in agriculture. pests and diseases cause crop harm or destruction of a section of the plant, leading to lower food productivity. in addition, in a number of less-developed countries, awareness of pesticide management and control, as well as diseases, is limited. some of the main reasons for decreasing food production are toxic diseases, poor disease control and extreme climate changes. the quality of farm crops may be influenced by bacterial spot, late blight, septoria and curved yellow leaf diseases. because of automatic leaf disease classification systems, action is easy after leaf disease signs are detected. applying image processing and machine learning methodologies, this research offers an efficient spot tagging leaf disease detection with pertinent feature selection model using machine learning technique (spldpfs-mlt). different diseases deplete chlorophyll in leaves generating dark patches on the surface of the leaf. machine learning algorithms can be used to identify image pre-processing, image segmentation, feature extraction and classification. compared with traditional models, the proposed model shows that the model performance is better than those existing.', 'reviewing important aspects of plant leaf disease detection and classification agriculture, being the dominant industry from the point of view of economical growth of countries like india, plays a vital role in fulfilling the demand of food. however, extreme weather conditions and several climate changes may invite notable infectious diseases in plants caused by fungi, viruses and bacteria. these plant diseases can be a major threat to food supply and hence it is important to identify and prevent the plants from the diseases at the early stages. the conventional approaches were dependent on the experts in the field and hence time consuming. since the technology is upgrading day by day and has plenty of its advantages in plant leaf disease detection field as well, various disease identification approaches using different domains have been proposed in the literature to detect and cure the plant diseases that occur on the plant leaves. although, many of the existing approaches have provided better results, challenges exist in order to achieve optimized results of plant leaf disease detection process. this paper reviews different methodologies under image processing, machine learning, deep learning and swarm intelligence domains for plant leaf disease detection. understanding of various diseases that occurs on plant leaves is very important in order to deal with it; hence this paper provides a detailed taxonomy about the different plant diseases and dataset that is popularly used in various existing approaches for training and testing purpose of plant leaf disease detection and its classifications.']"
30,agriculture,5,50,30_weed_detection_weeds_fruit,"['weed', 'detection', 'weeds', 'fruit', 'insect', 'pest', 'traps', 'pests', 'precision', 'mobile', 'identification', 'deep', 'insects', 'crop', 'image', 'deep learning', 'agriculture', 'vision', 'counting', 'production', 'images', 'recognition', 'environments', 'yolo', 'computer vision', 'field', 'fields', 'computer', 'yolov5', 'proposed']","['performance evaluation of deep transfer learning on multi-class identification of common weed species in cotton production systems precision weed management offers a promising solution for sustainable cropping systems through the use of chemical-reduced/non-chemical robotic weeding techniques, which apply suitable control tactics to individual weeds or small clusters. therefore, accurate identification of weed species plays a crucial role in such systems to enable precise, individualized weed treatment. despite recent progress, the development of a robust weed identification and localization system in the presence of unstructured field environments remains a serious challenge, requiring supervised modeling using large volumes of annotated data. this paper makes a first comprehensive evaluation of deep transfer learning (dtl) for identifying common weed species specific to cotton (gossypium hirsutum l.) production systems in southern united states (u.s.). a new dataset for weed identification was created, consisting of 5187 color images of 15 weed classes collected under natural light conditions and at varied weed growth stages, in cotton fields (primarily in mississippi and north carolina) during the 2020 and 2021 growth seasons. we evaluated 35 state-of-the-art deep learning models through transfer learning with repeated holdout validations and established an extensive benchmark for the considered weed identification task. dtl achieved high classification accuracy of f1 scores exceeding 95%, requiring reasonably short training time (less than 2.5 h) across models. resnext101 achieved the best overall f1-score of 98.93 ± 0.34%, whereas 10 out of the 35 models achieved f1 scores near or above 98.0%. however, the performance on minority weed classes with few training samples was less satisfactory for models trained with a conventional, unweighted cross entropy loss function. to address this issue, a weighted cross entropy loss function was adopted, which achieved substantially improved accuracies for minority weed classes (e.g., the f1-scores for xception and mnasnet on the spurred anoda weed increased from 48% to 90% and 50% to 82%, respectively). furthermore, a deep learning-based cosine similarity metric was employed to analyze the similarity among weed classes, assisting in the interpretation of classifications. both the codes (https://github.com/derekabc/cottonweeds) for model benchmarking and the weed dataset (https://www.kaggle.com/yuzhenlu/cottonweedid15) of this study are made publicly available, which expect to be a valuable resource for future research on weed identification and beyond.', 'yoloweeds: a novel benchmark of yolo object detectors for multi-class weed detection in cotton production systems weeds are among the major threats to cotton production. overreliance on herbicides for weed control has accelerated the evolution of herbicide-resistance in weeds and caused increasing concerns about environments, food safety and human health. machine vision systems for automated/robotic weeding have received growing interest towards the realization of integrated, sustainable weed management. however, in the presence of unstructured field environments and significant biological variability of weeds, it remains a serious challenge to develop reliable weed identification and detection systems. a promising solution to address this challenge are the development of arge-scale, annotated image datasets of weeds specific to cropping systems and data-driven ai (artificial intelligence) models for weed detection. among various deep learning architectures, a diversity of yolo (you only look once) detectors is well-suited for real-time application and has enjoyed great popularity for generic object detection. this study presents a new dataset (cottoweeddet12) of weeds important to cotton production in the southern united states (u.s.); it consists of 5648 images of 12 weed classes with a total of 9370 bounding box annotations, collected under natural light conditions and at varied weed growth stages in cotton fields. a novel, comprehensive benchmark of 25 state-of-the-art yolo object detectors of seven versions including yolov3, yolov4, scaled-yolov4, yolor and yolov5, yolov6 and yolov7, has been established for weed detection on the dataset. evaluated through the monte-caro cross validation with 5 replications, the detection accuracy in terms of map@0.5 ranged from 88.14 % by yolov3-tiny to 95.22 % by yolov4, and the accuracy in terms of map@[0.5:0.95] ranged from 68.18 % by yolov3-tiny to 89.72 % by scaled-yolov4. all the yolo models especially yolov5n and yolov5s have shown great potential for real-time weed detection, and data augmentation could increase weed detection accuracy. both the weed detection dataset and software program codes for model benchmarking in this study are publicly available, which will be to be valuable resources for promoting future research on big data and ai-empowered weed detection and control for cotton and potentially other crops.', 'deepcottonweeds (dcw): a novel benchmark of yolo object detectors for weed detection in cotton production systems weeds are among the major threats to cotton production. overreliance on herbicides for weed control has accelerated the evolution of herbicide-resistance in weeds and brought increasing concerns about environments, food safety and human health. machine vision systems for automated or robotic weeding have received significant interest in integrated, sustainable weed management. however, in the presence of unstructured field conditions and significant biological variability of weeds, it still remains a challenging task to develop robust weed identification and detection systems. two critical obstacles to addressing this challenge include the lack of dedicated, large-scale image datasets of weeds specific to crop (cotton) production and the development of machine learning models for weed detection. this study presents a new dataset of weeds important to the u.s. cotton production systems, consisting of 5648 images of 12 weed classes with a total of 9370 bounding box annotation, collected under natural light conditions and at varied weed growth stages in cotton fields. furthermore, a comprehensive benchmark of 18 selected state-of-the-art yolo object detectors, involving yolov3, yolov4, scaled-yolov, and yolov5, was established for weed detection on the dataset. the detection accuracy in terms of map@50 ranged from 88.14% by yolov3-tiny to 95.22% by yolov4, and the accuracy in terms of map@50[0.5:0.95] ranged from 68.18% by yolov3-tiny to 89.72 by scaled-yolov4. all the yolo models especially yolov5n and yolov5s showed great potential for real-time weed detection, and data augmentation could increase weed detection accuracy. both the codes and weed dataset for model benchmarking are publicly available (https://github.com/derekabc/dcw), which are expected to be valuable resources for future research in the field of weed detection and beyond.']"
31,agriculture,5,45,31_crop_robot_agriculture_unmanned,"['crop', 'robot', 'agriculture', 'unmanned', 'farming', 'agricultural', 'intelligent', 'farmers', 'crops', 'farms', 'pest', 'plant', 'production', 'pests', 'farm', 'equipment', 'robots', 'greenhouse', 'control', 'intelligence', 'artificial intelligence', 'diseases', 'greenhouses', 'artificial', 'sensing monitoring', 'conditions', 'pesticides', 'vision', 'weed', 'field']","['an intelligent agriculture application based on deep learning ever since the internet of things was developed, countries around the world have been applying the technology to solve current issues in agriculture, such as combating climate change and reducing labor costs. in the moment, intelligent agriculture has focused mainly on greenhouse cultivation; however, it takes significant amount of material and labor resources to build greenhouses, and their products are mostly high-valued cash crops. this study employs an outdoor farm as the field of experimentation and uses self-made model vehicles that patrols regularly to obtain information on the farms crops. the study first establishes deep learning in intelligent agriculture to understand the farmers farming skills so that such information may in turn help in further establishing an internet of things system that realizes automated farming systems. this study utilizes small, self-made model vehicles to gather crop information every day. the vehicles designed functions include the following: (1) automated patrol along preprogrammed routes, (2) automatic photographing of each area, (3) prevention of collision between the small, self-made model vehicles, and (4) data collection of each areas temperature and humidity conditions. in this study, deep learning is employed mainly towards training and prediction of conditions based on the crops temperature and humidity data as well as weather information. deep learning is established first, followed by the internet of things, in order to achieve intelligent agriculture. the study must first acquire the farmers cultivation skills; since such knowledge is passed on through experience and without any recordings, it can only be retrieved by means of the small model vehicles. in addition, the study also conducted simulation through hands-on experiments. the proposed scheme first collects all environment-related factors and conducts training on related coefficients, then utilizes the automatic irrigation system to allow the same kind of crops to grow in the most ideal environment. the system focuses on outdoor farms; it establishes an intelligent agriculture using minimal costs, resolving issues such as number of sensors needed, farm network accessibility, and electrical wiring.', 'intelligent greenhouse system based on remote sensing images and machine learning promotes the efficiency of agricultural economic growth with the rapid development of vegetable planting industry, a large amount of vegetable cultivation has helped many farmers get rid of poverty, and china has become the worlds largest vegetable growing country. in the process of planting vegetables, farmers may be threatened by diseases and insect pests. the occurrence of various diseases and insect pests is often irregular. farmers who grow vegetables mostly use their own experience to prevent and control diseases and insect pests, but experience alone cannot accurately judge the disease and insect pests. types, which lead to insufficient timely and effective control of diseases and insect pests, affect the growth and development of vegetables and even affect the yield and quality, thereby reducing farmers’ income. with the advancement of science, higher requirements have been put forward to construct greenhouses to provide a more scientific and efficient environment for the growth of vegetables. however, our country has not paid enough attention to the development of agricultural resources, which will not only affect our natural environment but also restrict our rural economic development. therefore, in order to realize the sustainable development of my countrys agricultural resources and economy, it is necessary to study the coordinated development of rural environmental factors and the economy. this paper designs an intelligent greenhouse system based on remote sensing images and machine learning. through this system we designed, we can realize remote monitoring of vegetables. then we can find the threat of pests to vegetable growth for the first time, and reduce the impact of pests on vegetable production and yield. the loss caused by quality, this system has an immeasurable effect on promoting the economic growth of our countrys agriculture.', 'lettuce production in intelligent greenhouses—3d imaging and computer vision for plant spacing decisions recent studies indicate that food demand will increase by 35–56% over the period 2010–2050 due to population increase, economic development, and urbanization. greenhouse systems allow for the sustainable intensification of food production with demonstrated high crop production per cultivation area. breakthroughs in resource-efficient fresh food production merging horticultural and ai expertise take place with the international competition “autonomous greenhouse challenge”. this paper describes and analyzes the results of the third edition of this competition. the competition’s goal is the realization of the highest net profit in fully autonomous lettuce production. two cultivation cycles were conducted in six high-tech greenhouse compartments with operational greenhouse decision-making realized at a distance and individually by algorithms of international participating teams. algorithms were developed based on time series sensor data of the greenhouse climate and crop images. high crop yield and quality, short growing cycles, and low use of resources such as energy for heating, electricity for artificial light, and co2 were decisive in realizing the competition’s goal. the results highlight the importance of plant spacing and the moment of harvest decisions in promoting high crop growth rates while optimizing greenhouse occupation and resource use. in this paper, images taken with depth cameras (realsense) for each greenhouse were used by computer vision algorithms (deepabv3+ implemented in detectron2 v0.6) in deciding optimum plant spacing and the moment of harvest. the resulting plant height and coverage could be accurately estimated with an r2 of 0.976, and a miou of 98.2, respectively. these two traits were used to develop a light loss and harvest indicator to support remote decision-making. the light loss indicator could be used as a decision tool for timely spacing. several traits were combined for the harvest indicator, ultimately resulting in a fresh weight estimation with a mean absolute error of 22 g. the proposed non-invasively estimated indicators presented in this article are promising traits to be used towards full autonomation of a dynamic commercial lettuce growing environment. computer vision algorithms act as a catalyst in remote and non-invasive sensing of crop parameters, decisive for automated, objective, standardized, and data-driven decision making. however, spectral indexes describing lettuces growth and larger datasets than the currently accessible are crucial to address existing shortcomings between academic and industrial production systems that have been encountered in this work.']"
32,agriculture,5,133,32_agriculture_farming_iot_agricultural,"['agriculture', 'farming', 'iot', 'agricultural', 'smart', 'technologies', 'farmers', 'internet', 'food', 'internet things', 'things', 'precision', 'smart farming', 'precision agriculture', 'smart agriculture', 'production', 'irrigation', 'digital', 'technology', 'crop', 'things iot', 'sensors', 'farm', 'intelligence', 'artificial intelligence', 'systems', 'ai', 'iot based', 'greenhouse', 'big']","['smart and sustainable agriculture: fundamentals, enabling technologies, and future directions agriculture is an important sector that plays a key role in the economic growth of countries. innovative agricultural advancements has undoubtedly supported the expansion of the capacity and efficiency of different agricultural activities. according to the united nations food and agriculture organization assessment, the worlds population is expected to reach 8.5 billion by 2030, and 9.6 billion by 2050 which will result in an unprecedented demand for food and agriculture products. it is estimated that the food production should increase by 70 percent to meet such a demand. given the limited farming space, lack of water, climate change, and constantly changing environmental conditions, new and innovative smart agriculture solutions must be developed. in general, there has been an increase in the agricultural production volume over the time to ensure the food security. efforts are being made to increase the quality and quantity of agribusiness products by transforming them into smart and connected products through a smart agriculture industry. smart and precision agriculture refers to the integration of technology, such as the internet of things, sensors, robotics, artificial intelligence, smart supply chains, big data analytics, and blockchain, into the agriculture industry. the internet of things era is the umbrella that covers and enables the other technological tools. smart technology integration can lead to a more productive and efficient agriculture via reducing the need for manual human interactions and making proactive intelligent decisions. this article aims at providing a survey on smart and sustainable agriculture focusing on its enabling technologies while providing future directions for improvement.', 'survey for smart farming technologies: challenges and issues internet of things (iot) has been a major influence in agriculture since its application to the sector. this paper provides an extensive review of the use of smart technologies in agriculture and elaborates the state-of-the-art technologies for smart agriculture including, internet of things, cloud computing, machine learning, and artificial intelligence. the application of smart farming to crop and animal production and post-harvesting is discussed. the impact of climate change on agriculture is also considered. this paper contributes to knowledge by iterating the challenges of smart technology to agriculture while highlighting the issues identified from existing framework of smart agriculture. the authors identify many gaps in existing research affecting the application of iot in smart farming, and suggest further research to improve the current food production globally, to provide better food management and sustainability measures across the globe.', 'smart agriculture and smart farming using iot technology it has become easier to access agriculture data in recent years as a result of a decline in digital breaches between agricultural producers and iot technologies. these future technologies can be used to boost productivity by cultivating food more sustainably while also preserving the environment, thanks to improved water use and input and treatment optimization. the internet of things (iot) enables the production of agricultural process-supporting systems. referred to as remote monitoring systems, decision support tools, automated irrigation systems, frost protection systems, and fertilisation systems, respectively. farmers and researchers must be provided with a detailed understanding of iot applications in agriculture as a result of the knowledge described above. this study is about using internet of things (iot) technologies and techniques to enhance agriculture. this article is meant to serve as an introduction to iot-based applications in agriculture by identifying need for such tools and explaining how they support agriculture.']"
36,agriculture,5,72,36_irrigation_yield_dssat_crop,"['irrigation', 'yield', 'dssat', 'crop', 'rice', 'yields', 'cotton', 'wheat', 'water', 'climate', 'decision support', 'maize', 'production', 'crops', 'irrigated', 'agrotechnology', 'rainfed', 'winter wheat', 'simulated', 'climate change', 'change', 'transfer', 'agricultural', 'winter', 'decision', 'increase', 'irrigation water', 'future', 'soil', 'support']","['irrigation and shifting planting date as climate change adaptation strategies for sorghum climate change is projected to have a global impact that affect food production and security. the objectives of this study were to determine the potential impact of climate change on sorghum yield for rainfed production systems and to evaluate the potential of irrigation and shifting planting dates as adaptation options for two major sorghum production regions in ethiopia. the decision support system for agrotechnology transfer (dssat) cropping system model (csm)-ceres-sorghum model was used to simulate the impact of climate change on sorghum yield for two representative concentration pathways (rcps; rcp 4.5 and rcp 8.5) and for three future periods including the 2025s (2010–2039), 2055s (2040–2069), and 2085s (2070–2099). the agricultural model improvement and inter-comparison project (agmip) framework was used to select five representative gcms for hot/dry, cool/dry, middle, hot/wet, and cool/wet climate scenarios. two climate change adaptation practices including supplemental irrigation at two levels (deficit and full) to the current rainfed production system and shifting planting dates were evaluated. the csm-ceres-sorghum model was calibrated and evaluated using eight years of experimental data from meisso, eastern ethiopia. the model was then run for kobo and meisso under different climate change and crop management scenarios. based on model evaluation results, the model performed well for simulating sorghum yield (r2 = 0.99), anthesis (r2 = 0.86, rmse = 1.3), and maturity (r2 = 0.79, rmse = 4.4). the results showed that the average temperature for kobo and meisso is expected to increase by up to 6 °c under rcp8.5 in 2085. for the rainfed production systems without adaptation practices, drought stress is projected to intensify during anthesis, which was reflected by projected yield reductions by up 2 t ha−1 for the two sites. full irrigation was effective in reducing moisture stress and, thereby, increasing sorghum yield by up to 3 t ha−1 for kobo and 2 t ha−1 for meisso. on average, full irrigation resulted in a 1 t ha−1 yield increase compared with deficit irrigation. early planting dates also resulted in an increase in yield compared to the baseline planting dates, especially when combined with supplemental irrigation, although late planting was consistently disadvantageous even with supplemental irrigation. this study highlighted that the csm-ceres-sorghum model can be effectively used to simulate climate change effects on sorghum yield and evaluate different climate change adaptation practices. the outcomes of this study can also help to implement management decisions towards climate change adaptation for the current subsistence and fragile rainfed crop production system in ethiopia and similar ecoregions across the globe.', 'modeling deficit irrigation-based evapotranspiration optimizes wheat yield and water productivity in arid regions climate change and water scarcity have put food security and sustainable development in arid regions at risk. irrigation based actual evapotranspiration (etc) has recently been added as a new tool in the decision support system for agrotechnology transfer (dssat) models and might improve irrigation water management, thus more research is needed. for this purpose, three wheat models (ceres, cropsim and n-wheat) in the latest version of dssat (v. 4.7.5) were calibrated and evaluated using experimental field data across three growing seasons. field data included irrigation by different fractions of etc as 80%, 100% and 120%. the calibrated models were then employed to predict wheat grain yield (gy), biomass yield (by), irrigation, evapotranspiration, water use efficiency-based evapotranspiration (wue_et), and water use efficiency-based irrigation (wue_irri) for 10 locations represent nile delta in long term simulation (1991–2020). the models showed robust simulations of etc compared to observed values under all corresponding treatments, demonstrating high calibration accuracy and the ability to predict yield and water for other locations in the long term. simulation treatments included automatic irrigation with different fractions of 50%, 60%, 70%, 80%, 90% and 100% from etc. hereinafter, the simulated gy and wue_et were compared with those obtained by farmers in all locations to specify the recommended treatment achieving higher yield and water productivity. in all locations, simulated gy and by ranged (4000–9000 kg ha-1), and (10,500–18,000 kg ha-1), respectively with associated uncertainty between treatments and locations. averaged over ten locations, and 30 years, the simulated gy under full irrigation treatment (100% etc), showed the superiority with an increase of 27.5%, 13.0%, 5.0%, 1.5%, and 0.4% relative to irrigation with 50%, 60%, 70%, 80%, and 90% etc, respectively. deficit irrigation-based et decreased wue_irri, whilst increased wue_et, achieving the higher value (20.0 kg ha-1 mm-1) with irrigation based 90% etc. however, deficit irrigation with 90% etc (i5) produced higher wue values than full irrigation (100% etc), with increases of 0.08% and 10.6% for wue_et and wue_irri, respectively. comparing simulated gy and wue_et with farmers values in all locations, simulated values under irrigation based 90% etc increased by 1.7% and 63%, respectively, confirming the importance of irrigation scheduling based 90% etc in maximizing wheat yield and water productivity in arid regions.', 'determining optimum irrigation termination periods for cotton production in the texas high plains cotton (gossypium hirsutum l.) production in the texas high plains (thp) region relies heavily on irrigation with groundwater from the underlying ogallala aquifer. however, rapidly declining groundwater levels in the aquifer and increasing pumping costs pose challenges for sustainability of irrigated cotton production in this region. adoption of efficient irrigation strategies, such as terminating irrigation at an appropriate time in the growing season, could enable producers to increase irrigation water use efficiency (iwue) while maintaining desired yield goals. the objective of this study was to determine optimum irrigation termination periods for cotton production in the thp under full and deficit irrigation conditions using the decision support system for agrotechnology transfer (dssat) cropgro-cotton model, which was evaluated in a prior study in the thp using measured data from an iwue field experiment at halfway, texas. the treatment factors in the field experiment included irrigation capacities of 0 mm d-1 (low, l), 3.2 mm d-1 (medium, m), and 6.4 mm d-1 (high, h), applied during the vegetative, reproductive, and maturation growth stages. this study focused on a full irrigation (hhh) treatment and three deficit irrigation (lmh, lhm, and lmm) treatments. eight irrigation termination dates with a one-week interval between 15 august and 30 september were simulated, and the impact of irrigation termination date on cotton iwue and seed cotton yield were studied by dividing the 39-year (1978 to 2016) simulation period into dry, normal, and wet years based on the precipitation received from 1 april to the simulated irrigation termination date. results indicated that the simulated iwue was consistently higher under the lhm, lmh, and lmm treatments when compared to the hhh treatment. based on the simulated average seed cotton yield and iwue, optimum irrigation termination periods for cotton were found to be the first week of september (about 118 days after planting, dap) for the hhh and lmh treatments and the second week of september (125 dap) for the lhm and lmm treatments in normal years. in wet years, optimum irrigation termination periods were a week earlier than those in normal years and a week later in dry years for the hhh, lhm, and lmm treatments. for the lmh treatment, the optimum irrigation termination period in wet years was the same as that in normal years and two weeks later in dry years. the results from this study along with field-specific, late-season information will assist thp cotton producers in making appropriate irrigation termination decisions for improving economic productivity of the ogallala aquifer and thereby ensuring water security for agriculture. however, the recommendations from this study should be used with caution, as the optimum irrigation termination periods could potentially change with changes in cultivar characteristics, soil type, climate, and, crop management practices.']"
42,agriculture,5,75,42_crop_yield_crops_farmers,"['crop', 'yield', 'crops', 'farmers', 'crop yield', 'agricultural', 'agriculture', 'yield prediction', 'crop yields', 'yields', 'prediction', 'production', 'machine', 'soil', 'machine learning', 'crop production', 'farming', 'algorithm', 'planting', 'india', 'food', 'learning', 'using', 'rice', 'parameters', 'cultivation', 'weather', 'prediction crop', 'wheat', 'predict crop']","['analysis of machine learning technique for crop selection and prediction of crop cultivation agriculture is a fundamental and basic source of living of many people and it is considered as a main part in economy and finance of any country. it is highly dependent on climate, weather and environment factors. efficiency in crop productivity depends on agricultural elements embracing soil, water, temperature, and climate changes. the challenges in agriculture field, specifically crop prediction are complex and a machine-learning approach for crop cultivation is proposed for the achievement of production goals. this paper is proposed to design a machine learning-based system for crop selection. it can analyze the quality of soil and water, agro-climatic conditions, and also the necessary crop requirements for selecting suitable crops using the database. also, supervising crop growth is proposed for monitoring the proper growth of the crop. a machine learning technique to develop crop prediction and soil quality check models. this model works on machine learning techniques like support vector machine (svm) algorithm. the classifiers like random forest (rf), gaussian naive bayes (nb), and k nearest neighbor (knn) are used in crop prediction process. based on machine learning algorithm and prediction accuracy, this model can help farmers at different locations for crop cultivation management.', 'prediction of crops based on a machine learning algorithm agriculture is crucial to any countrys economy. farmers around the world face a constant challenge in trying to keep up with the rising demand for food crops in the face of fluctuating climates and an alarming rise in population. one of the most widely grown cereals, wheat supplies a significant portion of the worlds main food supply. this heat-sensitive crop is being severely harmed by the unusual rise in environmental temperature and decrease in the amount of rainfall. scientists from all around the world have been looking at what are called climate sustainable agriculture practices in an effort to boost wheat crop yields while reducing the impact on the environment. predicting crop yields before harvest can assist scientists and farmers evaluate risks and implement preventative actions to maintain a consistent agricultural harvest. there are two main types of models used to predict harvest yields: crop growth models and data-driven models. the time, money, and accuracy costs associated with using crop growth models to predict crop yields stem from the fact that these methods are sensitive to environmental variables. so, the farmer cant do anything in the nick of time to boost his crops production. with the advent of machine learning algorithms, data-driven models have become even more effective at a fraction of the cost of traditional empirical models. machine learning and machine learning have come a long way, but they havent been completely used for precise crop output forecasting. with this study, the authors want to provide a reliable method for estimating future wheat harvests in one of indias punjab provinces. for precise and timely wheat crop yield prediction, an knn with dt hybrid machine learning model is proposed. to further improve the models performance, the researchers used a genetic algorithm to tune the knn-two dts most crucial hyper parameters: the size of its window and the number of neurons in its hidden layer. this study has also examined the impact of environmental factors on agricultural yield in order to isolate the most important environmental parameters for future regulation and monitoring in order to more accurately predict crop yield. the proposed model for predicting wheat crop yield was tested with a battery of trials. the effectiveness of the suggested model was validated through a comparative comparison with state-of-the-art approaches for yield prediction. farmers, policymakers, and planners can all benefit greatly from the proposed study by improving their ability to make informed decisions and take corrective and preventative action to boost agricultural yields.', 'crop yield prediction using random forest algorithm most agricultural crops have been badly affected by the effect of global climate change in india. in terms of their output over the past 20 years. it will allow policy makers and farmers to take effective marketing and storage steps to predict crop yields earlier in their harvest. this project will allow farmers to capture the yield of their crops before cultivation in the field of agriculture and thus help them make the necessary decisions. implementation of such a method with a web-based graphic software that is simple to use and the machine learning algorithm can then be distributed. the results obtained are granted access to the farmer. and yet there are various methods or protocols for such very data analytics in crop yield prediction, and we are able to predict agricultural productivity with guidance of all those algorithms. it utilizes a random forest algorithm. by researching such problems and issues such as weather, temperature, humidity, rainfall, humidity, there are no adequate solutions and inventions to resolve the situation we face. in countries like india, even in the agricultural sector, as there are many types of increasing economic growth. in addition, the processing is useful for forecasting the production of crop yields.']"
49,agriculture,5,44,49_yield_maize_wheat_crop,"['yield', 'maize', 'wheat', 'crop', 'yields', 'prediction', 'regression', 'yield prediction', 'harvest', 'wheat yield', 'production', 'weather', 'machine', 'ha', 'temperature', 'soil', 'linear', 'machine learning', 'rf', 'grain', 'ann mlp', 'food', 'phenological', 'climate', 'linear regression', 'random', 'maize yield', 'random forest', 'meteorological', 'used']","['improving winter wheat yield forecasting based on multi-source data and machine learning to meet the challenges of climate change, population growth, and an increasing food demand, an accurate, timely and dynamic yield estimation of regional and global crop yield is critical to food trade and policy-making. in this study, a machine learning method (random forest, rf) was used to estimate winter wheat yield in china from 2014 to 2018 by integrating satellite data, climate data, and geographic information. the results show that the yield estimation accuracy of rf is higher than that of the multiple linear regression method. the yield estimation accuracy can be significantly improved by using climate data and geographic information. according to the model results, the estimation accuracy of winter wheat yield increases dramatically and then flattens out over months; it approached the maximum in march, with r2 and rmse reaching 0.87 and 488.59 kg/ha, respectively; this model can achieve a better yield forecasting at a large scale two months in advance.', 'the prediction of wheat yield in the north china plain by coupling crop model with machine learning algorithms the accuracy prediction for the crop yield is conducive to the food security in regions and/or nations. to some extent, the prediction model for crop yields combining the crop mechanism model with statistical regression model (srm) can improve the timeliness and robustness of the final yield prediction. in this study, the accumulated biomass (ab) simulated by the agricultural production systems simulator (apsim) model and multiple climate indices (e.g., climate suitability indices and extreme climate indices) were incorporated into srm to predict the wheat yield in the north china plain (ncp). the results showed that the prediction model based on the random forest (rf) algorithm outperformed the prediction models using other regression algorithms. the prediction for the wheat yield at sm (the period from the start of grain filling to the milky stage) based on rf can obtain a higher accuracy (r = 0.86, rmse = 683 kg ha−1 and mae = 498 kg ha−1). with the progression of wheat growth, the performances of yield prediction models improved gradually. the prediction of yield at fs (the period from flowering to the start of grain filling) can achieve higher precision and a longer lead time, which can be viewed as the optimum period providing the decent performance of the yield prediction and about one month’s lead time. in addition, the precision of the predicted yield for the irrigated sites was higher than that for the rainfed sites. the apsim-simulated ab had an importance of above 30% for the last three prediction events, including fif event (the period from floral initiation to flowering), fs event (the period from flowering to the start of grain filling) and sm event (the period from the start of grain filling to the milky stage), which ranked first in the prediction model. the climate suitability indices, with a higher rank for every prediction event, played an important role in the prediction model. the winter wheat yield in the ncp was seriously affected by the low temperature events before flowering, the high temperature events after flowering and water stress. we hope that the prediction model can be used to develop adaptation strategies to mitigate the negative effects of climate change on crop productivity and provide the data support for food security.', 'integrating satellite and climate data to predict wheat yield in australia using machine learning approaches wheat is the most important staple crop grown in australia, and australia is one of the top wheat exporting countries globally. timely and reliable wheat yield prediction in australia is important for regional and global food security. prior studies use either climate data, or satellite data, or a combination of these two to build empirical models to predict crop yield. however, though the performance of yield prediction using empirical methods is improved by combining the use of climate and satellite data, the contributions from different data sources are still not clear. in addition, how the regression-based methods compare with various machine-learning based methods in their performance in yield prediction is also not well understood and needs in-depth investigation. this work integrated various sources of data to predict wheat yield across australia from 2000 to 2014 at the statistical division (sd)level. we adopted a well-known regression method (lasso, as a benchmark)and three mainstream machine learning methods (support vector machine, random forest, and neural network)to build various empirical models for yield prediction. for satellite data, we used the enhanced vegetation index (evi)from modis and solar-induced chlorophyll fluorescence (sif)from gome-2 and sciamachy as metrics to approximate crop productivity. the machine-learning based methods outperform the regression method in modeling crop yield. our results confirm that combining climate and satellite data can achieve high performance of yield prediction at the sd level (r2 ˜ 0.75). the satellite data track crop growth condition and gradually capture the variability of yield evolving with the growing season, and their contributions to yield prediction usually saturate at the peak of the growing season. climate data provide extra and unique information beyond what the satellite data have offered for yield prediction, and our empirical modeling work shows the added values of climate variables exist across the whole season, not only at some certain stages. we also find that using evi as an input can achieve better performance in yield prediction than sif, primarily due to the large noise in the satellite-based sif data (i.e. coarse resolution in both space and time). in addition, we also explored the potential for timely wheat yield prediction in australia, and we can achieve the optimal prediction performance with approximately two-month lead time before wheat maturity. the proposed methodology in this paper can be extended to different crops and different regions for crop yield prediction.']"
50,agriculture,5,94,50_yield_crop_yields_maize,"['yield', 'crop', 'yields', 'maize', 'production', 'climate', 'agricultural', 'rice', 'soybean', 'impacts', 'wheat', 'productivity', 'crops', 'weather', 'adaptation', 'change', 'climate change', 'corn', 'crop yield', 'future', 'variability', 'food', 'ca', 'cultivars', 'grain', 'sorghum', 'process based', 'traits', 'factors', 'suitability']","['the response of maize, sorghum, and soybean yield to growing-phase climate revealed with machine learning accurate representation of crop responses to climate is critically important to understand impacts of climate change and variability in food systems. we use random forest (rf), a diagnostic machine learning tool, to explore the dependence of yield on climate and technology for maize, sorghum and soybean in the us plains. we analyze the period from 1980 to 2016 and use a panel of county yields and climate variables for the crop-specific developmental phases: establishment, critical window (yield potential definition) and grain filling. the rf models accounted for between 71% to 86% of the yield variance. technology, evaluated through the time variable, accounted for approximately 20% of the yield variance and indicates that yields have steadily increased. responses to climate confirm prior findings revealing threshold-like responses to high temperature (yield decrease sharply when maximum temperature exceed 29 c and 30 c for maize and soybean), and reveal a higher temperature tolerance for sorghum, whose yield decreases gradually as maximum temperature exceeds 32.5 c. we found that sorghum and soybean responded positively to increases in cool minimum temperatures. maize yield exhibited a unique and negative response to low atmospheric humidity during the critical phase that encompasses flowering, as well as a strong sensitivity to extreme temperature exposure. using maize as a benchmark, we estimate that if warming continues unabated through the first half of the 21st century, the best climatic conditions for rainfed maize and soybean production may shift from iowa and illinois to minnesota and the dakotas with possible modulation by soil productivity.', 'predicting spatial and temporal variability in crop yields: an inter-comparison of machine learning, regression and process-based models pervious assessments of crop yield response to climate change are mainly aided with either process-based models or statistical models, with a focus on predicting the changes in average yields, whilst there is growing interest in yield variability and extremes. in this study, we simulate us maize yield using process-based models, traditional regression model and a machine-learning algorithm, and importantly, identify the weakness and strength of each method in simulating the average, variability and extremes of maize yield across the country. we show that both regression and machine learning models can well reproduce the observed pattern of yield averages, while large bias is found for process-based crop models even fed with harmonized parameters. as for the probability distribution of yields, machine learning shows the best skill, followed by regression model and process-based models. for the country as a whole, machine learning can explain 93% of observed yield variability, followed by regression model (51%) and process-based models (42%). based on the improved capability of the machine learning algorithm, we estimate that us maize yield is projected to decrease by 13.5% under the 2 c global warming scenario (by ∼2050 s). yields less than or equal to the 10th percentile in the yield distribution for the baseline period are predicted to occur in 19% and 25% of years in 1.5 c (by ∼2040 s) and 2 c global warming scenarios, with potentially significant implications for food supply, prices and trade. the machine learning and regression methods are computationally much more efficient than process-based models, making it feasible to do probabilistic risk analysis of climate impacts on crop production for a wide range of future scenarios.', 'observational constraint of process crop models suggests higher risks for global maize yield under climate change projecting future changes in crop yield usually relies on process-based crop models, but the associated uncertainties (i.e. the range between models) are often high. in this study, a machine learning (i.e. random forest, rf) based observational constraining approach is proposed for reducing the uncertainties of future maize yield projections by seven process-based crop models. based on the observationally constrained crop models, future changes in yield average and yield variability for the period 2080-2099 are investigated for the globe and top ten producing countries. results show that the uncertainties of crop models for projecting future changes in yield average and yield variability can be largely reduced by 62% and 52% by the rf-based constraint, respectively, while only 4% and 16% of uncertainty reduction is achieved by traditional linear regression-based constraint. compared to the raw simulations of future change in yield average (-5.13 ± 18.19%) and yield variability (-0.24 ± 1.47%), the constrained crop models project a much higher yield loss (-34.58 ± 6.93%) and an increase in yield variability (3.15 ± 0.71%) for the globe. regionally, the constrained models show the largest increase in yield loss magnitude in brazil, india and indonesia. our results suggest more agricultural risks under climate change than previously expected after observationally constraining crop models. the results obtained in this study point to the importance for observationally constraining process crop models for robust yield projections, and highlight the added value of using machine learning for reducing the associated uncertainties.']"
9,infrastructure,6,13,9_pavement_maintenance_deterioration_road,"['pavement', 'maintenance', 'deterioration', 'road', 'roads', 'warming impact', 'distress', 'lca', 'uncertainty', 'life cycle', 'ontario', 'life', 'attributes', 'asset', 'impact', 'cycle', 'gwp', 'infrastructure', 'traffic', 'global warming', 'management strategies', 'maintaining', 'condition index', 'warming', 'damage', 'condition', 'asset management', 'highway', 'agencies', 'management']","['climate change impact on infrastructure: a machine learning solution for predicting pavement condition index a decision-support tool was developed to predict the condition of asphalt roads in 2, 3, 5 and 6 years. the tool was developed based on analyzing a large dataset (more than 3000 road sections) extracted from the long-term pavement performance (ltpp) database. several algorithms were examined: two decision trees, k-nearest neighbors (k-nn), naïve bayes classifier, naïve bayes coupled with kernel estimator, random forest and gradient boosted trees. the last three achieved the highest accuracy levels (above 90%). the attributes used were intentionally selected to be related to climate stressors (such as temperature ranges, perspiration and freeze–thaw cycles) or basic road attributes (such as age and functional class) to enable the models quantify the impact of climate change. a major caveat of this study is that some climate stressors such as storm frequency and severity were not included in the model as there was no data available about them in the ltpp dataset. with the proposed tool, the impacts of different climate scenarios can be examined by running the model with inputs that reflect the attributes of each scenario. to illustrate this, we examined the deterioration of two sets of roads: one from ontario and one from texas. each set was examined in two climate scenarios. the analysis showed lower levels of deterioration for the ontario roads and exacerbation of deterioration for the roads in texas. it means that climate change may exacerbate or alleviate road deterioration depending on location. this type of analysis can be beneficial to the long-term policymaking in road infrastructure. for example, notwithstanding the impact of climate attributes that are not considered in this study, an ontario policymaker should expect that with the same design standards and the same maintenance regimes, the service levels of roads will be enhanced.', 'minimizing the global warming impact of pavement infrastructure through reinforcement learning life cycle assessment (lca) studies are frequently used to evaluate the environmental burdens of pavement facilities. this information can be used by decision-makers to advise their construction and maintenance policies. within the pavement life cycle, there are a variety of uncertainties, such as future traffic growth and pavement deterioration. currently, there is a lack of research examining the use of lca models that can simultaneously optimize construction and maintenance plans while accounting for several sources of uncertainty. this study presents an approach to lca modeling that implements a sub-type of reinforcement learning (rl) algorithms called q-learning. q-learning offers a model-free approach that can efficiently manage stochastic problems of parametric and non-parametric form. the algorithm iteratively learns a set of near-optimal decision rules to proactively manage pavement assets for a diverse range of possible future scenarios. these decision-rules are stored in a convenient look-up table, which will appeal to practitioners for its ease of use in probabilistic lca studies. this paper subsequently tests the performance of the q-learning approach across three representative case studies with varying traffic volumes: a local street-highway, a state highway, and an interstate. the case study results show that, on average, the proposed algorithm reduces the expected global warming impact of pavement infrastructure between 13% and 18% over a 50-year analysis period. based on our results, q-learning is a promising approach that can help decision-makers account for several sources of uncertainty and implement improved management strategies to mitigate the environmental impacts of their products and systems.', 'a machine-learning solution for quantifying the impact of climate change on roads modeling pavement performance is a must for road asset management. in the age of climate change, pavement performance models need to be able to quantify the impact of climate change on roads. this paper provides a practical decision-support tool for predicting the condition of asphalt roads in the short and long term under a changing climate. users have the option of running a predictive model under different values of climate stressors. the prediction of deterioration is performed via machine learning. more than a thousand examples of road sections from the long-term pavement performance (ltpp) database were used in the process of model training. the models can predict future values of pavement condition index (pci) with an accuracy above 80%. the results were implemented in a web-based platform, which includes a map with an interactive dashboard. users can query any road, input its data, and get relevant predictions about its deterioration in two, three, five and six years. to show the effectiveness of the solution two sets of examples were presented: two individual roads in ontario and british columbia and a group of 44 roads in ontario. the condition of the latter was predicted under a hypothetical climate change scenario. the results suggested that the roads in ontario will experience a more relaxed deterioration under this climate change scenario.']"
15,infrastructure,6,31,15_port_development_ecological_audit,"['port', 'development', 'ecological', 'audit', 'sustainable development', 'environment', 'industrial', 'innovation', 'tourism', 'intelligent', 'government', 'mining', 'evaluation', 'environmental', 'sustainable', 'impact factors', 'paper', 'economic', 'sd', 'regional', 'clusters', 'coal', 'capability', 'eco', 'organization', 'swarm', 'information', 'carrying capacity', 'development green', 'bp neural']","['application of comprehensive evaluation system for government ecological audit using computational intelligence algorithms government ecological audit has a major influence on our sustainable economic and social development. the comprehensive evaluation of government ecological audit becomes increasingly important as the deterioration of ecological environment. thus, it is great significance to set up scientific evaluation system of government ecological audit. firstly, a present condition of government ecological audit is discussed, and a computational process is used to make the evaluation system more integrated. secondly, a practical evaluation system of government ecological audit is presented to establish the model in comprehensive evaluating process. then, an evaluation example of government ecological audit is proposed by using computational intelligence algorithms of analytical hierarchy process (ahp). finally, some policy recommendations are made to strengthen the establishment of evaluation system for government ecological construction performance audit in this paper.', 'evolution of port ecological carrying capacity based on sd model construction of ecological port has become an important direction of port development. research on evaluation and prediction of evolution of port ecological carrying capacity trend can provide available and reliable method for ecological port construction. a system dynamics (sd) model for port ecological carrying capacity considering characteristics of port ecosystem is built in this paper, based on causal feedback analysis with aspects of socio-economic, resources and environment. the proposed model could simulate the process of port ecosystem evolution and evaluate the dynamic condition of port ecological carrying capacity with the supply-demand balance index of the ecological carrying capacity. in the model, gdp growth rate, investment proportion of environmental protection, comprehensive energy consumption per unit throughput, water consumption per unit of throughput, cod generation per unit of throughput, so2 generation per unit of throughput, solid waste generation per unit of throughput are selected as control variables. the proposed model is utilized to forecast the trend of evolution of port ecological carrying capacity from 2015 to 2030 in a case study of a port area of dalian port with regard to three port development scenarios, respectively natural development scenario, coordinated development scenario and environmental protection scenario. the results show that energy consumption is an important bottleneck restricting the improvement of the port ecological carrying capacity, in this sense that optimization of energy structure harbor and strengthening energy management contribute to improve port ecological carrying capacity, and coordinated development scenario is the optimal way to achieve the sustainable development of port ecosystem. this paper can provide a theoretical basis for the construction of eco-port.', 'evaluation of regional industrial cluster innovation capability based on particle swarm clustering algorithm and multi-objective optimization with the progress of the times and the development of science, industrial clusters have been regarded by all countries in the world as one of the important ways to enhance regional competitiveness, and become an inevitable trend of industrial development. the research on the innovation ability of industrial clusters can not only maintain sustainable development of industrial clusters and obtain sustained competitive advantages, but also provide reference for the governments policy formulation of industrial clusters. this paper aims to study the evaluation of regional industrial clusters innovation capability based on particle swarm clustering and multi-objective optimization. this paper uses the theory of industrial cluster innovation and takes regional industrial system as the empirical research object to establish a regional industrial system capability evaluation system, which is based on the selection of indicators, combined with analytic hierarchy process and factor analysis to evaluate industrial innovation capability. on this basis, the particle swarm clustering theory is used to verify the innovation ability and evaluation index system of industrial clusters, and provide a reference for the evaluation of the innovation ability of industrial clusters. this paper divides the regional cluster innovation capability into four aspects: innovation input capability, environment support capability, self-development capability and innovation output capability, and systematically analyzes the key elements and in the composition of innovation elements and their relationships. it then constructs the evaluation index system of regional cluster innovation capability. at the same time, this paper introduces clustering analysis algorithm and swarm intelligence algorithm into regional innovation evaluation, combines particle swarm optimization algorithm and k-means clustering algorithm, and optimizes particle swarm clustering algorithm by adjusting adaptive parameters and adding fitness variance. the experimental results of this paper show that from the results of the tested innovation potential of the three industrial clusters, industrial cluster f has the strongest innovation ability, with an evaluation coefficient of 0.851, followed by industrial cluster f, which has a value of 0.623. this result is consistent with the actual innovation status of the selected industry. from this point of view, the established particle swarm clustering model for evaluating the innovation capability of regional industrial clusters is reliable and can be used to evaluate the innovation capability of different industrial clusters.']"
62,infrastructure,6,45,62_cloud_centers_computing_center,"['cloud', 'centers', 'computing', 'center', 'energy consumption', 'energy', 'consumption', 'resource', 'cloud computing', 'scheduling', 'service', 'power', 'server', 'servers', 'applications', 'tasks', 'resources', 'cooling', 'efficient', 'cost', 'virtual', 'energy efficient', 'aware', 'utilization', 'total energy', 'virtualization', 'solution', 'migration', 'propose', 'green']","['hunter: ai based holistic resource management for sustainable cloud computing the worldwide adoption of cloud data centers (cdcs) has given rise to the ubiquitous demand for hosting application services on the cloud. further, contemporary data-intensive industries have seen a sharp upsurge in the resource requirements of modern applications. this has led to the provisioning of an increased number of cloud servers, giving rise to higher energy consumption and, consequently, sustainability concerns. traditional heuristics and reinforcement learning based algorithms for energy-efficient cloud resource management address the scalability and adaptability related challenges to a limited extent. existing work often fails to capture dependencies across thermal characteristics of hosts, resource consumption of tasks and the corresponding scheduling decisions. this leads to poor scalability and an increase in the compute resource requirements, particularly in environments with non-stationary resource demands. to address these limitations, we propose an artificial intelligence (ai) based holistic resource management technique for sustainable cloud computing called hunter. the proposed model formulates the goal of optimizing energy efficiency in data centers as a multi-objective scheduling problem, considering three important models: energy, thermal and cooling. hunter utilizes a gated graph convolution network as a surrogate model for approximating the quality of service (qos) for a system state and generating optimal scheduling decisions. experiments on simulated and physical cloud environments using the cloudsim toolkit and the cosco framework show that hunter outperforms state-of-the-art baselines in terms of energy consumption, sla violation, scheduling time, cost and temperature by up to 12, 35, 43, 54 and 3 percent respectively.', 'cloud-iot resource management based on artificial intelligence for energy reduction the rapid growth in demand for cloud services has led to the creation of large-scale data centers, which allows application service providers to lease data center capacity for application deployment as per user requirement in terms of quality of services (qos). these data centers consume a lot of electrical power which contributes to increased operating costs and carbon dioxide emissions. in addition, modern cloud computing environments must provide qos for their customers which leads them to a need to make a power-performance compromise that is to say in terms of energy consumption and service-level agreement (sla) compliance. that is why, we introduce, in this paper, an intelligent resource management policy for cloud data centers. the goal is to dynamically allocate and continuously consolidate virtual machines taking advantage of live migration and disengage inactive nodes to minimize power feeding in this cloud environment while maintaining the quality of service. we integrate some artificial intelligence concepts to ensure a dynamic resource management and a better power-performance compromise and significantly reduce the consumed energy.', 'optimizing energy consumption for cloud computing: a cluster and migration based approach (cmba) the increased use of it technologies and number of it users have triggered cloud computing resource demand including the need for more data centers. each data center consumes electricity for its un-interrupted operations and maintenance, therefore responsible for the emissions of carbon dioxide, a potent greenhouse gas causing climate change. hence, there is a necessity to provide a solution through which energy consumption for cloud data centers can be reduced. as virtual machine located in data center are run under loaded to maintain higher performance but it causes wastage of resources and power. while, task overloading severally reduce the performance of data center. to address this issue, we propose cmba (cluster and migration based approach) for cloud resource allocation that maps groups of tasks to customized virtual machine types based on processing, memory and network requirements. proper placement of workload with specific vms and dynamic migration concept reduce energy consumption for running physical machine and its respective host or data centers. taking altogether, intelligent customization of virtual machines by adopting cmba approach will maintain high efficiency of datacenters with reduced energy consumption.']"
68,infrastructure,6,65,68_iot_energy_communication_wireless,"['iot', 'energy', 'communication', 'wireless', 'networks', 'harvesting', 'devices', 'energy harvesting', 'network', 'routing', 'spectrum', '6g', 'nodes', 'iot devices', 'power', 'reinforcement learning', 'reinforcement', 'throughput', 'proposed', 'allocation', 'mobile', 'internet', 'environment', '5g', 'edge', 'learning', 'things', 'traffic', 'internet things', 'deep reinforcement']","['sac: a novel multi-hop routing policy in hybrid distributed iot system based on multi-agent reinforcement learning energy harvesting (eh) iot devices have attracted vast attention in both academia and industry as they can work sustainably by harvesting energy from the ambient environment. however, due to the weak and transient nature of harvesting power, eh technology is unable to support power-intensive iot devices such as iot edge servers. therefore, the hybrid iot system where the eh iot devices and non-eh iot devices co-exist is forthcoming. this paper explored the routing problem in such a hybrid distributed iot system. we first proposed a comprehensive multi-hop routing mechanism of this hybrid system. after that, we proposed a distributed multi-agent deep reinforcement learning algorithm, known as spatial asynchronous advantage actor-critic (sac), to optimize the system routing policy and energy allocation while maximizing the total amount of transmitted data and the overall data delivery to the sink node. the experiments indicate that sac can averagely complete at least $\\sim 1.5 \\times$ transmission rate and $\\sim 12.9\\times$ sink packet delivery rate compared with the baselines.', 'learning-based resource management for low-power and lossy iot networks internet of things (iot) networks are key to the realization of modern industries and societies. a key application of iot is in smart-grid communications. smart-grid networks are resource constrained in terms of computing power and energy capacity. similarly, the wireless links between devices are typically associated with high packet-loss rates, low throughput, and instability. to provide a sustainable communication mechanism, an iot network stack is proposed for these devices. however, each network stack layer has its own constraints. for example, to facilitate the operation of these low-power and lossy network (lln) devices, the international engineering task force (ietf) standardized a network-layer protocol called a routing protocol for low-power and lossy networks (rpls). rpl often creates an inefficient network in densely deployed and varying traffic load conditions. future dense iot-based networks are expected to automatically optimize the reliability and efficiency of communication by inferring the diverse features of both the environments and actions of the devices. machine learning (ml) provides a promising framework for such a dense network environment. in this study, we examine the underlying perspective of ml for such systems. we utilize the multiarmed bandit (mab)-based expected energy count (beex) technique, which provides nodes the ability to effectively optimize their operation. using the proposed mechanism, nodes can intelligently adapt their network-layer behavior. the performance of the proposed (beex) algorithm is evaluated through a contiki 3.0 cooja simulation. the proposed method improves the energy consumption and packet delivery ratio and produces a lower control overhead than other state-of-the-art mechanisms.', 'joint data transmission and energy harvesting for miso downlink transmission coordination in wireless iot networks the advent of simultaneous wireless information and power (swipt) has been regarded as a promising technique to provide power supplies for an energy sustainable internet of things (iot), which is of paramount importance due to the proliferation of high data communication demands of low-power network devices. in such networks, a multi-antenna base station (bs) in each cell can be utilized to concurrently transmit messages and energies to its intended iot user equipment (iot-ue) with a single antenna under a common broadcast frequency band, resulting in a multi-cell multi-input single-output (miso) interference channel (ic). in this work, we aim to find the trade-off between the spectrum efficiency (se) and energy harvesting (eh) in swipt-enabled networks with miso ics. for this, we derive a multi-objective optimization (moo) formulation to obtain the optimal beamforming pattern (bp) and power splitting ratio (pr), and we propose a fractional programming (fp) model to find the solution. to tackle the nonconvexity of fp, an evolutionary algorithm (ea)-aided quadratic transform technique is proposed, which recasts the nonconvex problem as a sequence of convex problems to be solved iteratively. to further reduce the communication overhead and computational complexity, a distributed multi-agent learning-based approach is proposed that requires only partial observations of the channel state information (csi). in this approach, each bs is equipped with a double deep q network (ddqn) to determine the bp and pr for its ue with lower computational complexity based on the observations through a limited information exchange process. finally, with the simulation experiments, we verify the trade-off between se and eh, and we demonstrate that, apart from the fp algorithm introduced to provide superior solutions, the proposed ddqn algorithm also shows its performance gain in terms of utility to be up to 1.23-, 1.87-, and 3.45-times larger than the advantage actor critic (a2c), greedy, and random algorithms, respectively, in comparison in the simulated environment.']"
73,infrastructure,6,45,73_attacks_attack_detection_security,"['attacks', 'attack', 'detection', 'security', 'intrusion', 'cyber', 'cloud', 'network', 'iot', 'computing', 'systems', 'smart', 'environment', 'cloud computing', 'internet', 'scada', 'proposed', 'threats', 'learning', 'based', 'iiot', 'anomaly', 'dynamic', 'devices', 'deep', 'based cloud', 'networks', 'sustainability', 'physical systems', 'classification']","['economic denial of sustainability (edos) detection using gans in sdn-based cloud cloud computing is now considered to be the most cost-effective platform for offering business and consumer it services over the internet. however, it is prone to new vulnera-bilities. specifically, a newly discovered type of attack, called an economic-denial-of-sustainability attack known as edos, exploits the pay-per-use model to scale up the resource usage over time to the degree that the cloud user has to pay for the unexpected usage charge. to prevent edos attacks, we propose an effective solution in the sdn-based cloud computing environment. we first introduce a machine-learning-based approach adopting a framework called mad-gan which applies an unsupervised multivariate anomaly detection technique based on genera-tive adversarial networks (gans), using the long-short-term-memory recurrent neural networks (lstm-rnn) to detect edos attacks. its main idea is to produce an anomaly score at each time step by learning a multivariate attribute. we then generate a dynamic threshold score to compare with the anomaly score produced to classify the network traffic as edos traffic or normal traffic. by realistic tests, our proposed scheme is demonstrated to outperform existing methods for edos attack detection. the detailed experiments conducted with different levels of edos attacks show that the proposed scheme is an efficient, innovative approach to defend edos attacks in the sdn-based cloud.', 'dynamic economic-denial-of-sustainability (edos) detection in sdn-based cloud in cloud computing, a new type of attack, called economic denial of sustainability (edos) attack, exploits the pay-per-use model to scale up the resource usage over time to the extent that the cloud user has to pay for the unexpected usage charge. to prevent edos attacks, we propose an efficient solution in the sdn-based cloud computing environment. in this paper, we first apply an unsupervised learning approach called long short-term memory (lstm), which is a multivariate time series anomaly detection, to detect edos attacks. its key idea is to try to predict values of the resource usage of a cloud consumer (cpu load, memory usage and etc). furthermore, unlike other existing proposals using a predefined threshold to classify the anomalies which generate high rate errors, in this work, we utilize a dynamic error threshold which delivers much better performance. through practical experiments, the proposed edos attack defender is proven to outperform existing mechanisms for edos attack detection. furthermore, it also outperforms some of the machine-learning-based methods, which we conducted the experiment ourselves. the comprehensive experiments conducted with various edos attack levels prove that the proposed mechanism is an effective, innovative approach to defense edos attacks in the sdn-based cloud.', 'artificial intelligence algorithm-based economic denial of sustainability attack detection systems: cloud computing environments cloud computing is currently the most cost-effective means of providing commercial and consumer it services online. however, it is prone to new flaws. an economic denial of sustainability attack (edos) specifically leverages the pay-per-use paradigm in building up resource demands over time, culminating in unanticipated usage charges to the cloud customer. we present an effective approach to mitigating edos attacks in cloud computing. to mitigate such distributed attacks, methods for detecting them on different cloud computing smart grids have been suggested. these include hard-threshold, machine, and deep learning, support vector machine (svm), k-nearest neighbors (knn), random forest (rf) tree algorithms, namely convolutional neural network (cnn), and long short-term memory (lstm). these algorithms have greater accuracies and lower false alarm rates and are essential for improving the cloud computing service provider security system. the dataset of nine injection attacks for testing machine and deep learning algorithms was obtained from the cyber range lab at the university of new south wales (unsw), canberra. the experiments were conducted in two categories: binary classification, which included normal and attack datasets, and multi-classification, which included nine classes of attack data. the results of the proposed algorithms showed that the rf approach achieved accuracy of 98% with binary classification, whereas the svm model achieved accuracy of 97.54% with multi-classification. moreover, statistical analyses, such as mean square error (mse), pearson correlation coefficient (r), and the root mean square error (rmse), were applied in evaluating the prediction errors between the input data and the prediction values from different machine and deep learning algorithms. the rf tree algorithm achieved a very low prediction level (mse = 0.01465) and a correlation r2 (r squared) level of 92.02% with the binary classification dataset, whereas the algorithm attained an r2 level of 89.35% with a multi-classification dataset. the findings of the proposed system were compared with different existing edos attack detection systems. the proposed attack mitigation algorithms, which were developed based on artificial intelligence, outperformed the few existing systems. the goal of this research is to enable the detection and effective mitigation of edos attacks.']"
74,infrastructure,6,40,74_blockchain_secure_security_smart,"['blockchain', 'secure', 'security', 'smart', 'privacy', 'iot', 'fl', 'communication', 'access', 'things', 'security privacy', 'federated', 'attacks', 'internet', 'smart city', 'sg', 'blockchain based', 'federated learning', 'edge', 'internet things', 'smart cities', 'computing', 'consensus', 'proposed', 'environments', 'smart environments', 'distributed', 'trust', 'control', 'devices']","['a secured blockchain method for multivariate industrial iot-oriented infrastructure based on deep residual squeeze and excitation network with single candidate optimizer iot infrastructures have been growing in recent years across a variety of industrial applications in sustainable smart cities and communities, including smart manufacturing and smart industries. infrastructure geared towards internet of things also includes cyber-physical system (cps). with a distributed environment, cps has experienced significant success in industrial applications and critical infrastructure. however, there are numerous difficulties in such an environment, including those related to scalability, centralization, communication latency, and security and privacy. deep residual squeeze and excitation network (drsen) with single candidate optimizer-based iot-oriented infrastructure is used in this manuscript to address these challenges. blockchain creates a distributed environment for cpss communication phase, and software-defined networking (sdn) establishes protocols for data forwarding in network. at the application layer of suggested infrastructure, a deep learning-based cloud is used to address communication latency, centralization, and scalability issues. it makes high-performance, affordable computing resources available for smart city applications like smart industrial and smart transportation. the proposed method is implemented in ns-3 tool and the effectiveness is assessed using f-measure, reliability, scalability, accuracy, sensitivity, specificity, and precision. the proposed method provides 35.39%, 23.87% and 20.67% higher reliability, 42.39%, 11.39%, 34.16% and 25.78% higher accuracy, 15.02%, 26.64% and 37.55% higher f-measure compared with existing techniques like industrial internet of things based on blockchain secure device authentication solution (iiot-bc-basa), industrial internet of things based on blockchain- interplanetary file system (iiot-bc-ipfs), industrial internet of things based on blockchain- shamir threshold cryptography (iiot-bc-shc), industrial internet of things based on blockchain- software define networking (iiot-bc-sdn) methods, respectively.', 'a deep learning-based iot-oriented infrastructure for secure smart city in recent years, the internet of things (iot) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. the cyber-physical system (cps) is also part of iot-oriented infrastructure. cps has gained considerable success in industrial applications and critical infrastructure with a distributed environment. this system aims to integrate the physical world to computational facilities as cyberspace. however, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. to mitigate these challenges, we propose a deep learning-based iot-oriented infrastructure for a secure smart city where blockchain provides a distributed environment at the communication phase of cps, and software-defined networking (sdn) establishes the protocols for data forwarding in the network. a deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. it enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. finally, we evaluated the performance of our proposed infrastructure. we compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. the evaluation of our implementation results shows that performance is improved.', 'blockchain-enabled privacy preserving of iot data for sustainable smart cities using machine learning the development of sensor technologies and an explosion of the inexpensive electronic circuit, the internet of things (iot) is emergent as an encouraging innovation to comprehend sustainable smart city. smart cities can bid various intelligent applications like smart transportation, smart banking, and industry 4.0, among others, to boost citizens life quality. however, security is one of the critical problems of a smart city. these emerging smart infrastructure and applications based on iot can benefit users only if vital private and secure features are guaranteed. hence, in this paper, blockchain-enabled privacy-preserving access control system (bpacs) has been suggested for iot data in a smart city environment. this study utilizes blockchain methods to construct a reliable and secure data-sharing policy between numerous data providers, where iot information is encoded and then verified on disseminated ledgers. furthermore, this study design protected construction blocks, like secure comparison and secure polynomial multiplications, by retaining cryptosystems and build a secure support vector machine (svm) and principle component analysis (pca) training algorithms. hard security analysis proves that the suggested model guarantees the privacy of the sensitive information for every data provider and the svm and pca model variables for data analysts.']"
79,infrastructure,6,56,79_healthcare_wearable_health_cancer,"['healthcare', 'wearable', 'health', 'cancer', 'sensors', 'medical', 'devices', 'care', 'workers', 'iot', 'home', 'work', 'lung', 'services', 'monitoring', 'older', 'patient', 'ageing', 'recognition', 'cognitive', 'patients', 'personalized', 'technology', 'stress', 'elderly', 'body', 'activity', 'human', 'computing', 'smart']","['a governmental programme to support well-being, ageing and care at home with new technologies purpose the better utilization of technology as the most promising solution for the sustainability challenge created by the ageing population has been stated (european environment agency 2020, technology advisory board 2021). however, the challenges exist and they are related not only to the technologies utilized but to their impacts and integration into the user contexts, such as the services in which they are used. the programme, presented here introduces the services that needs to be reformed with technology for people living at home within home care. method to introduce a governmental programme called ‘smart ageing and care at home’ (‘kati’ for the finnish acronym) which advance and support the implementation of new technologies and digital services for well-being, ageing and care of older people at home in finland. the programme itself consist of six regional projects across seven regions in finland including municipalities or associations of municipalities, actors from both the third and private sectors, and universities as partners. the programme is coordinated by the finnish institute for health and welfare (thl), and the regional projects participate in systematic assessment realized in multi-perspective way. to assess the technologies and their impacts and integration into the user contexts, a shared guideline and platform was built for the regional projects to support them and to follow their own regional plans in systematic way. the platform provide a concise way to guide the regional projects to evaluate the impacts and to describe the adoption of technology solutions and operation models that the regional projects intergrate into their services. results and discussion at the moment while the governmental programme is halfway, the regional projects have carried out pilots and deployments of various technology devices, services and systems (e.g. monitoring technologies, solutions supporting social activity, and technologies for care professionals) together with independent older adults, homecare customers, family members and care professionals. they have also implemented technology to collect health-related data to be used in care services to support independent and safe living at home, and developed registries for devices and applications. moreover, they have educated and trained homecare professionals for technology use, developed new care work roles, procurement processes and support services, and produced digital health technology assessments (haverinen et al., 2019). altogether, there are around one hundred technology solutions and operation models (e.g. video care and communication solutions, medicine dispension and monitoring devices, artificial intelligence services predicting changes in end-customers health and wellbeing, showrooms for seniors, 3d virtual training environments, and online shops to buy technologies for public services) that will be analysed to form a finnish model of technology-supported ageing and care of older people at home. more information about the shared guideline and platform, which is made to support the regional projects in systematic way will be discussed at the conference. it includes objectives, information about technology solutions and operation models (e.g. information about their mechanisms and contexts to be used), evaluation methods, results, and ethics. preliminary results of the adoption of technology solutions and operation models that the regional projects integrate into their services will also be presented. at the moment, 93 % out of the technology solutions and operation models that are planned to carry out are in progress.', 'reliable and resilient ai and iot-based personalised healthcare services: a survey recent technological (e.g., iot, 5g), and economic (e.g., un 2030 sustainable development goals) developments have transformed the healthcare sector towards more personalized and iot-based healthcare services. these services are realized through control and monitoring applications that are typically developed using artificial intelligence (ai)/machine learning (ml) based algorithms, that play a significant role to highlight the efficiency of traditional healthcare systems. current personalized healthcare services are dedicated in a specific environment to support technological personalization (e.g., personalized gadgets/devices). however, they are unable to consider different inter-related health conditions, leading to inappropriate diagnosis and affect sustainability and the long-term health/life of patients. towards this problem, the state-of-the-art healthcare 5.0 technology has evolved that supersede previous healthcare technologies. the goal of healthcare 5.0 is to achieve a fully autonomous healthcare service, that takes into account the interdependent effect of different health conditions of a patient. this paper conducts a comprehensive survey on personalized healthcare services. in particular, we first present an overview of key requirements of comprehensive personalized healthcare services (cphs) in modern healthcare internet of things (hiot), including the definition of personalization and an example use case scenario as a representative for modern hiot. second, we explored a fundamental three-layer architecture for iot-based healthcare systems using both ai and non-ai-based approaches, considering key requirements for cphs followed by their strengths and weaknesses in the frame of personalized healthcare services. third, we highlighted different security threats against each layer of iot architecture along with the possible ai and non-ai-based solutions. finally, we propose a methodology to develop reliable, resilient, and personalized healthcare services that address the identified weaknesses of existing approaches.', 'ai‐enabled framework for fog computing driven e‐healthcare applications artificial intelligence (ai) is the revolutionary paradigm to empower sixth generation (6g) edge computing based e‐healthcare for everyone. thus, this research aims to promote an ai‐based cost‐effective and efficient healthcare application. the cyber physical system (cps) is a key player in the internet world where humans and their personal devices such as cell phones, laptops, wearables, etc., facilitate the healthcare environment. the data extracting, examining and monitoring strategies from sensors and actuators in the entire medical landscape are facilitated by cloud‐enabled technologies for absorbing and accepting the entire emerging wave of revolution. the efficient and accurate examination of voluminous data from the sensor devices poses restrictions in terms of bandwidth, delay and energy. due to the heterogeneous nature of the internet of medical things (iomt), the driven healthcare system must be smart, interoperable, convergent, and reliable to provide pervasive and cost‐effective healthcare platforms. unfortunately, because of higher power consumption and lesser packet delivery rate, achieving interoperable, convergent, and reliable transmission is challenging in connected healthcare. in such a scenario, this paper has fourfold major contributions. the first contribution is the development of a single chip wearable electrocardiogram (ecg) with the support of an analog front end (afe) chip model (i.e., ads1292r) for gathering the ecg data to examine the health status of elderly or chronic patients with the iot‐based cyber physical system (cps). the second proposes a fuzzy‐based sustainable, interoperable, and reliable algorithm (fsira), which is an intelligent and self‐adaptive decision‐making approach to prioritize emergency and critical patients in association with the selected parameters for improving healthcare quality at reasonable costs. the third is the proposal of a specific cloud‐based architecture for mobile and connected healthcare. the fourth is the identification of the right balance between reliability, packet loss ratio, convergence, latency, interoperability, and throughput to support an adaptive iomt driven connected healthcare. it is examined and observed that our proposed approaches outperform the conventional techniques by providing high reliability, high convergence, interoperability, and a better foundation to analyze and interpret the accuracy in systems from a medical health aspect. as for the iomt, an enabled healthcare cloud is the key ingredient on which to focus, as it also faces the big hurdle of less bandwidth, more delay and energy drain. thus, we propose the mathematical trade‐offs between bandwidth, interoperability, reliability, delay, and energy dissipation for iomt‐oriented smart healthcare over a 6g platform.']"
80,infrastructure,6,38,80_reinforcement learning_reinforcement_agent_traffic,"['reinforcement learning', 'reinforcement', 'agent', 'traffic', 'reward', 'rl', 'deep reinforcement', 'multi agent', 'signal', 'robot', 'control', 'agents', 'learning', 'autonomous', 'deep', 'environment', 'problem', 'navigation', 'proposed', 'simulator', 'multi', 'algorithm', 'based', 'robots', 'learning rl', 'drl', 'simulation', 'controllers', 'game', 'transportation']","['toward conflict resolution with deep multi-agent reinforcement learning safety in air traffic management at the tactical level is ensured by human controllers. automatic detection and resolution tools are one way to assist controllers in their tasks. however, the majority of existing methods do not account for factors that can affect the quality and efficiency of resolutions. furthermore, future challenges such as sustainability and the environmental impact of aviation must be tackled. in this work, we propose an innovative approach to pairwise conflict resolution, by modeling it as a multi-agent reinforcement learning to improve the quality of resolutions based on a combination of several factors. we use multi-agent deep deterministic policy gradient to generate resolution maneuvers. we propose a reward function that besides solving the conflicts attempts to optimize the resolutions in terms of time, fuel consumption, and airspace complexity. the models are evaluated on real traffic, with a data augmentation technique utilized to increase the variance of conflict geometries. we achieve promising results with a resolution rate of 93%, without the agents having any previous knowledge of the dynamics of the environment. furthermore, the agents seem to be able to learn some desirable behaviors such as preferring small heading changes to solve conflicts in one time step. nevertheless, the nonstationarity of the environment makes the learning procedure nontrivial. we argue ways that tangible qualities such as resolution rate and intangible qualities such as resolution acceptability and explainability can be improved.', 'multi-agent reinforcement learning for traffic signal control: a cooperative approach the rapid growth of urbanization and the constant demand for mobility have put a great strain on transportation systems in cities. one of the major challenges in these areas is traffic congestion, particularly at signalized intersections. this problem not only leads to longer travel times for commuters, but also results in a significant increase in local and global emissions. the fixed cycle of traffic lights at these intersections is one of the primary reasons for this issue. to address these challenges, applying reinforcement learning to coordinating traffic light controllers has become a highly researched topic in the field of transportation engineering. this paper focuses on the traffic signal control problem, proposing a solution using a multi-agent deep q-learning algorithm. this study introduces a novel rewarding concept in the multi-agent environment, as the reward schemes have yet to evolve in the following years with the advancement of techniques. the goal of this study is to manage traffic networks in a more efficient manner, taking into account both sustainability and classic measures. the results of this study indicate that the proposed approach can bring about significant improvements in transportation systems. for instance, the proposed approach can reduce fuel consumption by 11% and average travel time by 13%. the results of this study demonstrate the potential of reinforcement learning in improving the coordination of traffic light controllers and reducing the negative impacts of traffic congestion in urban areas. the implementation of this proposed solution could contribute to a more sustainable and efficient transportation system in the future.', 'a deep reinforcement learning approach to traffic signal control traffic signal control using reinforcement learning has been proved to have potential in alleviating traffic congestion in urban areas. although research has been conducted in this field, it is still an open challenge to find an effective but low-cost solution to this problem. this paper presents multiple deep reinforcement learning-based traffic signal control systems that can help regulate the flow of traffic at intersections and then compares the results. the proposed systems are coupled with sumo (simulation of urban mobility), an agent-based simulator that provides a realistic environment to explore the outcomes of the models.']"
81,infrastructure,6,91,81_traffic_vehicle_vehicles_road,"['traffic', 'vehicle', 'vehicles', 'road', 'congestion', 'detection', 'traffic congestion', 'mobility', 'autonomous', 'transportation', 'traffic flow', 'recognition', 'safety', 'cities', 'smart', 'transport', 'parking', 'intelligent', 'deep', 'deep learning', 'environment', 'proposed', 'driving', 'time', 'intelligent transportation', 'systems', 'public', 'autonomous vehicles', 'video', 'real']","['traffic forecasting with deep learning timely forecast of traffic is very much needed for smart cities, which allows travelers and government agencies to make various decisions based on traffic flow. this will result in reduced traffic congestion and carbon dioxide emission. however, traffic forecasting is a challenging task due to the highly complex traffic pattern. standard time series techniques may not be able to capture the nonlinear and noisy nature of the traffic flow. in this paper, we investigate how the deep learning models capture these characteristics and provide better predictive performance over standard time series and regression models. we compare the performances of state-of-the-art deep learning models on two traffic flow data sets and show their effectiveness in traffic flow prediction over traditional models.', 'short-term traffic congestion prediction using hybrid deep learning technique a vital problem faced by urban areas, traffic congestion impacts wealth, climate, and air pollution in cities. sustainable transportation systems (stss) play a crucial role in traffic congestion prediction for adopting transportation networks to improve the efficiency and capacity of traffic management. in stss, one of the essential functional areas is the advanced traffic management system, which alleviates traffic congestion by locating traffic bottlenecks to intensify the interpretation of the traffic network. furthermore, in urban areas, accurate short-term traffic congestion forecasting is critical for designing transport infrastructure and for the real-time optimization of traffic. the main objective of this paper was to devise a method to predict short-term traffic congestion (sttc) every 5 min over 1 h. this paper proposes a hybrid xception support vector machine (xpsvm) classifier model to predict sttc. primarily, the xception classifier uses separable convolution, relu, and convolution techniques to predict the feature detection in the dataset. secondarily, the support vector machine (svm) classifier operates maximum marginal separations to predict the output more accurately using the weight regularization technique and a fine-tuned binary hyperplane mechanism. the dataset used in this work was taken from google maps and comprised snapshots of bangalore, karnataka, taken using the selenium automation tool. the experimental outcome showed that the proposed model forecasted traffic congestion with an accuracy of 97.16%.', 'an augmented deep learning inference approach of vehicle headlight recognition for on-road vehicle detection and counting vehicles have been a big part of many lives; from the time it is invented and as it increases popularity in the 20th century. though they offer the benefit of convenience, they also have certain negative effects as they add to air pollution and global warming, as well as risks when they are not handled properly. in recent years, the number of vehicles on the road is rapidly increasing and it causes different major concerns. one of the major effects of this increasing volume of vehicles is the traffic congestion it caused on our roads especially in the urban areas. this traffic congestion became one of the major problems in many cities in the world including metro manila, philippines. many options are discussed and implemented by the traffic management but it seems that it is still unsolved. in recent years, traffic congestions became unpredictable, there are parts of the cities that dont experience traffic congestion then suddenly traffic builds up to that area. also, traffic congestion might happen every hour of the day. with this concern, the study proposed a system for vehicle headlight recognition for on-road vehicle detection and counting. this study focused on the detection of the headlight of every vehicle that will be seen on the perimeter of the installed camera. the system can detect headlight vehicles during daytime and nighttime as we trained the ai to recognized the headlights in these scenarios. possible applications of this system can be in monitoring the volume of vehicles within the area and it can be used by traffic management authority in monitoring the build-up of traffic or traffic situation in an area so that they can provide an immediate solution, such as re-routing, one-way street conversions, etc.']"
82,infrastructure,6,42,82_buildings_building_smart_smart buildings,"['buildings', 'building', 'smart', 'smart buildings', 'energy', 'iot', 'home', 'intelligent', 'systems', 'indoor', 'control', 'intelligence', 'comfort', 'environments', 'cognitive', 'things', 'internet', 'energy efficiency', 'internet things', 'artificial intelligence', 'smart home', 'devices', 'technologies', 'homes', 'artificial', 'construction', 'smart building', 'architecture', 'energy consumption', 'environment']","['realization of topology matching algorithm for space node network of intelligent building platform based on machine learning with the development of the internet and delivery services, online exchange services have become more and more convenient and popular. the application of publishing/smart reading systems in the field of exchange services has changed the way people deal with information. the rapid development of emerging technologies such as the internet of things, virtual reality, and artificial intelligence, the continuous improvement of cloud computing and big data infrastructure, the rapid development of the building intelligence industry, and the development of building automation systems. the control algorithm in the new building intelligent platform has the characteristics of no center and distribution. at this stage, the engineering application test of the algorithm is difficult and there is a risk of damaging electromechanical equipment. it is necessary to build a simulation model combined with the new building intelligent simulation platform in the new building intelligent platform for simulation test and verification. in order to achieve the goal of building a sustainable development society, green buildings are widely popular because of their low energy consumption, people-oriented, local conditions and environment-friendly characteristics. the replica placement strategy for the new building intelligent platform performs better in load balancing than the default replica placement strategy; the replica selection strategy for the new building intelligent platform has higher user access efficiency than the default replica selection strategy. experiments verify the effectiveness of the two strategies.', 'survey of artificial intelligence of things for smart buildings: a closer outlook artificial intelligence of things (aiot) is a term used to describe the integration of artificial intelligence (ai) and internet of things (iot) technologies. aiot combines the capabilities of ai algorithms with the data generated by iot devices to enable real-time decision-making and automation of various processes. smart buildings refers to a type of building that utilizes advanced technologies to improve its efficiency, performance, and functionality of indoor tasks in a way that provide a safe and comfortable environment for occupants. this paper provides an overview of the research literature on aiot technologies that is contribute to the development of smart buildings and their functionality. we discuss the benefits of aiot empowered smart buildings, which include reduced energy consumption and costs, improved occupant comfort and productivity, and increased safety and security. we also discusses the challenges associated with the deployment of aiot in smart buildings, including data privacy and security concerns, interoperability issues, and the need for specialized expertise. further, we discuss the promising areas of future research that pave the way for further research on aiot empowered smart buildings. we concludes our work with a discussion of the potential for aiot empowered smart buildings to contribute to the sustainability of cities and improve the quality of life for their occupants.', 'b-smart: a reference architecture for artificially intelligent autonomic smart buildings the pervasive application of artificial intelligence and machine learning algorithms is transforming many industries and aspects of the human experience. one very important industry trend is the move to convert existing human dwellings to smart buildings, and to create new smart buildings. smart buildings aim to mitigate climate change by reducing energy consumption and associated carbon emissions. to accomplish this, they leverage artificial intelligence, big data, and machine learning algorithms to learn and optimize system performance. these fields of research are currently very rapidly evolving and advancing, but there has been very little guidance to help engineers and architects working on smart buildings apply artificial intelligence algorithms and technologies in a systematic and effective manner. in this paper we present b-smart: the first reference architecture for autonomic smart buildings. b-smart facilitates the application of artificial intelligence techniques and technologies to smart buildings by decoupling conceptually distinct layers of functionality and organizing them into an autonomic control loop. we also present a case study illustrating how b-smart can be applied to accelerate the introduction of artificial intelligence into an existing smart building.']"
83,infrastructure,6,57,83_iot_sensor_internet_things,"['iot', 'sensor', 'internet', 'things', 'internet things', 'monitoring', 'devices', 'mining', 'smart', 'air', 'wireless', 'environment', 'dust', 'environmental monitoring', 'emissions energy', 'environmental', 'real time', 'wireless sensor', 'applications', 'things iot', 'industrial', 'air quality', 'pollution', 'intelligence', 'time', 'technology', 'sensors', 'ambient intelligence', 'real', 'computing']","['smart embedded framework using arduino and iot for real-time noise and air pollution monitoring and alert system with rapid increase in human population, industrialization, infrastructural developments, vehicles and utilization of fossil fuels, climate change, noise, water and air pollution and other environmental issues are increasing drastically. to ensure healthy living and a better future, it is essential to monitor these issues and provide solutions to overcome them. the smart sensor networks that combine electronics, wireless communication and computer sciences is an emerging field of research that can contribute towards monitoring noise and air pollution level. this paper provides a solution for noise and air pollution level monitoring in any area of interest using wireless embedded computing system. all devices in the system inclusive of esp8266, xmega 2560, sound sensor, dust, gas, humidity and temperature sensor as well as wifi are connected via internet of things (iot). thingspeak environment is used for recording the collected sound and air quality information. alert is sent to the authorities whenever the pollution exceeds a certain set limit.', 'integration of wireless sensor network and iot for smart environment monitoring system air contamination, water waste, and radioactive contamination are significant environmental issues. adequate supervision is needed to ensure economic sustainability through the preservation of a good society. environmental tracking has become a smart environment monitoring (sem) system in recent times, with developments in the internet of things and the creation of advanced detectors. this situation evaluates substantial achievements and scientific studies on sem, including air quality control, water management, radiation emissions, and agricultural practices. a wireless sensor network and iot integrated system for smart environment monitoring (wsn-iot-sec) framework are proposed in this research. the analysis will be divided employing sem techniques applications, and each aim will then be further studied in terms of the detectors, machine learning models, and classifiers operated. a systematic study was carried out based on the studys evaluated results and patterns, indicating important suggestions and the sem analysiss influence. the researchers have discussed objectively how the advancements in mobile technologies, iot, and wireless sensor networks allow the control of the atmosphere an intelligent monitoring device. eventually, the concept of rigorous machine learning techniques has been proposed, denouncing techniques and developing appropriate wsn specifications.', 'advancement of environmental monitoring system using iot and sensor: a comprehensive analysis the emergence of the internet of things (iot) has brought a revolution in global communication network technology. it has acquired many day-to-day applications in healthcare, education, agriculture, etc. in addition, iot has also had a significant impact in the field of environmental monitoring.the significant factors in a healthy environment are air quality, water pollution, and waste management, where the worlds population can live securely. monitoring is necessary for us to achieve global sustainability. as monitoring technology has advanced in recent years, environmental monitoring systems have evolved from essential remote monitoring to an advanced environment monitoring (aem) system, incorporating internet of things (iot) technology and sophisticated sensor modules.the present manuscript aims to accomplish a critical review of noteworthy contributions and research studies about environmental monitoring systems, which involve monitoring air quality, water quality, and waste management.the rapid growth of the worlds population and the exhaustion of natural resources, coupled with the increasing unpredictability of environmental conditions, lead to significant concerns about worldwide food security, global warming, water pollution, and waste overflowing. automating tasks in the building environment, based on the internet of things (iot) application, is meant to eliminate problems with the traditional approach. this study aims to examine and evaluate numerous studies involving monitoring air, water, waste, and overall environmental pollution, as well as their effect on the environment. this article categorizes studies based on their research purposes, techniques, and findings. this paper examines advanced environmental monitoring systems through sensor technology, iot, and machine learning.']"
84,infrastructure,6,78,84_smart_cities_smart cities_city,"['smart', 'cities', 'smart cities', 'city', 'smart city', 'urban', 'technologies', 'sustainable', 'ai', 'governance', 'internet things', 'things', 'internet', 'citizens', 'intelligence', 'development', 'technology', 'digital', 'artificial intelligence', 'big', 'infrastructure', 'artificial', 'living', 'challenges', 'urbanism', 'sustainability', 'smart sustainable', 'iot', 'urban development', 'sustainable urban']","['integrating data-based strategies and advanced technologies with efficient air pollution management in smart cities the covid-19 pandemic has demonstrated that creative leadership based on data and citizen volunteers is more significant than vaccines themselves, so this study focuses on the collaboration of sophisticated technologies and human potential to monitor air pollution. air pollution contributes to critical environmental problems in various towns and cities. with the emergence of the smart city concept, appropriate methods to curb exposure to pollutants must be part of an appropriate urban development policy. this study presents a technologically driven air quality solution for smart cities that advertises energy-efficient and cleaner sequestration in these areas. it attempts to explore how to incorporate data-driven approaches and citizen participation into effective public sector pollution management in smart cities as a major component of the smart city definition. the smart city idea was developed as cities became more widespread through communication devices. this study addresses the technical criteria for implementing a framework that public administration can use to prepare for renovation of public buildings, minimizing energy use and costs and linking smart police stations to monitor air pollution as a part of an integrated city. such a digital transition in resource management will increase public governance energy performance and provide a higher standard for operations and a healthier environment. the study results indicate that complex processes lead to efficient and sustainable smart cities. this research discovered an interpretive pattern in how public agencies, private enterprises, and community members think and what they do in these regional contexts. it concludes that economic and social benefits could be realized by exploiting data-driven smart city development for its social and spatial complexities.', 'enabling technologies and sustainable smart cities the technological interventions in everyday processes has led to the rise of smart ecosystems where all aspects of everyday life like governance, transportation, agriculture, logistics, maintenance, education and healthcare are automated in some way or the other and can be controlled, managed and accessed remotely with the help of smart devices. this has led to the concept of smart cities where information communication and technology (ict) is merged with the existing traditional infrastructure of a city which is then coordinated and managed using digital technology. this idea of smart cities is slowly but surely coming into reality as many countries around the globe are adopting this idea and coming up with their own model of smart cities. at the core of smart city lies the sensors and actuators embedded in the smart devices that sense the environment for facilitating effective decision making. the microcontrollers available in these devices are programmed to take decisions automatically based on the information received from the sensors. this involves integration of several information and communication technologies like artificial intelligence, protocols, internet of things (iot), wireless sensor network (wsn) etc. this paper discusses and extensively reviews the role of enabling technologies in smart cities. the paper further highlights the challenges and limitations in the development of smart cities along with the mitigation strategies. specifically, three categories of challenges are identified namely technical, socio-economic and environmental giving specifics of each category. finally, some of the best practices for attaining sustainable smart cities are provided.', 'amalgamation of advanced technologies for sustainable development of smart city environment: a review the concept of smart city evolved with the integration of information and communication technology (ict) in various sub-systems and processes in urban environment. the development of the smart cities is the best possible solution to major urban issues. it contributes towards economic and social development of the residents. it aims to provide the cordial environment in the domains of healthcare, education, transportation, power generation and dissipation, security, living, industry, etc., to the inhabitants to make their lives comfortable. sustainability of these services is another major objective in a smart city framework. along with the true realization of the idea of a smart city, advanced computational and communication technologies are contributing hugely towards its sustainable development. communication technologies act as backbone to ensure connectivity at the various levels in a smart city framework. novel smart city solutions for different application domains are designed and deployed by the industry using advanced computational technologies like iot, artificial intelligence, blockchain, big data and cloud computing. in this work, authors discuss the concept of smart city, its architecture and sustainability. different operational domains in a smart city ecosystem are elaborated. the cyber physical aspect of the smart cities is discussed in brief. the role of various computational and communication technologies in the sustainable development of smart cities is presented. limiting factors in the deployment of various advanced technologies in different smart city domains are highlighted. security issues associated with the technological sustainable development of different smart city services along with existing solutions are discussed. the article is concluded by highlighting the future research directions.']"
90,infrastructure,6,43,90_decision_transport_decision support_urban,"['decision', 'transport', 'decision support', 'urban', 'sustainability', 'support', 'infrastructure', 'transportation', 'tourism', 'indicators', 'planning', 'city', 'criteria', 'sustainable', 'development', 'economic', 'making', 'policy', 'regeneration', 'cities', 'waste', 'paper', 'systems', 'environment', 'management', 'process', 'methodology', 'decision making', 'case', 'project']","['supporting sustainable transport appraisals using stakeholder involvement and mcda appraisal processes for transport initiatives are often characterised by their complexity involving a wide range of impacts that need to be addressed and many stakeholders that attempt to influence the decisions to be made. the increasing interest for the environment and sustainable development in general has stressed the need for taking a broad perspective into account when addressing transport initiatives. this means that economic, social and environmental dimensions need to be considered simultaneously in the appraisal process. the focus on incorporating such sustainability considerations has set new demands for the appraisal process and has revealed an increasing need for involving stakeholders in the decision support process to capture all aspects of the often complex decision problems. conventional appraisals within the transport area are often only based on cost-benefit analysis, which captures the impacts that can be assigned with a monetary value. thus there is a need for a decision support system that is able to assess the effect of other types of impacts as well and include this in the appraisal. this paper seeks to fill this gap in research by proposing a methodology making use of planning workshops and multi-criteria decision analysis in combination to improve the decision support. in order to serve the purpose of promoting a more sustainable transport planning approach a proposal is made for how the methodology can be integrated in the current practice for appraisal of infrastructure projects in denmark (and countries with similar ap-proaches). the paper concludes that the approach allowing for active stakeholder participation in the appraisal process can serve as a helpful and effective decision support system in the quest for more sustainable solutions to transport problems.', 'decision support system in public transport planning for promoting urban adaptation to climate change current land use policy in developed countries result in many new constructions sites and urban development. depending on the geodesign solutions, newly developed areas may influence their citizens to use private or public transport. therefore, to design more resilient and eco-friendly neighbourhoods, it is important to incorporate public transport planning at the early stage of urban planning. in order to promote urban adaptation to climate change it is crucial to activate citizens to participate in decisions making process related to the common space and available facilities. suitable for this purpose are the right tools for the presentation of various design solutions. this group of tools is called decision support systems. the aim of the research is verification of the suitability of the use of decision support system (communityviz) for public transport planning at the level for a master plan (local planning). the paper presents the possibility of using the communityviz system to create various scenarios of accessible public transport. simulations were performed on the newly designed housing area in san sebastian (spain). the designed model for the neighbourhood was used during the workshop with citizens of san sebastian, to incorporate their knowledge as local experts. based on the master plan, three scenarios of bus stops location were proposed. the dynamic model enabled to assess how many citizens may probably live in acceptable distance to bus stops, which may reflect the number of people for whom public transport may be an attractive solution for transportation needs. the results showed high usefulness of the analyzed decision support system to solve the problem of public transport designing. intuitive graphical presentation makes different variants clearly identified by all stakeholder groups.', 'new spatial decision support systems for sustainable urban and regional development purpose – policy makers are frequently challenged by the need to achieve sustainable development in cities and regions. current decision-making processes are based on evaluation support systems which are unable to tackle the problem as they cannot take a holistic approach or a full account of actors. the purpose of this paper is to present a new generation of evaluation systems to support decision making in planning and regeneration processes which involve expert participation. these systems ensure network representation of the issues involved and visualization of multiple scenarios. design/methodology/approach – a literature review is used for both revising existing evaluation tools in urban planning and the built environment and highlighting the need to give stakeholders (industry, cities, operators, etc.) new tools for collaborative or individual decisions and to facilitate scaling up solutions. an overview of the new generation of decision support systems, named multicriteria spatial decision support systems (mc-sdss) is provided and real case studies are analyzed to show their ability to tackle the problem. findings – recent research findings highlight that decisions in urban planning should be supported by collaborative and inclusive processes. otherwise, they will fail. the case studies illustrated in this study highlight the usefulness of mc-sdss for the successful resolution of complex problems, thanks to the visualization facilities and a network representation of the scenarios. research limitations/implications – the case studies are limited to the italian context. practical implications – these sdss are able to empower planners and decision makers to better understand the interaction between city design, social preferences, economic issues and policy incentives. therefore, they have been employed in several case studies related to territorial planning and regeneration processes. originality/value – this study provides three case studies and a review of the new mc-sdss methodology which involve the analytic network process technique to support decision-making in urban and regional planning.']"
95,infrastructure,6,94,95_design_building_construction_buildings,"['design', 'building', 'construction', 'buildings', 'bim', 'architectural', 'materials', 'decision', 'sustainable', 'environment', 'urban', 'energy', 'material', 'sustainability', 'layout', 'embodied', 'renovation', 'environmental', 'process', 'decision support', 'performance', 'optimization', 'life cycle', 'bridge', 'life', 'built', 'shading', 'project', 'designers', 'built environment']","['a decision support system for selecting sustainable materials in construction projects major construction activity has had a significant effect on the environment, the economy and society in the twenty first century. the gap between the energy required to supply such construction, and the actual energy produced, particularly in the building sector, has put enormous pressure on the selection of sustainable construction materials. even though the construction industry continues to place this pressure on natural resources, it has also resulted in an increase in the production of waste material, which is being dumped all over the world. in recent years, a lot of work has been undertaken to develop sustainable buildings/housing. the selection of appropriate sustainable building materials is one of the most important processes in the building design and construction process. immediate efforts are needed to improve the awareness of sustainable materials among decision makers as there is limited technological support to assist technical experts (i.e. engineers, estimators, architects, draftsmen etc.) in the decision-making process when searching for and selecting new sustainable construction materials and practices for different building components (e.g. walls, roof, slab etc.). moreover, cyber-physical infrastructure and information technologies are essential to support the sustainability of construction projects; this will be discussed further in this paper.', 'calculation of embodied ghg emissions in early building design stages using bim and nlp-based semantic model healing to reach the goals of limiting global warming, the embodied greenhouse gas (ghg) emissions of new buildings need to be quantified and optimized in the very early design stages, during which design decisions significantly influence the success of projects in achieving their performance goals. semantically rich building information models (bim) enable to perform an automated quantity take-off of the relevant elements for calculating a whole building life cycle assessment (lca). however, imprecise type and property information often found in todays bim practice hinders a seamless processing for downstream applications. at the same time, the early design stages are characterized by high uncertainty due to the lack of information and knowledge, making a holistic and consistent lca for supporting design decisions and optimizing performance challenging. in assessing this often vague information, it is essential to consider different levels of element and material information for matching bim to lca data. for example, the structural properties of concrete are not yet defined in early design stages and should instead be considered as a range of material options due to different compressive strength classes. this paper presents a novel methodology for automatically matching the coarse information available in bim models of the early design stages to the respective entries in lca databases as a basis for a fully automated calculation process of the embodied ghg emissions of new buildings. this approach solves the existing gap in the automation process of manually enriching bim models and adding information of lca data and missing layers of vague models. in more detail, the proposed method is based on natural language processing (nlp), using different strategies to increase performance in matching elements and materials from a bim model to a knowledge database to enrich environmental indicators of commonly used elements’ materials. the knowledge database contains all missing information for lcas and has different levels of information for a range of several potential design options of elements and materials, including their dependencies. accordingly, this paper investigates multiple nlp techniques and evaluates the performance of state-of-the-art deep learning models such as germanet, spacy, or bert. following this, the most performant nlp approach is used to provide an automatic workflow for matching industry foundation classes (ifc) elements to the knowledge database, facilitating a seamless lca in the early stages of design. for five different case studies, the performances of the proposed matching method are analyzed. finally, one case study is selected to compare the embodied emissions results to those of the conventional process.', 'integrating decision support system (dss) and building information modeling (bim) to optimize the selection of sustainable building components one of the challenges in sustainability analysis and its development is the optimum selection of sustainable materials to meet the projects requirements while doing sustainable design. this can only be achieved when project team adopt the use of a strategic approach while selecting the materials, although this could be a complex task for decision makers. building information modeling (bim) offers designers the ability to assess different design alternatives at the conceptual stage of a project. as a method of integration and through its modeling techniques, bim can be used to assess the impacts of design alternatives on the energy saving of buildings all over their life. furthermore, bim has the potential to help designers select the right type of materials during the early design stage, and make vital decisions when selecting the materials that have sustainable impact on the buildings life cycle. the main purpose of this study is to propose a methodology that integrates bim with decision-making problem-solving approaches (i.e. entropy-topsis) in order to efficiently optimize the selection of sustainable building components at the conceptual design stage of building projects. therefore, a decision support system (dss) is developed by using multiple criteria decision making (mcdm) techniques to aid the design team decide on and select the optimum type of sustainable building components and design families while doing conceptual design of proposed projects, based on three main criteria (i.e. environmental factors, economic factors-cost efficiency, and social well-being) in an attempt to identify the influence of design variations on the whole buildings sustainable performance. the multi-criteria procedure embedded in the dss relies on numerical models to simulate alternative situations, as well as ranking the alternatives and select the best ones based on both the owners strategic preferences and the availability of sustainable materials in the market. the set of models included in the dss describes the relationship between sustainability criteria, manufacturers sustainable materials and the interactions between project team that take place during the design of sustainable building projects. this paper aims at exposing the feasibility of using bim for analysing the life cycle costs of sustainable buildings at the conceptual stage. the design alternatives suggested by the dss are evaluated in an integrated environment that joins bim concept and life cycle cost (lcc) method to analyze the operational cost of the whole building. an actual building project is used to validate the workability and capability of the proposed methodology.']"
97,infrastructure,6,154,97_building_energy_buildings_building energy,"['building', 'energy', 'buildings', 'building energy', 'energy consumption', 'consumption', 'comfort', 'cooling', 'thermal', 'indoor', 'heating', 'thermal comfort', 'residential', 'performance', 'energy use', 'energy performance', 'occupancy', 'design', 'energy efficiency', 'hvac', 'ventilation', 'prediction', 'office', 'control', 'air', 'simulation', 'demand', 'retrofit', 'occupants', 'efficiency']","['data-driven urban energy simulation (due-s): a framework for integrating engineering simulation and machine learning methods in a multi-scale urban energy modeling workflow the world is rapidly urbanizing, and the energy intensive built environment is becoming increasingly responsible for the worlds energy consumption and associated environmental emissions. as a result, significant efforts have been put forth to develop methods that can accurately model and characterize building energy consumption in cities. these models aim to utilize physics-based building energy simulations, reduced-order calculations and statistical learning methods to assess the energy performance of buildings within a dense urban area. however, current urban building energy models are limited in their ability to account for the inter-building energy dynamics and urban microclimate factors that can have a substantial impact on building energy use. to overcome these limitations, this paper proposes a novel data-driven urban energy simulation (due-s) framework that integrates a network-based machine learning algorithm (resnet) with engineering simulation to better understand how buildings consume energy on multiple temporal (hourly, daily, monthly) and spatial scales in a city (single building, block, urban). we validate the proposed due-s framework on a proof of concept case study of 22 densely located university buildings in california, usa. our results indicate that the due-s framework is able to accurately predict urban scale energy consumption at hourly, daily and monthly intervals. moreover, our results also demonstrate that the integration of data-driven and engineering simulation approaches can partially capture the inter-building energy dynamics and impacts of the urban context and merits future work to explore how they can be improved to predict sub-urban scale energy predictions (single building, block). in the end, successfully predicting and modeling the energy performance of urban buildings has the potential to inform the decision-making of a wide variety of urban sustainability stakeholders including architects, engineers and policymakers.', 'the impact of climate change on a university campus’ energy use: use of machine learning and building characteristics global warming is expected to increase 1.5◦ c between 2030 and 2052. this may lead to an increase in building energy consumption. with the changing climate, university campuses need to prepare to mitigate risks with building energy forecasting models. although many scholars have developed buildings energy models (bems), only a few have focused on the interpretation of the meaning of bem, including climate change and its impacts. additionally, despite several review papers on bems, there is no comprehensive guideline indicating which variables are appropriate to use to explain building energy consumption. this study developed building energy prediction models by using statistical analysis: multivariate regression models, multiple linear regression (mlr) models, and relative importance analysis. the outputs are electricity (elc) and steam (stm) consumption. the independent variables used as inputs are building characteristics, temporal variables, and meteorological variables. results showed that categorizing the campus buildings by building type is critical, and the equipment power density is the most important factor for elc consumption, while the heating degree is the most critical factor for stm consumption. the laboratory building type is the most stm-consumed building type, so it needs to be monitored closely. the prediction models give an insight into which building factors remain essential and applicable to campus building policy and campus action plans. increasing stm is to raise awareness of the severity of climate change through future weather scenarios.', 'a hybrid simulation model to predict the cooling energy consumption for residential housing in hong kong in hong kong, buildings consume 90% of the electricity generated and over 60% of the city’s carbon emissions are attributable to generating power for buildings. in 2018, hong kong residential sector consumed 41,965 tj (26%) of total electricity generated, with private housing accounting for 52% and public housing taking in 26%, making them the two major contributors of greenhouse gas emissions. furthermore, air conditioning was the major source consuming 38% of the electricity generated for the residential building segment. strategizing building energy efficiency measures to reduce the cooling energy consumption of the residential building sector can thus have far-reaching benefits. this study proposes a hybrid simulation strategy that integrates artificial intelligence techniques with a building energy simulation tool (energyplus™) to predict the annual cooling energy consumption of residential buildings in hong kong. the proposed method predicts long-term thermal energy demand (annual cooling energy consumption) based on shortterm (hourly) simulated data. the hybrid simulation model can analyze the impacts of building materials, construction solutions, and indoor–outdoor temperature variations on the cooling energy consumed in apartments. the results indicate that using low thermal conductivity building materials for windows and external walls can reduce the annual cooling energy consumption by 8.19%, and decreasing the window-to-wall ratio from 80% to 40% can give annual cooling energy savings of up to 18%. moreover, significant net annual cooling energy savings of 13.65% can be achieved by changing the indoor set-point temperature from 24◦c to 26◦c. the proposed model will serve as a reference for building energy efficiency practitioners to identify key relationships between building physical characteristics and operational strategies to minimize cooling energy demand at a minimal time in comparison to traditional energy estimation methods.']"
11,landuse_urban,7,40,11_mobility_travel_built environment_cycling,"['mobility', 'travel', 'built environment', 'cycling', 'built', 'transport', 'trips', 'transportation', 'urban', 'trip', 'transit', 'environment', 'mode', 'active', 'emissions', 'urban mobility', 'city', 'cities', 'women', 'public', 'survey', 'transportation energy', 'urban form', 'household', 'behavior', 'sustainable', 'form', 'sharing', 'choice', 'usage']","['exploring associations between the built environment and cycling behaviour around urban greenways from a human-scale perspective the incorporation of cycling as a mode of transport has been shown to have a positive impact on reducing traffic congestion, improving mental health outcomes, and contributing to the development of sustainable cities. the proliferation of bike-sharing systems, characterised by their wide availability and high usage rates, has made cycling in urban areas more accessible and convenient for individuals. while the existence of a relationship between cycling behaviour and the built environment has been established, few studies have specifically examined this connection for weekdays and weekends. with the emergence of new data sources, new methodologies have become available for research into this area. for instance, bike-sharing spatio-temporal datasets have made it possible to precisely measure cycling behaviour over time, while street-view images and deep learning techniques now enable researchers to quantify the built environment from a human perspective. in this study, we used 139,018 cycling trips and 14,947 street-view images to examine the connection between the built environment consisting of urban greenways and cycling behaviour. the results indicated that the greenness and enclosure of the level of greenway were positively correlated with increased cycling on both weekdays and weekends. however, the openness of the greenway appears to have opposing effects on cycling behaviour depending on the day of the week, with high levels of openness potentially promoting cycling on weekends but hindering it on weekdays. based on the findings of this study, policymakers and planners should focus on the cycling environment and prioritise improving its comfort and safety to promote green transportation and bicycle-friendly cities.', 'examining threshold effects of built environment elements on travel-related carbon-dioxide emissions understanding how built environment features are associated with travel-related carbon-dioxide (co2) emissions is essential for planners to encourage environmentally sustainable travel through transportation and land use policies. applying gradient boosting decision trees to the data from the minneapolis-st. paul metropolitan area, this study addresses two gaps in the literature by identifying critical built environment determinants of co2 emissions, and more importantly, illustrating threshold effects of built environment elements. the results show that three neighborhood-level built environment factors have the strongest influences on co2 emissions: distance to the nearest transit stop, job density, and land use diversity. the distance to downtowns also has a substantial impact. this study further confirms that built environment variables are effective only within a certain range. these threshold effects offer valuable implications for planners to achieve desirable environmental benefits efficiently.', 'built environment interventions for emission mitigation: a machine learning analysis of travel-related co<inf>2</inf> in a developing city the transport sector accounts for more than one-fifth of global co2 emissions. reducing fossil fuel consumption and travel-related co2 emissions (tce) is a major approach to mitigating global climate change. urban planners worldwide propose to promote low-carbon travel by changing the built environment. therefore, understanding the relationships between built environment variables and tce is key to the development of land use and transportation policies. using 2019 regional household travel data from zhongshan, a polycentric urban area in china, this study developed a gradient boosting decision trees model to estimate the relative importance of built environment variables in predicting tce and their nonlinear associations with tce. built environment variables collectively contribute nearly half of the predictive power to predicting tce, suggesting the potential of built environment interventions. among them, location accessibility to city-level and township-level centers and population density are the top-three important features in predicting tce. furthermore, most built environment variables show threshold relationships with tce. the results suggest that polycentric development, intensification of town centers, and densification of street networks are conducive to tce mitigation. these findings inform planners of effective ranges of built environment variables to promote low-carbon travel.']"
57,landuse_urban,7,24,57_es_ecological_ess_ecosystem,"['es', 'ecological', 'ess', 'ecosystem', 'development', 'yellow river', 'yellow', 'ecosystem services', 'landscape', 'ecological security', 'mountain', 'conservation', 'sustainable development', 'services', 'water conservation', 'river', 'water', 'sustainable', 'capacity', 'carrying capacity', 'economic', 'tourism', 'carrying', 'land', 'reaches', 'china', 'grasslands', 'cultural', 'river basin', 'gep']","['socio-ecological determinants of multiple ecosystem services on the mediterranean landscapes of the ionian islands (greece) mediterranean islands are widely recognized as biodiversity hotspots, with a long history of human activities shaping multi-functional landscapes. socioeconomic and environmental factors are among the most important factors driving the creation of diverse landscapes, with a high supply of ecosystem services (es). however, these factors, along with climate change, could also have irreversible consequences on local ecosystems. thus, this study aimed to reveal the importance of socio-ecological factors in shaping es bundles to manage natural resources efficiently and enhance human well-being. using the ionian islands as a case study, we explored the relationships among multiple es, including their supply and demand indicators. we identified bundles of es to distinguish regions in which supply and demand exhibit different characteristics. an ensemble machine learning method (random forest - rf) was used to identify the most important socio-ecological variables out of 17 tested that contribute to es bundles. our results produced five bundles of es supply and six bundles of es demand. the most important variables for the distribution of es supply bundles were landscape heterogeneity, elevation, slope, landscape connectivity, and population. in comparison, variables representing elevation, slope, and population were among the most important variables contributing to es demand bundles. rf exhibited both good classification and predictability, which was supported by the accuracy measures. our findings demonstrated that research on es should account for underlying socio-ecological drivers that influence the supply and demand of es to improve our understanding of the possible impacts of future management decisions regarding the diverse mediterranean landscapes of the ionian islands.', 'effect of physical geographic and socioeconomic processes on interactions among ecosystem services based on machine learning a thorough understanding of the interactions of ecosystem services (ess) can enable effective es planning and management to curtail their degradation and enhance restoration. so it is essential to explore the ecosystem processes and driving mechanisms of es relationships. here, the objective of this study is to investigate the mechanisms of multiple es interactions and provide a decision-making reference for es management. a dimension reduction to 11 ess was performed using principal component analysis, and a machine learning approach based on a bayesian belief network (bbn) was used to identify the effects of physical geographic and socioeconomic processes on es trade-offs and synergies. considering the nansihu lake basin, china, as a study area, 11 ess, namely nutrient retention (nitrogen and phosphorus), aquatic products, habitat quality, pollination, carbon storage, landscape esthetic quality, water yield, food provision, biomass production, and erosion control, mapped in 2018 were quantified. four principal components, namely synergies between nutrient retention, aquatic production, and habitat quality; synergies between pollination, carbon storage, and landscape esthetic quality; trade-offs between water yield and food provision, biomass production; and erosion control, were extracted from multiple ess. bbn sensitivity analysis revealed that among the socioeconomic factors (land use type, population density, and night-time light), climate (precipitation, temperature), topogeography (digital elevation model, slope), and soil characteristics (soil types and soil texture), land use exhibits the most critical effect on es interactions. the response of es capacities and interactions to land use, climate, and soil management were predicted through bbn scenario analysis, and the results revealed that critical decision-making can optimize multiple ess. the results of this study can provide a guideline of es interactions for sustainable management to maximize es capacities and limit es trade-offs.', 'basin integrity and temporal-spatial connectivity of the water ecological carrying capacity of the yellow river the yellow river basin is an important ecological barrier and economic zone in china, and both its ecological conservation and sustainable high-quality development are directly related to the success or failure of the ecological civilization construction of china. currently, the yellow river basin, which is short on water resources, has a fragile natural ecology, and lags in economic and social development, is a key and difficult area for the ecological security and economic and social development of china. this paper summarizes the remarkable achievements in comprehensive prevention and control of soil erosion, such as the implementation of a series of major ecological projects, in the yellow river basin since the founding of new china, especially in recent years. these major ecological projects were implemented based on the different characteristics of the upstream, midstream, and downstream regions of the yellow river basin. however, the ecological function of the yellow river basin has not been fundamentally resolved. this is due to three main reasons. the first is the extent and spatial imbalance in the control of water and soil erosion in the yellow river basin watershed. ecosystem quality has degraded and water conservation has declined in the upper reaches of the yellow river, soil erosion is serious in the middle reaches, and the ecological flow is low and some estuary wetlands have shrunk in the lower reaches. the second reason is the adverse effect of water and sediment regulation on delta and coastal systems. vegetation coverage has declined at the source of the yellow river; there is the desertification of grasslands and the area of secondary bare lands has increased. some degraded black-soil beaches have undergone secondary degradation and associated disasters have also increased. the third reason includes water and sediment problems caused by large-scale coal mining, such as serious changes in topography and landforms. there is the acceleration of land desertification and water shortages, where the original landforms are destroyed and some vegetation disappears in the upper reaches; the ground collapses and serious soil erosion causes increased sediments to flow into the yellow river in the middle reaches. surface subsidence deformation, water accumulation, and secondary salinization lead to a decrease in land productivity in the lower reaches. it is proposed that the key to solving the current outstanding problems restricting ecological protection and high-quality sustainable development of the yellow river basin lies in the integrity of the water ecology of the basin, spatial-temporal connectivity and ecosystem health to realize the scientific allocation of water resources in ecology, industry/mining, life, agriculture, and water and sediment regulation in the context of climate warming. to realize this scientific objective, three tasks of research are urgently needed. first, the clarification of the theory and technology of the digital yellow river information platform, and timely and accurate determination of the rivers status are critical to understand and scientifically develop and manage the yellow river basin. second, water resource monitoring and cloud water resource development and utilization across the entire yellow river basin must be carried out, which will require full use of advanced technologies, such as artificial weather modification, rational development and utilization of cloud water resources, the construction of water resource monitoring and cloud water resource development and utilization as soon as possible, and promotion of the effective use of water resources. finally, identification and prediction of major ecological disaster risks are essential, which requires the integration of space-air-ground stereo observation; big data and machine learning; fusion of 5g, artificial intelligence, the internet of things, and other technical methods to establish the intelligent perception and identification of precursors for major disasters; disaster scenario simulation and risk prediction; and disaster risk prevention and control technology systems to enhance risk identification and management capabilities in ecological protection and high-quality sustainable development.']"
69,landuse_urban,7,75,69_urban_heat_lst_urban heat,"['urban', 'heat', 'lst', 'urban heat', 'land', 'heat island', 'surface', 'temperature', 'surface temperature', 'thermal', 'land surface', 'island', 'cities', 'urbanization', 'city', 'vegetation', 'cover', 'built', 'effect', 'urban areas', 'air temperature', 'urban climate', 'land cover', 'daytime', 'areas', 'temperature lst', 'land use', 'summer', 'climate', 'ta']","['prediction of seasonal urban thermal field variance index using machine learning algorithms in cumilla, bangladesh the intensity and formation of urban heat island (uhi) phenomena are closely related to land use/land cover (lulc) and land surface temperature (lst) change. the effect of uhi can be described quantitatively by urban thermal field variance index (utfvi). for measuring urban health and ensuring sustainable development, the analysis of lst and utfvi are receiving boosted attention. this study predicted lulc, seasonal (summer & winter) lst, and utfvi variations using machine learning algorithms (mlas) in cumilla city corporation (ccc), bangladesh. landsat 4–5 tm and landsat 8 oli satellite images were used for 1999, 2009, and 2019 to predict future scenarios for 2029 and 2039. mlas such as cellular automata (ca) and artificial neural network (ann) methods were used to predict the future change in lulc, lst, and utfvi. the result suggests that, in the year 2029 and 2039, the urban area will likely to be increased by around 8 % and 11 %, where significant decrease will be taken place in green cover by 9 % and 14 %. if the rapid urban growth continues, more than 30 % of the ccc area will likely to be experienced more than 33 °c temperature and strongest utfvi effect in the year 2029 and 2039. in addition, an average 4 °c higher lst was recorded in the urban area compared with vegetation cover. in urban construction practice, avoiding concentrated impermeable layers (built-up areas) and increasing green covers, are effective ways of mitigating the effect of utfvi. this study will contribute in achieving sustainable development and provide useful insights to understand the complex relationship among different elements of urban environments and promotion of city competence.', 'assessing surface urban heat island related to land use/land cover composition and pattern in the temperate mountain valley city of kathmandu, nepal rapid urban growth has coincided with a substantial change in the environment, including vegetation, soil, and urban climate. the surface urban heat island (uhi) is the temperature in the lowest layers of the urban atmosphere; it is critical to the surface’s energy balance and makes it possible to determine internal climates that affect the livability of urban residents. therefore, the surface uhi is recognized as one of the crucial global issues in the 21st century. this phenomenon affects sustainable urban planning, the health of urban residents, and the possibility of living in cities. in the context of sustainable landscapes and urban planning, more weight is given to exploring solutions for mitigating and adapting to the surface uhi effect, currently a hot topic in urban thermal environments. this study evaluated the relationship between land use/land cover (lulc) and land surface temperature (lst) formation in the temperate mountain valley city of kathmandu, nepal, because it is one of the megacities of south asia, and the recent population increase has led to the rapid urbanization in the valley. using landsat images for 2000, 2013, and 2020, this study employed several approaches, including machine learning techniques, remote sensing (rs)-based parameter analysis, urban-rural gradient analysis, and spatial composition and pattern analysis to explore the surface uhi effect from the urban expansion and green space in the study area. the results revealed that kathmandu’s surface uhi effect was remarkable. in 2000, the higher mean lst tended to be in the city’s core area, whereas the mean lst tended to move in the east, south, north, and west directions by 2020, which is compatible with urban expansion. urban periphery expansion showed a continuous enlargement, and the urban core area showed a predominance of impervious surface (is) on the basis of urban-rural gradient analysis. the city core had a lower density of green space (gs), while away from the city center, a higher density of gs predominated at the three time points, showing a lower surface uhi effect in the periphery compared to the city core area. this study reveals that landscape composition and pattern are significantly correlated with the mean lst in kathmandu. therefore, in discussing these findings in order to mitigate and adapt to prominent surface uhi effects, this study provides valuable information for sustainable urban planning and landscape design in mountain valley cities like kathmandu.', 'spatial variation of land use/cover composition and impact on surface urban heat island in a tropical sub-saharan city of accra, ghana rapid urbanization is one of the most crucial issues in the world of the 21st century. notably, the urban heat island phenomenon is becoming more prominent in megacities and their hinterlands in temperate and subtropical climatic regions. in the daytime in summer, there exists a high possibility of accelerating the land surface temperature (lst) in desert cities, due to the alterations made by human beings in the natural environment. in this study, we investigate the spatial formation of lst in a tropical sub-saharan city of accra, a gateway to west africa, using landsat data in 2003 and 2017. machine learning techniques and the different spatial and statistical methods such as tasseled cap transformation (tct), urban-rural gradient, and multiresolution grid-based and landscape metrics were employed to examine procured land use/cover (luc) and lst maps. luc was classified into five categories: built up, green 1, green 2, bare land, and water. the results of the analysis indicate that built up, green 2, and bare land had caused the highest heating effect while green 1 and water had caused the considerable cooling effect during the daytime in accra. the urban-rural difference in lst recorded 1.4 °c in 2003 and 0.28 °c in 2017. the mean size, mean shape, largest patch, and aggregation of built up, green 1, and green 2 had a strong relationship with the mean lst. it is essential for urban planners to carefully examine the formation and effect of the urban heat island (uhi) for sustainable urban development and landscape policy toward mitigation and adaptation planning in accra.']"
70,landuse_urban,7,105,70_urban_street_built_cities,"['urban', 'street', 'built', 'cities', 'landscape', 'built environment', 'spatial', 'environment', 'planning', 'street view', 'vitality', 'settlement', 'view', 'urban planning', 'city', 'sustainable', 'urban environment', 'urbanization', 'areas', 'development', 'greenery', 'urban growth', 'learning', 'images', 'sustainable urban', 'density', 'open', 'view images', 'settlements', 'design']","['the six dimensions of built environment on urban vitality: fusion evidence from multi-source data long-standing attention has been given to urban vitality and its association with the built environment (be). however, the multiplicity and complex impacts of be factors that shape urban vitality patterns have not been fully explored. for this purpose, multisource data from 1025 communities in wuhan, china, were combined to explore the be vitality nexus. a deep learning method was explored to segment street-view images, on which a composite indicator of urban vitality was developed with social media data. then, six dimensions of be factors, neighbourhood attributes, urban form and function, landscape, location, and street configuration, were incorporated into a spatial regression model to systematically examine the composite influences. the results show that population density, community age, open space, the sidewalk ratio, streetlights, shopping and leisure density, integration, and proximity to transportation are positive factors that induce urban vitality, whereas the effects of road density, proximity to parks, and green space have the opposite results. this study contributes to an improved understanding of the be nexus. managerial implications for mediating the relationship between planning policies and urban design strategies for the optimization of resource allocation and promotion of sustainable development are discussed.', 'exploring the association between street built environment and street vitality using deep learning methods street vitality has become an essential indicator for evaluating the attractiveness and potential of the sustainable development of urban blocks, and it can be reflected by the type and the frequency of peoples pedestrian activities on the street. while it is recognized that street built environment features affect pedestrian behavior and street vitality, quantifying the impact of these characteristics remains inconclusive. this paper proposes an automated deep learning approach to quantitatively explore the association between the street built environment and street vitality. first, we established a deep learning model for street vitality classification for automatic evaluation of street vitality based on the volumes and activities of pedestrians in the street through multiple object tracking and scene classification. then, we applied semantic segmentation to measure five selected vitality-related street built environment variables. finally, a linear regression model was applied to evaluate the built environment variables’ significance and effects on street vitality. to verify our methods accuracy and applicability, we selected a commercial complex in osaka as an illustrative example. the experimental results highlight that street width and transparency have significant positive effects on street vitality. compared with traditional methods, our approach is feasible, reliable, transferable, and more efficient.', 'using street view images and a geographical detector to understand how street-level built environment is associated with urban poverty: a case study in guangzhou understanding and ending poverty has become one of the most important sdg (sustainable development goals) all over the world. the street-level built environment is an important indicator to reflect urban poverty. however, traditional data such as satellite imagery may not provide fine-grained information of built environment at the street level. in recent years, street view image has become promising data for assessing an urban micro environment. this study aimed to use street view data and deep learning technique to examine the association between street-level built environment and urban poverty in guangzhou, china, from a geographical heterogeneity perspective. first, we measured urban poverty in guangzhou based on the index of multiple deprivation. second, we used the pyramid scene parsing network model for image segmentation and then performed principal component analysis to extract five major street view factors (i.e., vegetation enclosure sense, color complexity sense, road openness sense, sky openness sense, and building enclosure sense) from the street view data. third, we conducted the geographical detector analysis to examine how street view factors is associated with urban poverty. results suggested that vegetation enclosure sense, color complexity sense, and road openness sense are significantly related to the spatial heterogeneity of urban poverty. among all factors, vegetation enclosure sense played a leading role. the results also confirmed the coexistence of different street view factors have association with the spatial heterogeneity of urban poverty. in conclusion, street-level built environment is generally associated with urban poverty, and therefore our proposed method can be considered as an efficiently method for identifying urban poor communities.']"
22,water_marine_env,8,42,22_water_water bodies_bodies_extraction,"['water', 'water bodies', 'bodies', 'extraction', 'lake', 'images', 'water body', 'lakes', 'glacial', 'surface water', 'body', 'imagery', 'remote sensing', 'remote', 'sensing', 'deep', 'surface', 'deep learning', 'satellite', 'segmentation', 'ndwi', 'sentinel', 'resolution', 'landsat', 'water quality', 'clarity', 'sensing images', 'image', 'water extraction', 'features']","['extraction of urban water bodies from high-resolution remote-sensing imagery using deep learning accurate information on urban surface water is important for assessing the role it plays in urban ecosystem services in the context of human survival and climate change. the precise extraction of urban water bodies from images is of great significance for urban planning and socioeconomic development. in this paper, a novel deep-learning architecture is proposed for the extraction of urban water bodies from high-resolution remote sensing (hrrs) imagery. first, an adaptive simple linear iterative clustering algorithm is applied for segmentation of the remote-sensing image into high-quality superpixels. then, a new convolutional neural network (cnn) architecture is designed that can extract useful high-level features of water bodies from input data in a complex urban background and mark the superpixel as one of two classes: an including water or no-water pixel. finally, a high-resolution image of water-extracted superpixels is generated. experimental results show that the proposed method achieved higher accuracy for water extraction from the high-resolution remote-sensing images than traditional approaches, and the average overall accuracy is 99.14%.', 'deep feature and domain knowledge fusion network for mapping surface water bodies by fusing google earth rgb and sentinel-2 images mapping surface water bodies from fine spatial resolution optical remote sensing imagery is essential for the understanding of the global hydrologic cycle. although satellite data are useful for mapping, the limited spectral information captured by some satellite systems can be suboptimal for the task. for example, the very high-resolution images of google earth (ge) only contain rgb bands, which often means many water bodies and land objects are confused. sentinel-2 (s2) imagery has a spectral resolution more suitable for mapping water bodies, but its medium spatial resolution limits the ability for detailed mapping of water-land boundaries. this letter proposes a deep feature and domain knowledge fusion network (dfdkfnet) for mapping surface water bodies by fusing ge and s2 images while incorporating domain knowledge. dfdkfnet uses the remote sensing indices of normalized difference water index (ndwi) and normalized difference vegetation index (ndvi) derived from the s2 image as the representative domain knowledge to better extract water bodies from terrestrial features. a similar pixel-based approach is used to downscale the ndwi and ndvi maps to match the spatial resolution between the ge and s2 images. the dfdkfnet uses the ge and downscaled ndwi and ndvi images to extract the deep semantic features of water bodies, which are fused with the domain knowledge extracted from the ndwi and ndvi images. dfdkfnet was compared with several state-of-the-art algorithms, and the results show that dfdkfnet can enhance water body mapping accuracy.', 'accurate water extraction using remote sensing imagery based on normalized difference water index and unsupervised deep learning large-scale monitoring of surface water bodies is of great significance to the sustainable development of regional ecosystems. remote sensing is currently the main method of global earth observation. on the one hand, the traditional water index is simple and efficient, but it relies on a fixed global threshold, which leads to low accuracy for water extraction. on the other hand, deep learning has achieved state-of-the-art results in classification of spectral data, but it consumes a substantial amount of manpower and time to label sufficient high-quality samples. furthermore, spectral characteristics of water bodies in different areas vary greatly due to changes in atmospheric conditions and viewing geometry. in this paper, we propose a new accurate water extraction framework based on unsupervised deep learning and ndwi of multispectral images. binarized ndwi images are used to identify potential water bodies, and deep learning training is performed using these pseudo samples and labels. this process realizes the conversion from unlabeled learning to noisy label learning. first, we proposed a simple and fast binarization algorithm to segment as many real water bodies as possible from ndwi images. then a set of water confidence assessment rules was constructed from the four aspects of the spectrum, shape, agglomeration, and range. the water segments were scored and sorted to make the model start learning from easy samples and gradually advance to complex samples. finally, the adjusted co-teaching learning strategy is adopted to filter errors introduced by noisy labels through peer networks with different learning capabilities during training. we tested the accuracy of our method using gaofen image dataset (gid), sentinel-2 and landsat images of several water bodies in china. compared with other methods, our method improved the f1 score by 18.1–40.3% and 6.8–22.2% for gid and sentinel-2 images, respectively. in addition, our method is more stable in long-term water monitoring. the proposed method has the potential to be used for extracting water bodies with high accuracy on a large scale, especially in areas with complex environments and a lack of deep learning samples. and it provides a new idea for unsupervised learning in the current remote sensing field by fully combining remote sensing expertise and spectral information of ground objects.']"
27,water_marine_env,8,29,27_permafrost_plateau_tibet_qinghai,"['permafrost', 'plateau', 'tibet', 'qinghai', 'thaw', 'qinghai tibet', 'tibet plateau', 'qtp', 'thawing', 'lake', 'tibetan', 'ice', 'tundra', 'spatial distribution', 'lakes', 'area', 'distribution', 'tibetan plateau', 'deformation', 'land', 'region', 'permafrost degradation', 'susceptibility', 'ground', 'climate warming', 'surface', 'cover', 'warming', 'changes', 'temperature']","['changes in permafrost spatial distribution and active layer thickness from 1980 to 2020 on the tibet plateau the tibetan plateau (tp) is experiencing extensive permafrost degradation due to climate change, which seriously threatens sustainable water and ecosystem management in the tp and its downstream areas. understanding the evolution of permafrost is critical for studying changes in the water cycle, carbon flux, and ecology of the tp. in this study, we mapped the spatial distribution of permafrost and active layer thickness (alt) at 1 km resolution for each decade using empirical models and machine learning methods validated with borehole data. a comprehensive comparison of model results and validation accuracy shows that the machine learning method is more advantageous in simulating the permafrost distribution, while the alt simulated by the empirical model (i.e., stefan model) better reflects the actual alt distribution. we further evaluated the dynamics of permafrost distribution and alt from 1980 to 2020 based on the results of the better-performing models, and analyzed the patterns and influencing factors of the changes in permafrost distribution and alt. the results show that the permafrost area on the tp has decreased by 15.5 %, and the regionally average alt has increased by 18.94 cm in the 2010s compared to the 1980s. the average decreasing rate of permafrost area is 6.33 × 104 km2 decade−1, and the average increasing rate of alt is 6.31 cm decade−1. permafrost degradation includes the decreasing permafrost area and the thickening active layer mainly related to the warming of the tp. spatially, permafrost area decrease is more susceptible to occur at lower latitudes and lower altitudes, while alt increases more dramatically at lower latitudes and higher altitudes. in addition, permafrost is more likely to degrade to seasonally frozen ground in areas with deeper alt.', 'spatiotemporal patterns and regional differences in soil thermal conductivity on the qinghai–tibet plateau the qinghai–tibet plateau is an area known to be sensitive to global climate change, and the problems caused by permafrost degradation in the context of climate warming potentially have far-reaching effects on regional hydrogeological processes, ecosystem functions, and engineering safety. soil thermal conductivity (stc) is a key input parameter for temperature and surface energy simulations of the permafrost active layer. therefore, understanding the spatial distribution patterns and variation characteristics of stc is important for accurate simulation and future predictions of permafrost on the qinghai–tibet plateau. however, no systematic research has been conducted on this topic. in this study, based on a dataset of 2972 stc measurements, we simulated the spatial distribution patterns and spatiotemporal variation of stc in the shallow layer (5 cm) of the qinghai–tibet plateau and the permafrost area using a machine learning model. the monthly analysis results showed that the stc was high from may to august and low from january to april and from september to december. in addition, the mean stc in the permafrost region of the qinghai–tibet plateau was higher during the thawing period than during the freezing period, while the stc in the eastern and southeastern regions is generally higher than that in the western and northwestern regions. from 2005 to 2018, the difference between the stc in the permafrost region during the thawing and freezing periods gradually decreased, with a slight difference in the western hinterland region and a large difference in the eastern region. in areas with specific landforms such as basins and mountainous areas, the changes in the stc during the thawing and freezing periods were different or even opposite. the stc of alpine meadow was found to be most sensitive to the changes during the thawing and freezing periods within the permafrost zone, while the stc for bare land, alpine desert, and alpine swamp meadow decreased overall between 2005 and 2018. the results of this study provide important baseline data for the subsequent analysis and simulation of the permafrost on the qinghai–tibet plateau.', 'simulation of the present and future projection of permafrost on the qinghai-tibet plateau with statistical and machine learning models the comprehensive understanding of the occurred changes of permafrost, including the changes of mean annual ground temperature (magt) and active layer thickness (alt), on the qinghai-tibet plateau (qtp) is critical to project permafrost changes due to climate change. here, we use statistical and machine learning (ml) modeling approaches to simulate the present and future changes of magt and alt in the permafrost regions of the qtp. the results show that the combination of statistical and ml method is reliable to simulate the magt and alt, with the root-mean-square error of 0.53°c and 0.69\xa0m for the magt and alt, respectively. the results show that the present (2000–2015) permafrost area on the qtp is 1.04\xa0×\xa0106\xa0km2 (0.80–1.28\xa0×\xa0106\xa0km2), and the average magt and alt are −1.35\xa0±\xa00.42°c and 2.3\xa0±\xa00.60\xa0m, respectively. according to the classification system of permafrost stability, 37.3% of the qtp permafrost is suffering from the risk of disappearance. in the future (2061–2080), the near-surface permafrost area will shrink significantly under different representative concentration pathway scenarios (rcps). it is predicted that the permafrost area will be reduced to 42% of the present area under rcp8.5. overall, the future changes of magt and alt are pronounced and region-specific. as a result, the combined statistical method with ml requires less parameters and input variables for simulation permafrost thermal regimes and could present an efficient way to figure out the response of permafrost to climatic changes on the qtp.']"
28,water_marine_env,8,20,28_snow_snow depth_snow cover_depth,"['snow', 'snow depth', 'snow cover', 'depth', 'sd', 'cover', 'cm', 'products', 'resolution', 'qm', 'microwave', 'retrieval', 'mountain', 'brightness', 'fused', 'northeast china', 'passive microwave', 'ablation', 'china', 'high', 'reflectance', 'northeast', 'sensing', 'forest', 'frost', 'fine resolution', 'remote sensing', 'passive', 'fy', 'mapping']","['estimating snow-depth by fusing satellite and station observations: a deep learning approach deriving accurate snow depth is of great importance since snow cover is an informat i ve indicator of climate change. the objective of this study is to develop a snow-depth retrieva l algorithm based on a deep learning approach by fusing passive microwave remote sensing brightness temperature, station observations and gnss-r snow-depth product to improve the accuracy of snow-depth retrieval. the results show that dbn performs the best compared with three alternative algorithms.', 'a long-term daily gridded snow depth dataset for the northern hemisphere from 1980 to 2019 based on machine learning a high-quality snow depth product is very import for cryospheric science and its related disciplines. current long time-series snow depth products covering the northern hemisphere can be divided into two categories: remote sensing snow depth products and reanalysis snow depth products. however, existing gridded snow depth products have some shortcomings. remote sensing-derived snow depth products are temporally and spatially discontinuous and tend to underestimate snow depth, while reanalysis snow depth products have coarse spatial resolutions and great uncertainties. to overcome these problems, in our previous work we proposed a novel data fusion framework based on random forest regression of snow products from advanced microwave scanning radiometer for the earth observing system (amsr-e), advanced microwave scanning radiometer-2 (amsr2), global snow monitoring for climate research (globsnow), the northern hemisphere snow depth (nhsd), era-interim, and modern-era retrospective analysis for research and applications, version 2 (merra-2), incorporating geolocation (latitude and longitude), and topographic data (elevation), which were used as input independent variables. more than 30,000 ground observation sites were used as the dependent variable to train and validate the model in different time periods. this fusion framework resulted in a long time series of continuous daily snow depth product over the northern hemisphere with a spatial resolution of 0.25°. here, we compared the fused snow depth and the original gridded snow depth products with 13,272 observation sites, showing an improved precision of our product. the evaluation indices of the fused (best original) dataset yielded a coefficient of determination r2 of 0.81 (0.23), root mean squared error (rmse) of 7.69 (15.86) cm, and mean absolute error (mae) of 2.74 (6.14) cm. most of the bias (88.31%) between the fused snow depth and in situ observations was in the range of −5 cm to 5 cm. the accuracy assessment of independent snow observation sites–sodankylä (sod), old aspen (oas), old black spruce (obs), and old jack pine (ojp)–showed that the fused snow depth dataset had high precision for snow depths of less than 100 cm with a relatively homogeneous surrounding environment. the results of random point selection and independent in situ site validation show that the accuracy of the fused snow depth product is not significantly improved in deep snow areas and areas with complex terrain. in the altitude range of 100 m to 2000 m, the fused snow depth had a higher precision, with r2 varying from 0.73 to 0.86. the fused snow depth had a decreasing trend based on the spatiotemporal analysis and mann-kendall trend test method. this fused snow depth product provides the basis for understanding the temporal and spatial characteristics of snow cover and their relation to climate change, hydrological and water cycle, water resource management, ecological environment, snow disaster and hazard prevention.', 'prediction of snow depth based on multi-source data and machine learning algorithms as an important element of the earths surface, snow cover plays an important role in the global terrestrial ecosystem, climate change, water cycle and energy cycle. snow depth (sd) provides information on the spatial distribution of snow cover and material energy information. it is also used to study the climatic effects of snow cover, water balance in the basin, snowmelt runoff simulation, and monitoring and evaluation of snow disasters. snow depth data has become an indispensable basic supporting data in multi-disciplinary research. however, the current snow depth data is relatively poor in completeness and consistency and cannot meet the needs of related scientific research and industry applications, which will bring great confusion to users. this study attempts to use machine learning methods to effectively integrate snow depth data products from multiple sources to obtain a snow depth data set with high consistency in china. this paper chooses the passive microwave remote sensing data (westdc), ground-based data (canadian meteorological centre, cmc) and land surface models (global land data assimilation system, gldas; the nasa modern-era retrospective analysis for research and applications merra2; the european centre for medium-range weather forecasts interim reanalysis, era-interim) as the main sd data source of random forest model, and considering the influencing factors (e.g., land cover, snow class, forest cover fraction, surface roughness). the results show that the random forest fusion model method can effectively gather the advantages of each data source, improve the accuracy of snow depth estimation, and reduce the inconsistency between snow depth data from multiple sources. the correlation coefficient between the fused snow depth dataset and the observed snow depth can reach 0.87, and the root mean square error is 5.1 cm. therefore, the multi-source snow depth fusion using random forest model can improve the accuracy of snow depth estimation.']"
29,water_marine_env,8,110,29_ice_sea ice_sea_arctic,"['ice', 'sea ice', 'sea', 'arctic', 'glacier', 'glaciers', 'sar', 'permafrost', 'mass', 'lake', 'polar', 'lakes', 'radar', 'debris', 'images', 'antarctic', 'sentinel', 'microwave', 'thaw', 'resolution', 'deep', 'deep learning', 'extent', 'glacial', 'imagery', 'satellite', 'learning', 'net', 'segmentation', 'passive microwave']","['development of ann-based algorithm to estimate wintertime sea ice temperature profile over the arctic ocean the thermal structure of the arctic sea ice is a critical indicator in the atmosphere-sea ice-ocean energy budget and, thus, for understanding arctic warming and associated climate change. therefore, understanding this thermal structure and its monitoring should be vital. however, it is challenging to obtain a three-dimensional view of the thermal structure of the sea ice (such as the temperature profile) through satellite measurements because of the lack of understanding of the nonlinear relationship between sea ice emission and measured radiance at the top of the atmosphere. in this study, a model was developed to estimate the temperature profile within the arctic sea ice during winter using satellite-borne passive microwave measurements. an artificial neural network (ann) technique based on deep learning was introduced, and the nonlinear relationship between satellite-measured brightness temperatures and buoy-measured sea ice temperature profiles was learned. the ann model was mapped and verified using the 10-fold cross-validation technique. the developed ann model was able to restore the sea ice temperatures at all specified levels with correlation coefficients &#x003e; 0.95, absolute biases &#x003c; 0.1 k, and root mean square errors &#x003c; 1.6 k. the retrieved temperature results well represent expected thermal structures, in addition to the snow-sea ice interface temperature similar to that in the published literature. besides the data for validating climate model simulations, the results also promise applications for improving the sea ice growth model performance by tightly constraining the vertical thermal structure in the sea ice growth model.', 'sea ice extent prediction with machine learning methods and subregional analysis in the arctic the decline of sea ice in the arctic region is a critical indicator of rapid global warming and can also influence the feedback processes in the arctic, so the prediction of sea ice extent and thickness plays an important role in climate modeling and prediction. this paper uses machine learning methods to predict the sea ice extent, and by adjusting the methods and factors, which include the climate variables, the past sea ice extent, and the simple linear-regression-simulated sea ice extent, then we found the best combination to give the result with the highest r2 score. we noticed that with longer periods of past sea ice extent data and shorter periods of climate data, the results appeared to be better. this might be related to the difference in climate and ocean memory. the sub-region sea ice extent prediction shows that the regions with whole-year ice cover are easier to predict and that those regions with sudden weather changes and significant seasonal variability appear to have lower r2 scores in the sea ice extent prediction.', 'a year-round satellite sea-ice thickness record from cryosat-2 arctic sea ice is diminishing with climate warming1 at a rate unmatched for at least 1,000 years2. as the receding ice pack raises commercial interest in the arctic3, it has become more variable and mobile4, which increases safety risks to maritime users5. satellite observations of sea-ice thickness are currently unavailable during the crucial melt period from may to september, when they would be most valuable for applications such as seasonal forecasting6, owing to major challenges in the processing of altimetry data7. here we use deep learning and numerical simulations of the cryosat-2 radar altimeter response to overcome these challenges and generate a pan-arctic sea-ice thickness dataset for the arctic melt period. cryosat-2 observations capture the spatial and the temporal patterns of ice melting rates recorded by independent sensors and match the time series of sea-ice volume modelled by the pan-arctic ice ocean modelling and assimilation system reanalysis8. between 2011 and 2020, arctic sea-ice thickness was 1.87 ± 0.10 m at the start of the melting season in may and 0.82 ± 0.11 m by the end of the melting season in august. our year-round sea-ice thickness record unlocks opportunities for understanding arctic climate feedbacks on different timescales. for instance, sea-ice volume observations from the early summer may extend the lead time of skilful august–october sea-ice forecasts by several months, at the peak of the arctic shipping season.']"
37,water_marine_env,8,32,37_ocean_bay_carbon_ph,"['ocean', 'bay', 'carbon', 'ph', 'sea', 'argo', 'chlorophyll', 'oceanic', 'sea surface', 'global', 'satellite', 'products', 'bgc', 'npp', 'chla', 'surface', 'oceans', 'carbonate', 'waters', 'global ocean', 'dissolved', 'aquaculture', 'marine', 'nutrient', 'situ', 'co2', 'organic', 'primary', 'br', 'biogeochemistry']","['improved quantification of ocean carbon uptake by using machine learning to merge global models and pco<inf>2</inf> data the ocean plays a critical role in modulating climate change by sequestering co2 from the atmosphere. quantifying the co2 flux across the air-sea interface requires time-dependent maps of surface ocean partial pressure of co2 (pco2), which can be estimated using global ocean biogeochemical models (gobms) and observational-based data products. gobms are internally consistent, mechanistic representations of the ocean circulation and carbon cycle, and have long been the standard for making spatio-temporally resolved estimates of air-sea co2 fluxes. however, there are concerns about the fidelity of gobm flux estimates. observation-based products have the strength of being data-based, but the underlying data are sparse and require significant extrapolation to create global full-coverage flux estimates. the lamont doherty earth observatory-hybrid physics data (ldeo-hpd) pco2 product is a new approach to estimating the temporal evolution of surface ocean pco2 and air-sea co2 exchange. ldeo-hpd uses machine learning to merge high-quality observations with state-of-the-art gobms. we train an extreme gradient boosting (xgb) algorithm to learn a non-linear relationship between model-data mismatch and observed predictors. gobm fields are then corrected with the predicted model-data misfit to estimate real-world pco2 for 1982–2018. the resulting reconstruction by ldeo-hpd is in better agreement with independent pco2 observations than other currently available observation-based products. within uncertainties, ldeo-hpd global ocean uptake of co2 agrees with other products and the global carbon budget 2020.', 'remote sensing of global sea surface ph based on massive underway data and machine learning seawater ph is a direct proxy of ocean acidification, and monitoring the global ph distribution and long-term series changes is critical to understanding the changes and responses of the marine ecology and environment under climate change. owing to the lack of sufficient global-scale ph data and the complex relationship between seawater ph and related environmental variables, generating time-series products of satellite-derived global sea surface ph poses a great challenge. in this study, we solved the problem of the lack of sufficient data for ph algorithm development by using the massive underway sea surface carbon dioxide partial pressure (pco2) dataset to structure a large data volume of near in situ ph based on carbonate calculation between underway pco2 and calculated total alkalinity from sea surface salinity and relevant parameters. the remote sensing inversion model of ph was then constructed through this massive ph training dataset and machine learning methods. after several tests of machine learning methods and groups of input parameters, we chose the random forest model with longitude, latitude, sea surface temperature (sst), chlorophyll a (chla), and mixed layer depth (mld) as model inputs with the best performance of correlation coefficient (r2 = 0.96) and root mean squared error (rmse = 0.008) in the training set and r2 = 0.83 (rmse = 0.017) in the testing set. the sensitivity analysis of the error variation induced by the uncertainty of sst and chla (sst ≤ ±0.5 °c and chla ≤ ±20%; rmsesst ≤ 0.011 and rmsechla ≤ 0.009) indicated that our sea surface ph model had good robustness. monthly average global sea surface ph products from 2004 to 2019 with a spatial resolution of 0.25° × 0.25° were produced based on the satellite-derived sst and chla products and modeled mld dataset. the ph model and products were validated using another independent station-measured ph dataset from the global ocean data analysis project (glodap), showing good performance. with the time-series ph products, refined interannual variability and seasonal variability were presented, and trends of ph decline were found globally. our study provides a new method of directly using remote sensing to invert ph instead of indirect calculation based on the construction of massive underway calculated ph data, which would be made useful by comparing it with satellite-derived pco2 products to understand the carbonate system change and the ocean ecological environments responding to the global change.', 'estimation of fugacity of carbon dioxide in the east sea using in situ measurements and geostationary ocean color imager satellite data the ocean is closely related to global warming and on-going climate change by regulating amounts of carbon dioxide through its interaction with the atmosphere. the monitoring of ocean carbon dioxide is important for a better understanding of the role of the ocean as a carbon sink, and regional and global carbon cycles. this study estimated the fugacity of carbon dioxide (ƒco2 ) over the east sea located between korea and japan. in situ measurements, satellite data and products from the geostationary ocean color imager (goci) and the hybrid coordinate ocean model (hycom) reanalysis data were used through stepwise multi-variate nonlinear regression (mnr) and two machine learning approaches (i.e., support vector regression (svr) and random forest (rf)). we used five ocean parameters—colored dissolved organic matter (cdom; <0.3 m−1 ), chlorophyll-a concentration (chl-a; <21 mg/m3 ), mixed layer depth (mld; <160 m), sea surface salinity (sss; 32–35), and sea surface temperature (sst; 8–28 ◦ c)—and four band reflectance (rrs) data (400 nm–565 nm) and their ratios as input parameters to estimate surface seawater ƒco2 (270–430 µatm). results show that rf generally performed better than stepwise mnr and svr. the root mean square error (rmse) of validation results by rf was 5.49 µatm (1.7%), while those of stepwise mnr and svr were 10.59 µatm (3.2%) and 6.82 µatm (2.1%), respectively. ocean parameters (i.e., sea surface salinity (sss), sea surface temperature (sst), and mixed layer depth (mld)) appeared to contribute more than the individual bands or band ratios from the satellite data. spatial and seasonal distributions of monthly ƒco2 produced from the rf model and sea-air co2 flux were also examined.']"
40,water_marine_env,8,20,40_ocean_marine_underwater_oceans,"['ocean', 'marine', 'underwater', 'oceans', 'feed', 'autonomous', 'ocean acidification', 'monitoring', 'coastal', 'acidification', 'aquaculture', 'marine environment', 'feeding', 'offshore', 'environment', 'maritime', 'new', 'sea', 'observing', 'vehicles', 'florida', 'marine ecosystems', 'labor', 'ocean observing', 'advances', 'artificial', 'oil spills', 'intelligence', 'deep sea', 'ecosystems']","['iot based ocean acidification monitoring system with ml based edge analytics ocean acidification (oa) is often referred to as an evil twin of climate change. increased global atmospheric emission of co2 has increased its concentration in the ocean due to dissolution and absorption of atmospheric co2, leading to ocean acidification. this further affects the coral reefs and other calcifiers such as pteropods (marine snails), shellfish (clams, etc.), crustaceans (shrimp and lobsters), starfish, sea urchins, and their kin (echinoderms) and interferes with the marine food web and leads to food shortages thereby affecting the socio-economic lives of the people living in the coastal area. to quantify the above impacts and to study other biochemical variations in the ocean, it is necessary to monitor ocean acidification for a longer time period. current methods for monitoring ocean acidification employ gliders, drifters, buoys, mooring, and periodic testing of samples collected during research voyages. these methods are often not cost-effective, time-efficient, requires a lot of manpower, and provides inferior spatio-temporal resolution. this research work analyzes the existing ocean acidification data collected from the aloha station at the north pacific ocean and propose a predictive model at the edge level by using regression to reduce the effective cost of the iot system. the proposed regression model is also used to design a novel iot architecture for designing an energy-efficient, near real-time ocean acidification monitoring model. the proposed iot hardware can be deployed by using buoys and fishing boats to further reduce the monitoring cost and increase the spatio-temporal resolution of the data compared to the existing monitoring systems.', 'submarine cables as precursors of persistent systems for large scale oceans monitoring and autonomous underwater vehicles operation long-term and reliable marine ecosystems monitoring is essential to address current environmental issues, including climate change and biodiversity threats. the existing oceans monitoring systems show clear data gaps, particularly when considering characteristics such as depth coverage or measured variables in deep and open seas. over the last decades, the number of fixed and mobile platforms for in situ ocean data acquisition has increased significantly, covering all oceans regions. however, these are largely dependent on satellite communications for data transmission, as well as on research cruises or opportunistic ship surveys, generally presenting a lag between data acquisition and availability. in this context, the creation of a widely distributed network of smart cables (science monitoring and reliable telecommunications) - sensors attached to submarine telecommunication cables - appears as a promising solution to fill in the current ocean data gaps and ensure unprecedented oceans health continuous monitoring. the k2d (knowledge and data from the deep to space) project proposes the development of a persistent oceans monitoring network based on the use of telecommunications cables and autonomous underwater vehicles (auvs). the approach proposed includes several modules for navigation, communication and energy management, that enable the cost-effective gathering of extensive oceans data. these include physical, chemical, and biological variables, both registered with bottom fixed stations and auvs operating in the water column. the data that can be gathered have multiple potential applications, including oceans health continuous monitoring and the enhancement of existing ocean models. the latter, in combination with geoinformatics and artificial intelligence, can create a continuum from the deep sea to near space, by integrating underwater remote sensing and satellite information to describe earth systems in a holistic manner.', 'radioactivity monitoring in ocean ecosystems (ramones) natural radioactivity in the marine environment has been present since the earths formation, while artificial radionuclides were introduced into the oceans in 1944. more recent direct sources exist that feed the oceans, such as low-level liquid discharges from reprocessing plants, large-scale releases due to disasters (e.g. fukushima hit by the tsunami in 2011), and smaller-scale radiological events. exploration of submarine environments should consider the existence of radioactivity in terms of its short- and long-term impact on marine and coastal ecosystems, also in correlation to natural hazards, such as seismic activity over submarine faults or activity of hydrothermal vent fields near the seabed. significantly undersampled in oceans, radioactivity poses real risks to marine ecosystems and human population, urging for detailed, data-driven modeling. ramones is a new h2020-eu fet proactive project [2] aiming to offer new and efficient solutions for in in situ, continuous, long-term monitoring of radioactivity in harsh subsea environments. a new generation of submarine radiation sensing instruments, assisted by state-of-the-art (soa) robotics and artificial intelligence (ai) will be developed towards understanding radiation related risks near and far from coastal areas, while providing data towards shaping new policies and guidelines for environmental sustainability, economic growth and human health, offering a framework for defining future environmental intelligence guidelines and practices. the main ambition is to lay a radical new path to close the existing marine radioactivity under-sampling gap and foster new interdisciplinary research in threatened natural deep-sea ecosystems. ramones will invest a significant effort to provide tools for long-term, rapid deployments, propose new robotics and ai-driven supported methodologies, and offer scaled-up solutions to researchers, policy makers and communities. ramones will combine soa equipment from various disciplines and advanced modeling in fine synergy, and design new and effective approaches for the marine environment to provide efficient response to natural and man-made hazards, shaping future policies for the global population.']"
43,water_marine_env,8,42,43_coral_underwater_fish_marine,"['coral', 'underwater', 'fish', 'marine', 'reefs', 'coral reefs', 'ocean', 'reef', 'images', 'image', 'vision', 'species', 'monitoring', 'yolo', 'detection', 'deep', 'segmentation', 'computer vision', 'video', 'computer', 'training', 'counting', 'deep learning', 'imagery', 'habitats', 'dataset', 'classification', 'automated', 'learning', 'recognition']","['combining photogrammetric computer vision and semantic segmentation for fine-grained understanding of coral reef growth under climate change corals are the primary habitat-building life-form on reefs that support a quarter of the species in the ocean. a coral reef ecosystem usually consists of reefs, each of which is like a tall building in any city. these reef-building corals secrete hard calcareous exoskeletons that give them structural rigidity, and are also a prerequisite for our accu-rate 3d modeling and semantic mapping using advanced photogrammetric computer vision and machine learning. underwater videography as a modern underwater remote sensing tool is a high-resolution coral habitat survey and mapping technique. in this paper, detailed 3d mesh models, digital surface models and orthophotos of the coral habitat are generated from the collected coral images and underwa-ter control points. meanwhile, a novel pixel-wise semantic segmentation approach of orthophotos is performed by ad-vanced deep learning. finally, the semantic map is mapped into 3d space. for the first time, 3d fine-grained semantic modeling and rugosity evaluation of coral reefs have been completed at millimeter (mm) accuracy. this provides a new and powerful method for understanding the processes and characteristics of coral reef change at high spatial and temporal resolution under climate change.', 'novel approaches to enhance coral reefs monitoring with underwater image segmentation coral reefs not only inhabiting millions of species that are primarily or completely associated with them, but also produce economic and cultural benefits to coastal societies around the world. in recent years, affected by climate change and human factors, coral reef ecosystem has been experiencing accelerated degradation. coral reef monitoring activities are therefore required to assess the impact of adverse factors on corals and to track subsequent recovery or decline. the collection of image data has become a common approach in the field of underwater monitoring, but traditional coral image data analysis mainly has high time and labor costs. we need to investigate the spatial distribution of different coral populations in the study area through image segmentation methods to help oceanographers develop effective management and conservation strategies. in fact, deep learning has shown better prediction performance than traditional image processing or traditional machine learning algorithms in coral image segmentation tasks. starting from classification of random point annotations, segmentation of sparsely labeled data, and segmentation of densely labeled data, this paper summarizes state-of-the-art techniques of segmentation in deep learning applied to underwater images. then, we discuss the problems and challenges of cnn-based underwater image segmentation of coral reefs, and make corresponding solutions or possible directions for future.', 'computer vision and deep learning for fish classification in underwater habitats: a survey marine scientists use remote underwater image and video recording to survey fish species in their natural habitats. this helps them get a step closer towards understanding and predicting how fish respond to climate change, habitat degradation and fishing pressure. this information is essential for developing sustainable fisheries for human consumption, and for preserving the environment. however, the enormous volume of collected videos makes extracting useful information a daunting and time-consuming task for a human being. a promising method to address this problem is the cutting-edge deep learning (dl) technology. dl can help marine scientists parse large volumes of video promptly and efficiently, unlocking niche information that cannot be obtained using conventional manual monitoring methods. in this paper, we first provide a survey of computer visions (cvs) and dl studies conducted between 2003 and 2021 on fish classification in underwater habitats. we then give an overview of the key concepts of dl, while analysing and synthesizing dl studies. we also discuss the main challenges faced when developing dl for underwater image processing and propose approaches to address them. finally, we provide insights into the marine habitat monitoring research domain and shed light on what the future of dl for underwater image processing may hold. this paper aims to inform marine scientists who would like to gain a high-level understanding of essential dl concepts and survey state-of-the-art dl-based fish classification in their underwater habitat.']"
45,water_marine_env,8,16,45_shoreline_coastal_erosion_coastal erosion,"['shoreline', 'coastal', 'erosion', 'coastal erosion', 'accretion', 'sea level', 'sea', 'extraction', 'coast', 'level rise', 'image', 'rate', 'monitoring', 'coasts', 'landsat', 'seabed', 'images', 'rise', 'classification', 'sediment', 'point clouds', 'coastal areas', 'image classification', '3d', 'changes', 'remote sensing', 'satellite', 'detection', 'km', 'level']","['a vision-based intelligent system for coastal erosion monitoring global warming and climate change are inevitable global issues. it is predicted they will cause global sea level rise (slr) in incoming years, which may impacting the development of coastal zones, especially in the low lying areas: coastal areas are home to more than one billion people across the globe. the first visible signal of natural hazards is the erosion of the shoreline: erosion is a natural phenomenon due to a fluid substance that flows on a solid one and makes its crumbling; the combination of unstoppable events-upraising sea level-together with critical human activities can speedup this phenomenon.in the last years new software-based approaches in the erosion monitoring have been proposed, mainly based on satellite images, while a few number of them is based on rgb cameras. but a solution that is at the same time cheap, easy to be installed and used, and that does not require human intervention at runtime is not yet been proposed.in this work we propose a system for coastal erosion monitoring based on rgb cameras. this system is very cheap and easy to be installed and maintained, and allows to continuously check the monitored area. the system has been installed and tested in a marine protected area in the salento region, in the south of italy. preliminary results are encouraging and highlight the feasibility of a monitoring system that can be installed everywhere, at reasonable costs.', 'assessment of coastal variations due to climate change using remote sensing and machine learning techniques: a case study from west coast of india climate change and other environmental disturbances are causing sea level rise all over the world. due to sea-level rise and other unprecedented atmospheric phenomena caused by climate change, indian coasts are vulnerable to coastal erosion. the kerala coast, at the southern tip of indias west coast, has experienced a sea change in the last decade. to address coastal variations, we investigate coastal erosion, coastal accretion, and shoreline changes (from 2006 to 2020) along this coast between pozhiyoor and anchuthengu (58 km). to monitor the status and predict changes along the coast, remote sensing, gis, field checks, and machine learning tools were used. the data analysis reveals that the shoreline configuration, rate of beach accretion, and erosion have all changed significantly. when the rate of erosion in the 58-km-long coastal stretch was examined, it was discovered that approximately 42 km face acute erosion, 13 km face accretion, and approximately 3 km face neither accretion nor erosion and remain in equilibrium. according to the estimates, approximately 2.62 km2 of land has been eroded away from the shore over a 14-year period, while 0.7 km2 of land has been accreted. at locations where the influence of river discharge or groynes is minimal, the normal rate of accretion in the stretch is around 1–2 m/y. nonetheless, the rate of accretion increased to 5–8 m/y where groynes or river mouths, or both, have a significant influence on shoreline stability. the normal rate of erosion in the stretch under consideration, however, is around 5 m/y. with a rate of 10.59 m/y, pozhiyoor had the highest erosion rate. according to the data, the rate of erosion is faster between pozhikkara and veli, which includes the popular shanghumugham beach near thiruvananthapuram international airport. the increase in shoreline changes is primarily due to increased cyclone occurrences in the arabian sea, the formation of swell waves, changes in wave energy, and sea level rise due to climate change. the slow rate of sediment discharge by rivers, groyne construction, groyne spacing, groyne length, long shore currents, and preferential northward sediment transport all play a role in the formation and destruction of beaches along indias west coast. using machine learning techniques, a prediction model for the year 2027 was created using data from the previous 14 years (2006–2020). according to the model, almost the entire stretch will experience severe erosion, with the rate remaining consistently high between shanghumugham and anchuthengu. as a result, appropriate hybrid engineering solutions are required for coastal stability all along the beach, particularly at these two locations.', 'combining remote sensing analysis with machine learning to evaluate short-term coastal evolution trend in the shoreline of venice with increasing storminess and incessant sea-level rise, coastal erosion is becoming a primary issue along many littorals in the world. to cope with present and future climate change scenarios, it is important to map the shoreline position over years and assess the coastal erosion trends to select the best risk management solutions and guarantee a sustainable management of communities, structures, and ecosystems. however, this objective is particularly challenging on gentle-sloping sandy coasts, where also small sea-level changes trigger significant morphological evolutions. this study presents a multidisciplinary study combining satellite images with machine learning and gis-based spatial tools to analyze short-term shoreline evolution trends and detect erosion hot-spots on the venice coast over the period 2015–2019. firstly, advanced image preprocessing, which is not frequently adopted in coastal erosion studies, was performed on satellite images downloaded within the same tidal range. secondly, different machine learning classification methods were tested to accurately define shoreline position by recognizing the land-sea interface in each image. finally, the application of the digital shoreline analysis system tool was performed to evaluate and visualize coastal changes over the years. overall, the case study littoral reveals to be stable or mainly subjected to accretion. this is probably due to the high presence of coastal protection structures that stabilize the beaches, enhancing deposition processes. in detail, with respect to the total length of the considered shoreline (about 83 km), 5 % of the coast is eroding, 36 % is stable, 52 % is accreting and 7 % is not evaluable. despite a significant coastal erosion risk was not recognized within this region, well-delimited erosion hot-spots were mapped in correspondence of caorle, jesolo and cavallino-treporti municipalities. these areas deserve higher attention for territorial planning and prioritization of adaptation measures, facing climate change scenarios and sea-level rise emergencies in the context of integrated coastal zone management.']"
46,water_marine_env,8,34,46_seagrass_meadows_coastal_mapping,"['seagrass', 'meadows', 'coastal', 'mapping', 'extent', 'ecosystem', 'habitats', 'imagery', 'marine', 'habitat', 'islands', 'spectral', 'intertidal', 'ecosystems', 'sea', 'remote', 'blue', 'remote sensing', 'conservation', 'oil spills', 'mangroves', 'sensing', 'spills', 'classification', 'oil', 'resolution', 'monitoring', 'sentinel', 'disease', 'ad']","['superpixel for seagrass mapping: a novel method using planetscope imagery and machine learning in tauranga harbour, new zealand seagrass ecosystem provides valuable ecosystem services and is significant blue carbon sink. this resource, however, has been degraded across the globe with a loss rate of 7% year−1 to the end of twentieth century. the loss of seagrass meadows might lead to an unexpected emission of co2 into the atmosphere, aggravating global warming and resulting in potential damages to regional ecology and economies. accurate mapping of meadows extent in different coverages from remotely sensed data, therefore is in high demand as the first step in the strategy of monitoring, report, verification (mrv) that underpins large scale conservation of global seagrass. despite the higher accuracy of seagrass mapping in recent years, several challenges still persist, particularly when dealing with degraded, sparse seagrass meadows. in this research, we propose a novel and high accuracy approach for mapping dense and sparse meadows of the small size zostera muelleri seagrass, using high spatial resolution imagery (planetscope) at 3\xa0m spatial resolution, and advanced machine learning (ml) models for a ten-fold cross-validation superpixel-based classification in tauranga harbour, new zealand. we archive high mapping accuracy (overall accuracy = 0.913, kappa coefficient (κ) = 0.786, matthews correlation coefficient (mcc) = 0.796 and f1 = 0.908) using the lightgbm model from a set of superpixel image coupled with the bayesian optimization for hyper-parameter tuning. our proposed approach is solid and reliable with evidences of improving κ (10%) and mcc (11%) when compared with pixel-based image classification, and is expected to provide novel, effective techniques for quantifying the spatial distribution and area of seagrass ecosystem worldwide.', 'spatially explicit seagrass extent mapping across the entire mediterranean the seagrass posidonia oceanica is the main habitat-forming species of the coastal mediterranean, providing millennial-scale ecosystem services including habitat provisioning, biodiversity maintenance, food security, coastal protection, and carbon sequestration. meadows of this endemic seagrass species represent the largest carbon storage among seagrasses around the world, largely contributing to global blue carbon stocks. yet, the slow growth of this temperate species and the extreme projected temperature and sea-level rise due to climate change increase the risk of reduction and loss of these services. currently, there are knowledge gaps in its basin-wide spatially explicit extent and relevant accounting, therefore accurate and efficient mapping of its distribution and trajectories of change is needed. here, we leveraged contemporary advances in earth observation—cloud computing, open satellite data, and machine learning—with field observations through a cloud-native geoprocessing framework to account the spatially explicit ecosystem extent of p. oceanica seagrass across its full bioregional scale. employing 279,186 sentinel-2 satellite images between 2015 and 2019, and a human-labeled training dataset of 62,928 pixels, we mapped 19,020 km2 of p. oceanica meadows up to 25\xa0m of depth in 22 mediterranean countries, across a total seabed area of 56,783 km2. using 2,480 independent, field-based points, we observe an overall accuracy of 72%. we include and discuss global and region-specific seagrass blue carbon stocks using our bioregional seagrass extent estimate. as reference data collections, remote sensing technology and biophysical modelling improve and coalesce, such spatial ecosystem extent accounts could further support physical and monetary accounting of seagrass condition and ecosystem services, like blue carbon and coastal biodiversity. we envisage that effective policy uptake of these holistic seagrass accounts in national climate strategies and financing could accelerate transparent natural climate solutions and coastal resilience, far beyond the physical location of seagrass beds.', 'species level mapping of a seagrass bed using an unmanned aerial vehicle and deep learning technique background: seagrass beds are essential habitats in coastal ecosystems, providing valuable ecosystem services, but are threatened by various climate change and human activities. seagrass monitoring by remote sensing have been conducted over past decades using satellite and aerial images, which have low resolution to analyze changes in the composition of different seagrass species in the meadows. recently, unmanned aerial vehicles (uavs) have allowed us to obtain much higher resolution images, which is promising in observing fine-scale changes in seagrass species composition. furthermore, image processing techniques based on deep learning can be applied to the discrimination of seagrass species that were difficult based only on color variation. in this study, we conducted mapping of a multispecific seagrass bed in saroma-ko lagoon, hokkaido, japan, and compared the accuracy of the three discrimination methods of seagrass bed areas and species composition, i.e., pixel-based classification, object-based classification, and the application of deep neural network. methods: we set five benthic classes, two seagrass species (zostera marina and z. japonica), brown and green macroalgae, and no vegetation for creating a benthic cover map. high-resolution images by uav photography enabled us to produce a map at fine scales (<1 cm resolution). results: the application of a deep neural network successfully classified the two seagrass species. the accuracy of seagrass bed classification was the highest (82%) when the deep neural network was applied. conclusion: our results highlighted that a combination of uav mapping and deep learning could help monitor the spatial extent of seagrass beds and classify their species composition at very fine scales.']"
51,water_marine_env,8,43,51_coral_reef_marine_reefs,"['coral', 'reef', 'marine', 'reefs', 'habitat', 'ocean', 'species', 'sea', 'coastal', 'extinction', 'stressors', 'benthic', 'environmental', 'communities', 'richness', 'biodiversity', 'waters', 'pacific', 'atlantic', 'triangle', 'ecosystems', 'effects', 'ph', 'habitats', 'distribution', 'coral reefs', 'risk', 'temperature', 'oceans', 'genera']","['high-resolution modeling of thermal thresholds and environmental influences on coral bleaching for local and regional reef management coral reefs are one of the worlds most threatened ecosystems, with global and local stressors contributing to their decline. excessive sea-surface temperatures (ssts) can cause coral bleaching, resulting in coral death and decreases in coral cover. a sst threshold of 1°c over the climatological maximum is widely used to predict coral bleaching. in this study, we refined thermal indices predicting coral bleaching at high-spatial resolution (1 km) by statistically optimizing thermal thresholds, as well as considering other environmental influences on bleaching such as ultraviolet (uv) radiation, water turbidity, and cooling effects. we used a coral bleaching dataset derived from the web-based monitoring system sango map project, at scales appropriate for the local and regional conservation of japanese coral reefs. we recorded coral bleaching events in the years 2004-2016 in japan. we revealed the influence of multiple factors on the ability to predict coral bleaching, including selection of thermal indices, statistical optimization of thermal thresholds, quantification of multiple environmental influences, and use of multiple modeling methods (generalized linear models and random forests). after optimization, differences in predictive ability among thermal indices were negligible. thermal index, uv radiation, water turbidity, and cooling effects were important predictors of the occurrence of coral bleaching. predictions based on the best model revealed that coral reefs in japan have experienced recent and widespread bleaching. a practical method to reduce bleaching frequency by screening uv radiation was also demonstrated in this paper.', 'coral reef bleaching under climate change: prediction modeling and machine learning the coral reefs are important ecosystems to protect underwater life and coastal areas. it is also a natural attraction that attracts many tourists to eco-tourism under the sea. however, the impact of climate change has led to coral reef bleaching and elevated mortality rates. thus, this paper modeled and predicted coral reef bleaching under climate change by using machine learning techniques to provide the data to support coral reefs protection. supervised machine learning was used to predict the level of coral damage based on previous information, while unsupervised machine learning was applied to model the coral reef bleaching area and discovery knowledge of the relationship among bleaching factors. in supervised machine learning, three widely used algorithms were included: naïve bayes, support vector machine (svm), and decision tree. the accuracy of classifying coral reef bleaching under climate change was compared between these three models. unsupervised machine learning based on a clustering technique was used to group similar characteristics of coral reef bleaching. then, the correlation between bleaching conditions and characteristics was examined. we used a 5-year dataset obtained from the department of marine and coastal resources, thailand, during 2013–2018. the results showed that svm was the most effective classification model with 88.85% accuracy, followed by decision tree and naïve bayes that achieved 80.25% and 71.34% accuracy, respectively. in unsupervised machine learning, coral reef characteristics were clustered into six groups, and we found that seawater ph and sea surface temperature correlated with coral reef bleaching.', 'expediting the search for climate-resilient reef corals in the coral triangle with artificial intelligence featured application: we have developed a machine-learning approach for identifying climate-resilient corals in the solomon islands. numerous physical, chemical, and biological factors influence coral resilience in situ, yet current models aimed at forecasting coral health in response to climate change and other stressors tend to focus on temperature and coral abundance alone. to develop more robust predictions of reef coral resilience to environmental change, we trained an artificial intelligence (ai) with seawater quality, benthic survey, and molecular biomarker data from the model coral pocillopora acuta obtained during a research expedition to the solomon islands. this machine-learning (ml) approach resulted in neural network models with the capacity to robustly predict (r2 = ~0.85) a benchmark for coral stress susceptibility, the “coral health index,” from significantly cheaper, easier-to-measure environmental and ecological features alone. a gui derived from an ml desirability analysis was established to expedite the search for other climate-resilient pocilloporids within this coral triangle nation, and the ai specifically predicts that resilient pocilloporids are likely to be found on deeper fringing fore reefs in the eastern, more sparsely populated region of this under-studied nation. although small in geographic expanse, we nevertheless hope to promote this first attempt at building ai-driven predictive models of coral health that accommodate not only temperature and coral abundance, but also physiological data from the corals themselves.']"
52,water_marine_env,8,43,52_fish_species_fisheries_catch,"['fish', 'species', 'fisheries', 'catch', 'marine', 'fishing', 'pacific', 'abundance', 'fishery', 'migration', 'distribution', 'sea', 'japanese', 'habitat', 'species distribution', 'ocean', 'sea surface', 'transferability', 'fish species', 'recruitment', 'change', 'environmental', 'north pacific', 'north', 'climate change', 'drivers', 'sst', 'genomic', 'climate', 'spanish']","['impact of climate change on wintering ground of japanese anchovy (engraulis japonicus) using marine geospatial statistics the distribution and fluctuations in abundance of small pelagic species such as anchovy are largely affected by climate change. we hypothesized that the future projected rise in temperature will result to a northward shift of japanese anchovy (engraulis japonicus) habitat and a subsequent increase in relative abundance. to test this hypothesis, we explored the link between japanese anchovy abundance and environmental conditions using machine-learning and statistical models. the models were fitted with catch per unit effort (cpue) as the response variable and remotely sensed data of sea surface temperature (sst), sea surface chlorophyll-a (chl-a), assimilated information of sea surface salinity (sss), meridional and zonal ocean currents, and depth as environmental covariates. our results showed that the abundance of e. japonicus was significantly influenced by environmental factors. in particular, salinity front and sst highlight strong relationships with winter cpue distribution. based on these models, the results reinforced our hypothesis and showed that the warming ocean will drive a substantial shift in japanese anchovy habitat in the china seas. sst and cpue showed negative correlations with the el niño southern oscillation (enso) index. these findings underpin ramifications of the climate-driven habitat shift of small pelagic fish species on the regional marine ecosystem in the china seas.', 'bottom-up drivers for global fish catch assessed with reconstructed ocean biogeochemistry from an earth system model identifying bottom-up (e.g., physical and biogeochemical) drivers for fish catch is essential for sustainable fishing and successful adaptation to climate change through reliable prediction of future fisheries. previous studies have suggested the potential linkage of fish catch to bottom-up drivers such as ocean temperature or satellite-retrieved chlorophyll concentration across different global ecosystems. robust estimation of bottom-up effects on global fisheries is, however, still challenging due to the lack of long-term observations of fisheries-relevant biotic variables on a global scale. here, by using novel long-term biological and biogeochemical data reconstructed from a recently developed data assimilative earth system model, we newly identified dominant drivers for fish catch in globally distributed coastal ecosystems. a machine learning analysis with the inclusion of reconstructed zooplankton production and dissolved oxygen concentration into the fish catch predictors provides an extended view of the links between environmental forcing and fish catch. furthermore, the relative importance of each driver and their thresholds for high and low fish catch are analyzed, providing further insight into mechanistic principles of fish catch in individual coastal ecosystems. the results presented herein suggest the potential predictive use of their relationships and the need for continuous observational effort for global ocean biogeochemistry.', 'application of a fish habitat model considering mesoscale oceanographic features in evaluating climatic impact on distribution and abundance of pacific saury (cololabis saira) the continuing development of appropriate environmental predictors is important to improving the performance of fish habitat models. mesoscale oceanographic features (mofs) such as fronts, eddies, or upwelling zones, sometimes appearing as “oases in a fluid desert” occupying >50 km ocean surface, have customarily been regarded as key factors driving fish distributions. however, previous algorithms quantifying oceanographic features have not provided greatly enhanced predictive power to the fish habitat models which are characterized by a low variable importance due to inadequate attention having been paid to its scale-specific aspects. here, a new predictor representing mofs is introduced and a reappraisal by common fish habitat models of the role of the mofs in fish distribution within a case study on pacific saury (cololabis saira) in the northwestern pacific is reported. when the new predictor with mofs was introduced, the performances of three commonly used fish habitat models were all improved significantly (26%–30%), together with the paramount importance of mofs among all predictive variables, suggesting that mofs may exert significant effects on pacific saury distribution. using the optimal model (random forest) selected among the three, the habitat distribution of pacific saury in oyashio water was then reconstructed for the period of 1993–2020 to explore its spatio-temporal variations and its relationship with the abundance variation. the “suitable habitats” (areas of reconstructed catch > 12 tons) during major fishing seasons (august–november) estimated by the optimal habitat model for pacific saury showed a clear northward shift of 0.045°/year for the period of 1993–2020, associated with the poleward movement of both oceanic isotherms and the oyashio extension due to northward movement of the whole wind field in concert with global warming. the estimated annual suitable habitat area (sha) during the early fishing seasons (june–september) showed large inter-annual variations with a peak around 2010 and a valley in 2015, and correlated significantly with the abundance index of pacific saury, implying that the sha can be regarded as a practical indicator of abundance. moreover, it was also found that a marked decline in the sha was involved in the dramatic decrease in the abundance and catch of pacific saury after 2010, while eastward movements of sha resulted from mofs possibly aggravated the decrease in the catch of japanese waters by changing its southern migration route. this study illustrates the significance of mofs in predicting fish distribution by means of empirical habitat models and may provide new insights for understanding fish habitat variability in relation to physical-biological interactions in the ocean.']"
91,water_marine_env,8,30,91_ocean_subsurface_argo_sea,"['ocean', 'subsurface', 'argo', 'sea', 'sea surface', 'sss', 'salinity', 'surface', 'global', 'eddy', 'global ocean', 'satellite', 'oceanic', 'deep', 'neural', 'altimetry', 'anomaly', 'temperature', '1993', 'neural network', 'interior', 'nn', 'remote', 'remote sensing', 'bias', 'sensing', 'resolution', 'retrieval', 'mean', 'network']","['reconstructing ocean heat content for revisiting global ocean warming from remote sensing perspectives global ocean heat content (ohc) is generally estimated using gridded, model and reanalysis data; its change is crucial to understanding climate anomalies and ocean warming phenomena. however, argo gridded data have short temporal coverage (from 2005 to the present), inhibiting understanding of long-term ohc variabilities at decadal to multidecadal scales. in this study, we utilized multisource remote sensing and argo gridded data based on the long short-term memory (lstm) neural network method, which considers long temporal dependence to reconstruct a new long time-series ohc dataset (1993–2020) and fill the pre-argo data gaps. moreover, we adopted a new machine learning method, i.e., the light gradient boosting machine (lightgbm), and applied the well-known random forests (rfs) method for comparison. the model performance was measured using determination coefficients (r2) and root-mean-square error (rmse). the results showed that lstm can effectively improve the ohc prediction accuracy compared with the lightgbm and rfs methods, especially in long-term and deep-sea predictions. the lstm-estimated result also outperformed the ocean projection and extension neural network (open) dataset, with an r2 of 0.9590 and an rmse of 4.45 × 1019 in general in the upper 2000 m for 28 years (1993–2020). the new reconstructed dataset (named open-lstm) correlated reasonably well with other validated products, showing consistency with similar time-series trends and spatial patterns. the spatiotemporal error distribution between the open-lstm and iap datasets was smaller on the global scale, especially in the atlantic, southern and pacific oceans. the relative error for open-lstm was the smallest for all ocean basins compared with argo gridded data. the average global warming trends are 3.26 × 108 j/m2/decade for the pre-argo (1993–2004) period and 2.67 × 108 j/m2/decade for the time-series (1993–2020) period. this study demonstrates the advantages of lstm in the time-series reconstruction of ohc, and provides a new dataset for a deeper understanding of ocean and climate events.', 'subsurface temperature reconstruction for the global ocean from 1993 to 2020 using satellite observations and deep learning the reconstruction of the ocean’s 3d thermal structure is essential to the study of ocean interior processes and global climate change. satellite remote sensing technology can collect large-scale, high-resolution ocean observation data, but only at the surface layer. based on empirical statistical and artificial intelligence models, deep ocean remote sensing techniques allow us to retrieve and reconstruct the 3d ocean temperature structure by combining surface remote sensing observations with in situ float observations. this study proposed a new deep learning method, convolutional long short-term memory (convlstm) neural networks, which combines multisource remote sensing observations and argo gridded data to reconstruct and produce a new long-time-series global ocean subsurface temperature (st) dataset for the upper 2000 m from 1993 to 2020, which is named the deep ocean remote sensing (dors) product. the data-driven convlstm model can learn the spatiotemporal features of ocean observation data, significantly improves the model’s robustness and generalization ability, and outperforms the lighgbm model for the data reconstruction. the validation results show our dors dataset has high accuracy with an average r2 and rmse of 0.99/0.34 °c compared to the argo gridded dataset, and the average r2 and nrmse validated by the en4-profile dataset over the time series are 0.94/0.05 °c. furthermore, the st structure between dors and argo has good consistency in the 3d spatial morphology and distribution pattern, indicating that the dors dataset has high quality and strong reliability, and well fills the pre-argo data gaps. we effectively track the global ocean warming in the upper 2000 m from 1993 to 2020 based on the dors dataset, and we further examine and understand the spatial patterns, evolution trends, and vertical characteristics of global st changes. from 1993 to 2020, the average global ocean temperature warming trend is 0.063 °c/decade for the upper 2000 m. the 3d temperature trends revealed significant spatial heterogeneity across different ocean basins. since 2005, the warming signal has become more significant in the subsurface and deeper ocean. from a remote sensing standpoint, the dors product can provide new and robust data support for ocean interior process and climate change studies.', 'reconstruction of three-dimensional temperature and salinity fields from satellite observations observation of the ocean is crucial to the studies of ocean dynamics, climate change, and biogeochemical cycle. however, current oceanic observations are patently insufficient, because the in situ observations are of difficulty and high cost while the satellite remote-sensed measurements are mainly the sea surface data. to make up for the shortage of ocean interior data and make full use of the abundant satellite data, here we develop a data-driven deep learning model to estimate ocean subsurface and interior variables from satellite-observed sea surface data. exclusively and simply using satellite data, three-dimensional ocean temperature and salinity fields are successfully reconstructed, which are at 26 level depths from 0 to 2,000 m. we further design a scheme to increase the horizontal resolution from 1° to 1/4°, which is higher than the argo gridded data. estimations from our model are accurate, reliable, and stable for a wide range of research areas and periods. dynamic height fields that are derived from the estimated temperature and salinity, as well as the associated ocean geostrophic flows, are also calculated and analyzed, which indicates the potentials of our model for reconstructing the ocean circulation fields as well. this study enriches oceanic observations with respect to vertical dimension and horizontal resolution, which can largely make up for the paucity of the subsurface and deep ocean observation, both before and during argo era. this work also provides some new foundations for and insights into geoscience and climate change fields.']"
108,water_marine_env,8,19,108_wave_coastal_drilling_reef,"['wave', 'coastal', 'drilling', 'reef', 'shoreline', 'saltwater intrusion', 'saltwater', 'buoy', 'estuaries', 'intrusion', 'wave height', 'sea', 'coasts', 'coast', 'water levels', 's1', 'river discharge', 'water level', 'vulnerability', 'east coast', 'ml', 'ocean', 'numerical', 'altimeter', 'hs', 'discharge', 'level', 'lithology', 'height', 'km']","['synergistic multi-altimeter for estimating water level in the coastal zone of beibu gulf using sel, ales + and bfast algorithms accurately monitoring and predicting the large-scale dynamic changes of water levels in coastal zones is essential for its protection, restoration and sustainable development. however, there has been a challenge for achieving this goal using a single radar altimeter and retracking technique due to the diversity and complexity of coastal waveforms. to solve this issue, we proposed an approach of estimating water level of the coastal zone in beibu gulf, china, by combination of waveform classifications and multiple sub-waveform retrackers. this paper stacked random forest (rf), xgboost and catboost algorithms for building an ensemble learning (sel) model to classify coastal waveforms, and further evaluated the performance of three retracking strategies in refining waveforms using cryosat-2, saral, sentinel-3 altimeters. we compared the estimation accuracy of the coastal water levels between the single altimeter and synergistic multi-altimeter, and combined breaks for additive season and trend (bfast), mann-kendall mutation test (mk) with long short-term memory (lstm) algorithms to track the historical change process of coastal water levels, and predict its future development trend. this paper found that: (1) the sel algorithm achieved high-precision classification of different coastal waveforms with an average accuracy of 0.959, which outperformed three single machine learning algorithms. (2) combination of threshold retracker and ales+ retracker (tr_ales+) achieved the better retracking quality with an improvement of correlation coefficient (r, 0.089~0.475) and root mean square error (rmse, 0.008∼ 0.029 m) when comparing to the threshold retracker & primary peak cog retracker and threshold retracker & primary peak threshold retracker. (3) the coastal water levels of cryosat-2, saral, sentinel-3 and multi-altimeter were in good agreement (r>0.66, rmse<0.135m) with copernicus climate change service (c3s) water level. (4) the coastal water levels of the beibu gulf displayed a slowly rising trend from 2011 to 2021 with an average annual growth rate of 8mm/a, its lowest water level focused on may-august, the peak of water level was in october-november, and the average annual growth rate of water level from 2022-2031 was about 0.6mm/a. these results can provide guidance for scientific monitoring and sustainable management of coastal zones.', 'modeling surface wave dynamics in upper delaware bay with living shorelines living shorelines gain increasing attention because they stabilize shorelines and reduce erosion. this study leverages physics-based models and bagged regression tree (brt) machine learning algorithm to simulate wave dynamics at a living shoreline composed of constructed oyster reefs (cors) in upper delaware bay. the physics-based models consist of coupled delft3d-flow and swan in four-level nested domains. the model accuracy converges with increasing mesh resolution. the simulated wave-induced current circulation substantiates the effectiveness of cors in trapping sediments. the simulated yearly-averaged wave power correlates qualitatively with historical shoreline retreat rates. brt is adopted to improve the model accuracy, identify key processes responsible for simulation errors in wave height (hs) and wave period (tp), and quantify their importance. in the cors sheltered area, brt reveals that simulation errors of wind seas mainly arise from wind forcing, wave breaking and wave triad interactions. wave breaking is seven times more important than wind forcing for simulating hs, while wind forcing and triad interactions are of equal importance for simulating tp. simulation errors of swells mostly stem from bottom friction and offshore wave boundary conditions. results from this study can help the assessment and adaptive management of cors-based living shoreline restoration projects under climate change.', 'prediction of wave conditions using a machine learning framework on the east coast of korea kim, t. and lee, w.-d., 2023. prediction of wave conditions using a machine learning framework on the east coast of korea. journal of coastal research, 39(1), 143-153. charlotte (north carolina), issn 0749-0208. currently, technology for rapid and accurate wave information prediction is required because of the increase in human and property damage caused by high waves as a result of climate change on the east coast of korea. over the past few years, the volume of data produced has become large as the world enters the era of the fourth industrial revolution, and research on machine learning models that can use these data is actively being conducted. in this study, three machine learning models (xgboost, support vector regression, and linear regression) were used to make accurate estimates of waves on the east coast of korea, and the model suitable for the data was selected. the input data that were used to train the models included air barometric pressure, wind, and wave data collected using a deep-sea buoy (donghae), and the data were used to construct models for predicting offshore wave heights and wave periods. of the three models, xgboost exhibited a nash-sutcliffe efficiency (nse) of 0.89 and a root-mean-square error (rmse) of 19.7 cm for wave height and an nse of 0.81 and an rmse of 0.66 s for wave period. xgboost was selected as suitable for predicting offshore wave heights and wave periods at maengbang on the east coast of korea, and excellent and reliable consistency was observed between the observed and the predicted values. because machine learning models have short computation times and high accuracy, they can become an alternative to conventional physics-based wave models.']"
131,water_marine_env,8,31,131_sst_sea_sea surface_prediction,"['sst', 'sea', 'sea surface', 'prediction', 'iod', 'temperature', 'surface temperature', 'enso', 'surface', 'ocean', 'temperature prediction', 'water temperature', 'short', 'term memory', 'term', 'memory', 'short term', 'long short', 'predictability', 'marine', 'high water', 'long', 'lstm', 'convlstm', 'series', 'forecasting', 'time', 'forecast', 'time series', 'korean']","['time series prediction of sea surface temperature using lstm sea surface temperature (sst) is a key indicator of the global climate system. it is an essential factor in simulations of atmospheric models, weather predictions, and the study of marine ecosystems. of these interests, one of the most important is studying changes in sea surface temperatures that result from the anthropogenic forcing of climate, a process known as global warming. an accurate prediction of sea surface temperature is highly beneficial towards understanding climate change, preserving marine ecosystems, etc. this research focuses on processing sea surface temperature data to perform a time series forecasting using a unique long short-term memory neural architecture to predict sea surface temperature in bay of bengal region. given its reasonable high accuracy and low error rate, the assessment metrics show that the recommended neural network for sst prediction can be employed for real-time applications.', 'abnormal water temperature prediction model near the korean peninsula using lstm sea surface temperature (sst) is a factor that greatly influences ocean circulation and ecosystems in the earth system. as global warming causes changes in the sst near the korean peninsula, abnormal water temperature phenomena (high water temperature, low water temperature) occurs, causing continuous damage to the marine ecosystem and the fishery industry. therefore, this study proposes a methodology to predict the sst near the korean peninsula and prevent damage by predicting abnormal water temperature phenomena. the study area was set near the korean peninsula, and era5 data from the european center for medium-range weather forecasts (ecmwf) was used to utilize sst data at the same time period. as a research method, long short-term memory (lstm) algorithm specialized for time series data prediction among deep learning models was used in consideration of the time series characteristics of sst data. the prediction model predicts the sst near the korean peninsula after 1- to 7-days and predicts the high water temperature or low water temperature phenomenon. to evaluate the accuracy of sst prediction, coefficient of determination (r2), root mean squared error (rmse), and mean absolute percentage error (mape) indicators were used. the summer (jas) 1-day prediction result of the prediction model, r2=0.996, rmse=0.119°c, mape=0.352% and the winter (jfm) 1-day prediction result is r2=0.999, rmse=0.063°c, mape=0.646%. using the predicted sst, the accuracy of abnormal sea surface temperature prediction was evaluated with an f1 score (f1 score=0.98 for high water temperature prediction in summer (2021/08/05), f1 score=1.0 for low water temperature prediction in winter (2021/02/19)). as the prediction period increased, the prediction model showed a tendency to underestimate the sst, which also reduced the accuracy of the abnormal water temperature prediction. therefore, it is judged that it is necessary to analyze the cause of underestimation of the predictive model in the future and study to improve the prediction accuracy.', 'a dbulstm-adaboost model for sea surface temperature prediction sea surface temperature (sst) is an important parameter to measure the energy and heat balance of sea surface. the change of sea surface temperature has an important impact on the marine ecosystem, marine climate and marine environment. therefore, sea surface temperature prediction has become an significant research direction in the field of ocean. this article proposes a dbulstm-adaboost model based on ensemble learning. the model is composed of deep bidirectional and unidirectional long short term memory (dbulstm) and adaboost strong learner. dbulstm can capture the forward and backward dependence of time series, and the dbulstm model is integrated with adaboost strong learner to reduce the variance and bias of prediction and realize the short and medium term prediction of sst at a single point scale. experimental results show that the model can improve the accuracy and stability of sst prediction. experiments on the east china sea and south china sea with different prediction lengths show that the model is almost superior to other classical models in different sea areas and at different prediction levels. compared with full-connected lstm (fc-lstm) model, the root-meansquare error is reduced by about 0.1.']"
132,water_marine_env,8,29,132_sea level_sea_level_wave,"['sea level', 'sea', 'level', 'wave', 'storm', 'coastal', 'surge', 'rise', 'level rise', 'mean sea', 'wave energy', 'coastal areas', 'seawater', 'tide', 'sea levels', 'intrusion', 'numerical', 'neural', 'neural network', 'wave height', 'eemd', 'cnn', 'cyclones', 'deep', 'global sea', 'ocean', 'areas china', 'variability', 'reconstruction', 'height']","['predicting regional coastal sea level changes with machine learning all ocean basins have been experiencing significant warming and rising sea levels in recent decades. there are, however, important regional differences, resulting from distinct processes at different timescales (temperature-driven changes being a major contributor on multi-year timescales). in view of this complexity, it deems essential to move towards more sophisticated data-driven techniques as well as diagnostic and prognostic prediction models to interpret observations of ocean warming and sea level variations at local or regional sea basins. in this context, we present a machine learning approach that exploits key ocean temperature estimates (as proxies for the regional thermosteric sea level component) to model coastal sea level variability and associated uncertainty across a range of timescales (from months to several years). our findings also demonstrate the utility of machine learning to estimate the possible tendency of near-future regional sea levels. when compared to actual sea-level records, our models perform particularly well in the coastal areas most influenced by internal climate variability. yet, the models are widely applicable to evaluate the patterns of rising and falling sea levels across many places around the globe. thus, our approach is a promising tool to model and anticipate sea level changes in the coming (1–3) years, which is crucial for near-term decision making and strategic planning about coastal protection measures.', 'deep learning of sea-level variability and flood for coastal city resilience due to climate change, it is important to study the relationship between floods and sea-level rise in coastal city resilience. in this research sea surface temperature (sst) from modis, wind speed, precipitation, and sea-level rise from satellite altimetry are investigated for dynamic sea-level variability. an annual sst increase of 0.1c° is observed around the gothenburg coast. also in the middle of the north sea, an annual increase of about 0.2c° is evident. the annual sea surface height (ssh) trend is 3 mm on the gothenburg coast. we have a strong positive spatial correlation between sst and ssh near the gothenburg coast. in the next step, dynamic sea-level variability is predicted with a convolution neural network and long short term memory. root mean square error of wind speed, precipitation, sst, and mean sea-level forecasts are ±0.84 m/s, ±48.75 mm, ±3.48c° and ±24 mm, respectively. the 5-year trends of mean seal level show a significant increase from 28 mm/year to 46 mm/year in the last 5 year periods and the rate of increase has doubled. in the final step, the water rise of 5–10 m in gothenburg city was simulated, and in the worst scenario, more than 50 % of the city will be damaged.', 'comparative study of forecasting global mean sea level rising using machine learning over the last few decades, climate change has become a crucial challenge resulting in the continued burgeoning of the ocean and atmospheric warming, meaning sea levels will likely continue to rise at higher rates than in the present era. continued sea-level rises may very well lead to cataclysmic natural disasters on a global scale. the current overall local and global sea-level changes are being monitored using tide stations and satellite radar altimeters. however, these tools are not designed to predict a possible future scenario of sea-level rise. the purpose of this paper is to predict the most probable future global sea-level rise using advanced machine learning models. a total of 28 years worth of sea-level rise data has been utilized for training our models using various machine learning algorithms, e.g., linear regression, moving average, dense neural network (dnn), wavenet (a type of deep convolutional neural network).']"
2,climate_weather,9,26,2_cloud_dust_clouds_images,"['cloud', 'dust', 'clouds', 'images', 'deep', 'satellite', 'classification', 'deep learning', 'detection', 'cnn', 'image', 'event', 'convolutional', 'ground based', 'aerosol', 'classes', 'lidar', 'based cloud', 'atmospheric', 'network', 'optical', 'type', 'ground', 'neural', 'mobilenet', 'convolutional neural', 'types', 'water vapor', 'weather', 'pls']","['classification of ground-based cloud images by contrastive self-supervised learning clouds have an enormous influence on the hydrological cycle, earth’s radiation budget, and climate changes. accurate automatic recognition of cloud shape based on ground-based cloud images is beneficial to analyze the atmospheric motion state and water vapor content, and then to predict weather trends and identify severe weather processes. cloud type classification remains challenging due to the variable and diverse appearance of clouds. deep learning-based methods have improved the feature extraction ability and the accuracy of cloud type classification, but face the problem of lack of labeled samples. in this paper, we proposed a novel classification approach of ground-based cloud images based on contrastive self-supervised learning (cssl) to reduce the dependence on the number of labeled samples. first, data augmentation is applied to the input data to obtain augmented samples. then contrastive self-supervised learning is used to pre-train the deep model with a contrastive loss and a momentum update-based optimization. after pre-training, a supervised fine-tuning procedure is adopted on labeled data to classify ground-based cloud images. experimental results have confirmed the effectiveness of the proposed method. this study can provide inspiration and technical reference for the analysis and processing of other types of meteorological remote sensing data under the scenario of insufficient labeled samples.', 'a deep learning model for detecting dust in earths atmosphere from satellite remote sensing data in this paper we develop a deep learning model to distinguish dust from cloud and surface using satellite remote sensing image data. the occurrence of dust storms is increasing along with global climate change, especially in the arid and semi-arid regions. originated from the soil, dust acts as a type of aerosol that causes significant impacts on the environment and human health. the dust and cloud data labels used in this paper are from calipso (cloud-aerosol lidar and infrared pathfinder satellite observation) satellite. the radiometric channels and geometric parameters from viirs (visible infrared imaging radiometer suite) satellite sensor serve as features for our model. we trained and tested our deep learning model using 10,000 samples in march 2012. the developed model has five hidden layers and 512 neurons in each layer. the classification accuracy on the test set is 71.1%. in addition, we performed a shuffling procedure to identify the importance of features, which is calculated as the increase in the prediction error after we permute the features values. we also developed a method based on genetic algorithm to find the best subset of features for dust detection. the results show that the genetic algorithm can select a subset of features that have comparable performance as that of a model with all features. the shuffling procedure and the genetic algorithm both identify geometric information as important features for detecting mineral dust. the chosen subset will improve computational efficiency for dust detection and improve physical based methods.', 'cloud-mobinet: an abridged mobile-net convolutional neural network model for ground-based cloud classification more than 60 percent of the global surface is covered by clouds, and they play a vital role in the hydrological circle, climate change, and radiation budgets by modifying shortwaves and longwave. weather forecast reports are critical to areas such as air and sea transport, energy, agriculture, and the environment. the time has come for artificial intelligence-powered devices to take the place of the current method by which decision-making experts determine cloud types. convolutional neural network models (cnns) are starting to be utilized for identifying the types of clouds that are caused by meteorological occurrences. this study uses the publicly available cirrus cumulus stratus nimbus (ccsn) dataset, which consists of 2543 ground-based cloud images altogether. we propose a model called cloud-mobinet for the classification of ground-based clouds. the model is an abridged convolutional neural network based on mobilenet. the architecture of cloud-mobinet is divided into two blocks, namely the mobilenet building block and the support mobilenet block (sm block). the mobilenet building block consists of the weights of the depthwise separable convolutions and pointwise separable convolutions of the mobilenet model. the sm block is made up of three dense network layers for feature extraction. this makes the cloud-mobinet model very lightweight to be implemented on a smartphone. an overall accuracy success of 97.45% was obtained for the ccsn dataset used for cloud-type classification. cloud-mobinet promises to be a significant model in the short term, since automated ground-based cloud classification is anticipated to be a preferred means of cloud observation, not only in meteorological analysis and forecasting but also in the aeronautical and aviation industries.']"
23,climate_weather,9,52,23_soil moisture_moisture_sm_soil,"['soil moisture', 'moisture', 'sm', 'soil', 'ssm', 'surface', 'resolution', 'cci', 'm3', 'situ', 'spatial', 'satellite', 'products', 'lst', 'land surface', 'esa', 'surface soil', 'land', 'spatial resolution', 'temporal', 'temporal resolution', 'product', 'correlation', 'km', 'high', 'inversion', 'gap', 'root', 'precipitation', 'downscaling']","['generation of global 1km daily soil moisture product from 2000 to 2020 using ensemble learning motivated by the lack of long-term global soil moisture products with both high spatial and temporal resolutions, a global 1km daily spatiotemporally continuous soil moisture product (glass sm) was generated from 2000 to 2020 using an ensemble learning model (extreme gradient boosting - xgboost). the model was developed by integrating multiple datasets, including albedo, land surface temperature, and leaf area index products from the global land surface satellite (glass) product suite, as well as the european reanalysis (era5-land) soil moisture product, in situ soil moisture dataset from the international soil moisture network (ismn), and auxiliary datasets (multi-error-removed improved-terrain (merit) dem and global gridded soil information (soilgrids)). given the relatively large-scale differences between point-scale in situ measurements and other datasets, the triple collocation (tc) method was adopted to select the representative soil moisture stations and their measurements for creating the training samples. to fully evaluate the model performance, three validation strategies were explored: random, site independent, and year independent. results showed that although the xgboost model achieved the highest accuracy on the random test samples, it was clearly a result of model overfitting. meanwhile, training the model with representative stations selected by the tc method could considerably improve its performance for site- or year-independent test samples. the overall validation accuracy of the model trained using representative stations on the site-independent test samples, which was least likely to be overfitted, was a correlation coefficient (r) of 0.715 and root mean square error (rmse) of 0.079m3m-3. moreover, compared to the model developed without station filtering, the validation accuracies of the model trained with representative stations improved significantly for most stations, with the median r and unbiased rmse (ubrmse) of the model for each station increasing from 0.64 to 0.74 and decreasing from 0.055 to 0.052m3m-3, respectively. further validation of the glass sm product across four independent soil moisture networks revealed its ability to capture the temporal dynamics of measured soil moisture (r=0.69-0.89; ubrmse = 0.033-0.048m3m-3). lastly, the intercomparison between the glass sm product and two global microwave soil moisture datasets - the 1km soil moisture active passive/sentinel-1 l2 radiometer/radar soil moisture product and the european space agency climate change initiative combined soil moisture product at 0.25- indicated that the derived product maintained a more complete spatial coverage and exhibited high spatiotemporal consistency with those two soil moisture products. the annual average glass sm dataset from 2000 to 2020 can be freely downloaded from 10.5281/zenodo.7172664 (zhang et al., 2022a), and the complete product at daily scale is available at http://glass.umd.edu/soil_moisture/ (last access: 12 may 2023).', 'high-resolution quantitative retrieval of soil moisture based on multisource data fusion with random forests: a case study in the zoige region of the tibetan plateau accurate high-resolution soil moisture mapping is critical for surface studies as well as climate change research. currently, regional soil moisture retrieval primarily focuses on a spatial resolution of 1 km, which is not able to provide effective information for environmental science research and agricultural water resource management. in this study, we developed a quantitative retrieval framework for high-resolution (250 m) regional soil moisture inversion based on machine learning, multisource data fusion, and in situ measurement data. specifically, we used various data sources, including the normalized vegetation index, surface temperature, surface albedo, soil properties data, precipitation data, topographic data, and soil moisture products from passive microwave data assimilation as input parameters. the soil moisture products simulated based on ground model simulation were used as supplementary data of the in situ measurements, together with the measured data from the maqu observation network as the training target value. the study was conducted in the zoige region of the tibetan plateau during the nonfreezing period (may–october) from 2009 to 2018, using random forests for training. the random forest model had good accuracy, with a correlation coefficient of 0.885, a root mean square error of 0.024 m³/m³, and a bias of −0.004. the ground-measured soil moisture exhibited significant fluctuations, while the random forest prediction was more accurate and closely aligned with the field soil moisture compared to the soil moisture products based on ground model simulation. our method generated results that were smoother, more stable, and with less noise, providing a more detailed spatial pattern of soil moisture. based on the permutation importance method, we found that topographic factors such as slope and aspect, and soil properties such as silt and sand have significant impacts on soil moisture in the southeastern tibetan plateau. this highlights the importance of fine-scale topographic and soil property information for generating high-precision soil moisture data. from the perspective of inter-annual variation, the soil moisture in this area is generally high, showing a slow upward trend, with small spatial differences, and the annual average value fluctuates between 0.3741 m3/m3 and 0.3943 m3/m3. the intra-annual evolution indicates that the monthly mean average soil moisture has a large geographical variation and a small multi-year linear change rate. these findings can provide valuable insights and references for regional soil moisture research.', 'surface soil moisture retrieval of china using multi-source data and ensemble learning large-scale surface soil moisture (ssm) distribution is very necessary for agricultural drought monitoring, water resource management, and climate change research. however, the current large-scale ssm products have relatively coarse spatial resolution, which limits their application. in this study, we estimate the 1 km daily ssm in china based on ensemble learning using a multi-source data set including in situ soil moisture measurements from 2980 meteorological stations, modis surface reflectance products, smap (soil moisture active passive) soil moisture products, era5-land dataset, srtm dem and soil texture. among them, in situ measurements are used as independent variables, and other data are used as dependent variables. in order to improve the spatio-temporal completeness of ssm, the missing value in smap soil moisture products were reconstructed using the discrete cosine transformation-penalized partial least square (dct-pls) method to provide spatially complete background field information for soil moisture retrieval. the results show that the reconstructed soil moisture value has high quality, and the dct-pls method can fully utilize the three-dimensional spatiotemporal information to fill the data gaps. subsequently, the performance of four ensemble learning models of random forest (rf), extremely randomized trees (ert), extreme gradient boosting (xgboost), and light gradient boosting machine (lightgbm) for soil moisture retrieval was evaluated. the lightgbm outperformed the other three machine learning models, with a correlation coefficient (r2) of 0.88, a bias of 0.0004 m³/m³, and an unbiased root mean square error (ubrmse) of 0.0366 m³/m³. the high correlation between the in situ soil moisture and the predicted values at each meteorological station further indicate that lightgbm can well capture the temporal variation of soil moisture. finally, the model was used to map the 1 km daily ssm in china on the first day of each month from may to october 2018. this study can provide some reference and help for future long-term daily 1 km surface soil moisture mapping in china.']"
93,climate_weather,9,31,93_radiation_solar radiation_solar_diffuse,"['radiation', 'solar radiation', 'solar', 'diffuse', 'mj', 'cloud', 'wm', 'par', 'satellite', 'meteorological', 'sky', 'error', 'sst', 'estimation', 'rmse', 'global', 'earths', 'arima', 'mean', 'stations', 'china', 'albedo', 'surface', 'measurements', 'timescale', 'gp', 'himawari', 'input', 'daily', 'downward']","['improving solar radiation estimation in china based on regional optimal combination of meteorological factors with machine learning methods the values of global solar radiation are important fundamental data for potential evapotranspiration estimation, solar energy utilization, climate change study, crop growth model, and etc. this research tried to explore the optimal combination of input meteorological factors and the machine learning methods for the estimation of daily solar radiation under different climatic conditions so as to improve the estimation accuracy. based on the correlation between meteorological factors, different meteorological factor input combinations were established and the support vector machine method was used to estimate global solar radiation at 80 weather stations in four climatic regions of china mainland. the results showed that, the optimal combinations of input meteorological factors were different in the four different climatic zones in china mainland. three meteorological factors of sunshine hours, extraterrestrial radiation, and air temperature had greater impacts on the solar radiation estimation. adding the factor of precipitation could obviously improve the estimation accuracy in humid regions, but not remarkably in arid regions. wind speed had very little influence on solar radiation estimation. the accuracies of machine learning methods were better than the angstrom-prescott formula and the multiple linear regression method. among them, support vector machine and extreme learning machine were more appropriate. in some sites, the root mean square error of support vector machine method was even 20% less than that of the angstrom-prescott formula. in general, reasonable division of the areas and establishment of appropriate input combinations of meteorological factors according to the climatic conditions, combined with machine learning methods, can effectively improve the accuracy of solar radiation estimation.', 'constructing high-resolution (10 km) daily diffuse solar radiation dataset across china during 1982–2020 through ensemble model diffuse solar radiation is an essential component of surface solar radiation that contributes to carbon sequestration, photovoltaic power generation, and renewable energy production in terrestrial ecosystems. we constructed a 39-year (1982–2020) daily diffuse solar radiation dataset (chssdr), using era5 and merra_2 reanalysis data, with a spatial resolution of 10 km through a developed ensemble model (generalized additive models, gam). the validation results, with ground-based measurements, showed that gam had a high and stable performance with the correlation coefficient (r), root-mean-square error (rmse), and mean absolute error (mae) for the sample-based cross-validations of 0.88, 19.54 wm−2, and 14.87 wm−2, respectively. chssdr had the highest consistency with ground-based measurements among the four diffuse solar radiation products (ceres, era5, jiea, and chssdr), with the least deviation (mae = 15.06 wm−2 and rmse = 20.22 wm−2) and highest r value (0.87). the diffuse solar radiation values in china range from 59.13 to 104.65 wm−2, with a multi-year average value of 79.39 wm−2 from 1982 to 2020. generally, low latitude and low altitude regions have larger diffuse solar radiation than high latitude and high altitude regions, and eastern china has less diffuse solar radiation than western china. this dataset would be valuable for analyzing regional climate change, photovoltaic applications, and solar energy resources. the dataset is freely available from figshare.', 'potential of bayesian additive regression trees for predicting daily global and diffuse solar radiation in arid and humid areas this study aims to evaluate the potential of bayesian additive regression trees (bart) for predicting global and diffuse solar radiation. long-term daily weather data were collected at four stations in arid and humid areas. models with different input combinations were created. the default parameters within r language package of bart were used. model accuracy was assessed with pearson correlation coefficient (r), root mean square error (rmse), mean absolute error (mae), relative root mean square error (rrmse), and nash-sutcliffe efficiency coefficient (nse). taylor diagram was applied to illustrate model performance. on average, the model with sunshine duration, theoretical sunshine duration, mean temperature, maximum temperature, minimum temperature, relative humidity, and rainfall performed best for predicting global solar radiation, with mean r of 0.973, rmse of 1.685 mj/m2d, nse of 0.944, rrmse of 0.124, and mae of 1.265 mj/m2d. the model with sunshine duration, theoretical sunshine duration, global solar radiation, extraterrestrial solar radiation, and day of year outperformed others for predicting diffuse solar radiation, with mean r of 0.912, rmse of 1.291 mj/m2d, nse of 0.827, rrmse of 0.214, and mae of 0.933 mj/m2d. the results showed that bart was a suitable method for predicting global and diffuse solar radiation using climatic variables.']"
94,climate_weather,9,34,94_lst_temperature_land surface_surface,"['lst', 'temperature', 'land surface', 'surface', 'surface temperature', 'reconstruction', 'modis', 'resolution', 'products', 'air temperature', 'spatial', 'temporal', 'land', 'tp', 'surface air', 'ta', 'air', 'product', 'missing', 'temperature lst', 'km', 'high', 'mean', 'sky', 'reanalysis', 'datasets', 'rmse', 'lai', 'ts', 'satellite']","['sensitivity analysis of the training set to the performance of the machine learning-based land surface temperature reconstruction for cloud covered pixels land surface temperature (lst) represents integrated features of land atmosphere physical and dynamic processe, it is a key element in the fields of climate change, the land-atmosphere energy budget, and the global hydrological cycle, vegetation monitoring urban climate and environmental studies. thermal infrared remote sensing is an important technique for monitoring lst. however, the moderate resolution imaging spectroradiometer (modis) data are severely contaminated by cloud cover, which limits the applications of lst products. in recent years, the development of machine learning algorithms provides a promising technique for the reconstruction of lst under clouds. however, the accuracy of the cloud cover pixel reconstruction method based on machine learning is directly related to the number and regional distribution of training samples. in order to quantitatively evaluate the impact of the number and regional distribution of training samples on the lst reconstruction accuracy, based on modis land products and meteosat second generation (msg) incident short-wave radiation products, the lst reconstruction model depending on random forest method to construct an lst linking model for lsts and a range of influencing factors were fitted based on clear-sky observations, which was then applied to cloud-covered pixels to obtain an lst reconstruction, the proposed reconstruction model was applied to carry out the influence of different training samples on the reconstruction accuracy of lst. the results show that: (1) a visual comparison with daily lst observations from (msg) incident short-wave radiation products indicated that the lsts reconstructed using this method were representative of lst patterns resulting from the influence of key variables including solar radiation intensity, vegetation cover, and geographical factor. (2) the accuracy of lst reconstruction improves significantly with the increase of the amount of training sample data, and the reconstruction accuracy is also different in different seasons. when the amount of training data increases from 5% to 95%, there are seasonal differences between summer and autumn due to the differences in vegetation and solar radiation. the variation range of correlation coefficient and root mean square error in summer is larger than that in autumn. (3) the random sampling method has higher and stabler accuracy than the regional sampling method because of its spatial representativeness, which can reduce the root mean square error to less than 2.1 k and increase the correlation coefficient to more than 0.93. even if the amount of data is small, the reconstruction accuracy with the random sampling method is relatively stable, the negative effect of the insufficient number of training samples on the reduction of reconstruction accuracy is weakened. (4) the training sets were divided according to different elevations and vegetation coverage ranges to reconstruct the lst, and the results showed that the reconstruction accuracy was better when the range of the training sets included the range of the reconstruction area, that is, when the training set contains enough data features, it has a satisfactory spatial representation. the research results show that the proposed reconstruction model has a strong potential to reconstruct lsts under cloud-covered conditions, and can also accurately describe the spatial distributions of lst. it also can provide a reference for future machine learning methods to select appropriate training samples and reconstruct the lst with high accuracy.', 'a two-step deep learning framework for mapping gapless all-weather land surface temperature using thermal infrared and passive microwave data blending data from thermal infrared (tir) and passive microwave (pmw) measurements is a promising solution for generating the all-weather land surface temperature (lst). however, owing to swath gaps in pmw data and the resolution inconsistence between tir and pwm data, spatial details are often incomplete or considerable losses are generated in the all-weather lst using traditional methods. this study was conducted to develop a two-step deep learning framework (tdlf) for mapping gapless all-weather lst over the chinas landmass using modis and amsr-e lst data. in the tdlf, a multi-temporal feature connected convolutional neural network bidirectional reconstruction model was developed to obtain the spatially complete amsr-e lst. a multi-scale multi-temporal feature connected generative adversarial network model was then designed to blend spatially complete amsr-e lst and cloudy-sky modis lst, and generate gapless all-weather lst data. gapless all-weather lst data were evaluated using six in-situ lst data from the tibetan plateau (tp) and the heihe river basin (hrb). the root mean squared errors (rmses) of the gapless all-weather lst were 1.71–2.0 k with determination coefficients (r2) of 0.94–0.98 under clear conditions, and rmses of 3.41–3.87 k and r2 of 0.88–0.94 were obtained under cloudy conditions. compared to the existing pmw-based all-weather lsts, the validation accuracy and image quality (such as spatial detail) of the generated gapless all-weather lsts were superior. the tdlf does not require the use of any additional data and can potentially be implemented with other satellite tir and pwm sensors to produce long-term, gapless, all-weather modis lst records on a global scale. such a capability is beneficial for generating further gapless all-weather soil moisture and evapotranspiration datasets that can all be applied in global climate change research.', 'research on deep learning methods for amsr-e land surface temperature data reconstruction land surface temperature (lst) is a key parameter in the physical process of surface radiant energy balance and the water cycle. obtaining lst data accurately and promptly, and mastering its temporal and spatial changes, are of considerable importance to climate change research. thermal infrared (tir) measurements are limited in practical applications due to cloud cover and other effects. passive microwave (pmw) remote sensing measurements can penetrate clouds and are less affected by atmospheric interference, which has the advantage of obtaining all-weather surface radiation information. the microwave remote sensing data advanced microwave scanning radiometer for eos (amsr-e) can obtain all-weather lst, which can be used as a supplement to the missing lst information in thermal infrared (tir) products under cloudy conditions. however, the amsr-e data has the problem of lack of information due to the satellite orbit scanning gap of its own sensor, causing the obtained amsr-e lst data to be greatly restricted in practical applications. therefore, proposing an effective method for solving the problem is necessary.based on the superiority of deep learning in solving non-linear problems and the high dynamic variability of lst, this paper proposes a multi-temporal feature-connected convolutional neural network (mtfc-cnn) that uses specific input combinations of multi-temporal information and spatial fusion units. the network structure is based on the characteristics of the temporal and spatial distribution of missing track gaps in amsr-e lst data, and the reconstruction of missing lst values is carried out from the timing information.in the simulation experiment, the 2010 annual data was divided into fight data subsets in four seasons and into day and night. the average root mean square error of the reconstructed lst value is approximately 1.0 k and the coefficient of determination r2 is above 0.88. compared with the other two methods, namely, spline interpolation (spline) and time multiple linear regression (regress), the reconstruction effect of the mtfc-cnn method performs better regardless of seasons or day and night, proving that mtfc-cnn is better than the other two methods at mining the characteristics of temporal and spatial changes in lst. in real experiments, through comparison with modis lst products, the lst value reconstructed at the missing area is consistent with it at other areas in temporal and spatial distribution. the reconstruction results show that the lst in the mainland china region shows a gradually increasing trend from january to july and a gradually decreasing trend from august to december. this finding is consistent with the temperature changes in the four seasons. the change in lst during the day is more significant than that at night. in summer, the temperature in northwest china is significantly higher than that in other regions. in winter, the temperature in northeast china is generally lower than that in other regions. at night, the difference between summer and winter is more obvious. the difference in lst changes at night in autumn is relatively close.the experimental results show that the mtfc-cnn method proposed in this paper mines the spatio-temporal variation information of lsts more effectively than two traditional methods and achieves better results in reconstructing the orbital gap missing in amsr-e lst data. the method provides the possibility for the reconstruction of missing information from tir lst data under the cloud.']"
98,climate_weather,9,26,98_lake_lakes_water_water quality,"['lake', 'lakes', 'water', 'water quality', 'doc', 'freshwater', 'temperature', 'blooms', 'climate', 'aquatic', 'bayesian', 'summer', 'recreation', 'bayesian network', 'quality', 'water temperature', 'concentrations', 'pond', 'clarity', 'temperatures', 'algal', 'variability', 'near term', 'sediment', 'ecosystems', 'conditional', 'organic', 'change', 'recreational', 'dissolved']","['estimating future temperature maxima in lakes across the united states using a surrogate modeling approach a warming climate increases thermal inputs to lakes with potential implications for water quality and aquatic ecosystems. in a previous study, we used a dynamic water column temperature and mixing simulation model to simulate chronic (7-day average) maximum temperatures under a range of potential future climate projections at selected sites representative of different u.s. regions. here, to extend results to lakes where dynamic models have not been developed, we apply a novel machine learning approach that uses gaussian process regression to describe the model response surface as a function of simplified lake characteristics (depth, surface area, water clarity) and climate forcing (winter and summer air temperatures and potential evapotranspiration). we use this approach to extrapolate predictions from the simulation model to the statistical sample of u.s. lakes in the national lakes assessment (nla) database. results provide a national-scale scoping assessment of the potential thermal risk to lake water quality and ecosystems across the u.s. we suggest a small fraction of lakes will experience less risk of summer thermal stress events due to changes in stratification and mixing dynamics, but most will experience increases. the percentage of lakes in the nla with simulated 7-day average maximum water temperatures in excess of 30c is projected to increase from less than 2% to approximately 22% by the end of the 21st century, which could significantly reduce the number of lakes that can support cold water fisheries. site-specific analysis of the full range of factors that influence thermal profiles in individual lakes is needed to develop appropriate adaptation strategies.', 'phytoplankton and cyanobacteria abundances in mid-21st century lakes depend strongly on future land use and climate projections land use and climate change are anticipated to affect phytoplankton of lakes worldwide. the effects will depend on the magnitude of projected land use and climate changes and lake sensitivity to these factors. we used random forests fit with long-term (1971–2016) phytoplankton and cyanobacteria abundance time series, climate observations (1971–2016), and upstream catchment land use (global clumondo models for the year 2000) data from 14 european and 15\xa0north american lakes basins. we projected future phytoplankton and cyanobacteria abundance in the 29 focal lake basins and 1567\xa0lakes across focal regions based on three land use (sustainability, middle of the road, and regional rivalry) and two climate (rcp 2.6 and 8.5) scenarios to mid-21st century. on average, lakes are expected to have higher phytoplankton and cyanobacteria due to increases in both urban land use and temperature, and decreases in forest habitat. however, the relative importance of land use and climate effects varied substantially among regions and lakes. accounting for land use and climate changes in a combined way based on extensive data allowed us to identify urbanization as the major driver of phytoplankton development in lakes located in urban areas, and climate as major driver in lakes located in remote areas where past and future land use changes were minimal. for approximately one-third of the studied lakes, both drivers were relatively important. the results of this large scale study suggest the best approaches for mitigating the effects of human activity on lake phytoplankton and cyanobacteria will depend strongly on lake sensitivity to long-term change and the magnitude of projected land use and climate changes at a given location. our quantitative analyses suggest local management measures should focus on retaining nutrients in urban landscapes to prevent nutrient pollution from exacerbating ongoing changes to lake ecosystems from climate change.', 'delineating the relative contribution of climate related variables to chlorophyll-a and phytoplankton biomass in lakes using the era5-land climate reanalysis data understanding the climatic drivers of eutrophication is critical for lake management under the prism of the global change. yet the complex interplay between climatic variables and lake processes makes prediction of phytoplankton biomass a rather difficult task. quantifying the relative influence of climate-related variables on the regulation of phytoplankton biomass requires modelling approaches that use extensive field measurements paired with accurate meteorological observations. in this study we used climate and lake related variables obtained from the era5-land reanalysis dataset combined with a large dataset of in-situ measurements of chlorophyll-a and phytoplankton biomass from 50 water bodies to develop models of phytoplankton related responses as functions of the climate reanalysis data. we used chlorophyll-a and phytoplankton biomass as response metrics of phytoplankton growth and we employed two different modelling techniques, boosted regression trees (brt) and generalized additive models for location scale and shape (gamlss). according to our results, the fitted models had a relatively high explanatory power and predictive performance. boosted regression trees had a high pseudo r2 with the type of the lake, the total layer temperature, and the mix-layer depth being the three predictors with the higher relative influence. the best gamlss model retained mix-layer depth, mix-layer temperature, total layer temperature, total runoff and 10-m wind speed as significant predictors (p<0.001). regarding the phytoplankton biomass both modelling approaches had less explanatory power than those for chlorophyll-a. concerning the predictive performance of the models both the brt and gamlss models for chlorophyll-a outperformed those for phytoplankton biomass. overall, we consider these findings promising for future limnological studies as they bring forth new perspectives in modelling ecosystem responses to a wide range of climate and lake variables. as a concluding remark, climate reanalysis can be an extremely useful asset for lake research and management.']"
99,climate_weather,9,104,99_groundwater_water quality_quality_water,"['groundwater', 'water quality', 'quality', 'water', 'nitrate', 'pollution', 'river', 'concentrations', 'aquifer', 'potential', 'concentration', 'factors', 'nitrogen', 'watershed', 'wells', 'tn', 'parameters', 'resources', 'drinking', 'tp', 'machine', 'high', 'mg', 'used', 'machine learning', 'phosphorus', 'ml', 'svm', 'basin', 'monitoring']","['groundwater quality assessment by multi-model comparison: a comprehensive study during dry and wet periods in semi-arid regions with the impact of human engineering activities, groundwater pollution has seriously threatened the health of human life. accurate water quality assessment is the basis of controlling groundwater pollution and improving groundwater management, especially in specific regions. a typical semi-arid city in fuxin province of china is taken as an example. we use remote sensing and gis to compile four environmental factors, such as rainfall, temperature, lulc, and ndvi, to analyze and screen the correlation of indicators. the differences among the four algorithms were compared by using hyperparameters and model interpretability, including random forest (rf), support vector machine support vector machine (svm), decision tree (dt), and k-nearest neighbor (knn). the groundwater quality of the city during the dry and wet periods was comprehensively evaluated. the results show that the rf model has higher integrated precision (mse = 0.11, 0.035; rmse = 0.19,0.188; r2 = 0.829,0.811; roc = 0.98, 0.98). the quality of shallow groundwater is poor in general, 29%, 38%, 33% of the groundwater quality in low-water period is iii, iv, v water. thirty-three percent and 67% of the groundwater quality in the high-water period were iv and v water. the proportion of poor water quality in high-water period was higher than that in low-water period, which was consistent with the actual investigation. this study provides a kind of machine learning method for the semi-arid area, which cannot only promote the sustainable development of groundwater in this area, but also provide reference for the management policy of related departments.', 'groundwater quality analysis and drinkability prediction using artificial intelligence water quality strongly influences sustainable growth of a healthy society and green environment. according to the international initiative on water quality (iiwq) of the unesco intergovernmental hydrological programme (ihp), it is essential to address water-quality issues holistically in developed and developing countries. due to rapid urbanization and industrialization in many developing countries, groundwater - one of the major sources of drinking is getting highly affected. the traditional laboratory-based chemical testing process with conventional statistical methods is often used to analyze water quality. however, it is time-consuming. recently, artificial intelligence (ai) based approaches have proven to be a better alternative for analysis and prediction of the quality of water, provided with its chemical components’ data. in this paper, we present research focusing on groundwater quality analysis using artificial intelligence (ai) in a case study of odisha, an eastern- state of india and the data acquired from the northern delta, the north central coast of vietnam. the dataset in vietnam is collected by the ministry of natural resources and environment, providing technical regulations on water resources monitoring. the central groundwater board and the government of india collect the dataset from india. the target problem is formulated as a multi-class classification problem to predict groundwater quality for drinking suitability by who standards. ai methodologies such as logistic regression, k-nn, support vector machine (svm) variants, decision tree, adaboost and xgboost are used. prediction results have demonstrated that adaboost, the xgboost and the polynomial svm model accurately classified the water quality classes with an accuracy of 92% and 98%, respectively. it would help decision-makers effectively choose the best source of water for drinking.', 'enhancing waste management and prediction of water quality in the sustainable urban environment using optimized algorithm of least square support vector machine and deep learning techniques urban groundwater influences a wide range of processes in the natural world, including climatic, geological, geomorphic, biogeochemical, ecotoxicological, hydrological, and sanitary processes, supporting several ecological services. waste groundwater management refers to the activities and practices used to ensure groundwater resources sustainable use and protection. this can include monitoring and evaluating groundwater resources status and protecting groundwater from pollution and other forms of degradation. many research works have been implemented in managing groundwater. there need to be more parametric measurements available with the current technology to monitor water quality. groundwater management and monitoring water quality in the urban environment is an important task, as urbanization can lead to increased contamination of groundwater sources. one method for managing and monitoring groundwater quality is proposed using a least squares support vector machine (ls-svm) with a particle optimization algorithm. the ls-svm with pso algorithm i s used in groundwater management as a method for monitoring and evaluating the quality of groundwater resources. the ls-svm is a machine learning algorithm that uses the least squares approach to model complex data relationships. the pso algorithm is a particle optimization algorithm that optimizes the parameters of the ls-svm model. by combining these two techniques, the ls-svm with pso algorithm provides a more accurate prediction of groundwater quality compared to other algorithms such as knn and svm. the accuracy rate of various algorithms with groundwater pollution dataset with the algorithms of knn 75.32%, svm 81.78%, kcm 77.16%, and proposed work of lssvm-pso 92.73%.']"
125,climate_weather,9,73,125_drought_spi_droughts_index,"['drought', 'spi', 'droughts', 'index', 'precipitation', 'standardized precipitation', 'standardized', 'drought index', 'drought events', 'hydrological', 'precipitation index', 'meteorological drought', 'climate', 'rainfall', 'events', 'months', 'drought conditions', 'water', 'sydney', 'meteorological', 'drought monitoring', 'prediction', 'svr', 'agricultural', 'using', 'temperature', 'indices', 'index spi', 'extreme', 'forecasting']","['drought assessment and monitoring through blending of multi-sensor indices using machine learning approaches for different climate regions drought triggered by a deficit of precipitation, is influenced by various environmental factors such as temperature and evapotranspiration, and causes water shortage and crop failure problems. in this study, sixteen remote sensing based drought factors from the moderate resolution imaging spectroradiometer (modis) and tropical rainfall measuring mission (trmm) satellite sensors were used to monitor meteorological and agricultural drought during 2000-2012 growing seasons for different climate regions in the usa. standardized precipitation index (spi) with time scales from 1 to 12 months and crop yield data were used as reference data of meteorological and agricultural drought, respectively. the relationship between sixteen remote sensing based drought factors and in situ reference data was modeled through three machine learning approaches: random forest, boosted regression trees, and cubist, which have proved to be robust and flexible in many regression tasks. results showed that random forest produced the best performance (r2=0.93, rmse=0.3) for spi prediction among the three approaches. land surface-related drought factors, e.g., land surface temperature (lst) and evapotranspiration (et) showed higher relative importance for short-term meteorological drought while vegetation-related drought factors, e.g., normalized difference vegetation index (ndvi) and normalized multi-band drought index (nmdi) showed higher relative importance for long-term meteorological drought by random forest. six drought factors were selected based on the relative importance by their category to develop drought indicators that represent meteorological and agricultural drought by using the relative importance as weights. while trmm showed higher relative importance for meteorological drought, lst and ndvi showed higher relative importance for agricultural drought in the arid and humid regions, respectively. finally, drought distribution maps were produced using the drought indicators and compared with the u.s. drought monitor (usdm) maps, which showed a strong visual agreement.', 'toward a robust, impact-based, predictive drought metric this work presents a new approach to defining drought by establishing an empirical relationship between historical droughts (and wet spells) documented in impact reports, and a broad range of observed climate features using random forest (rf) models. the new drought indicator quantifies the conditional probability of drought, considering multiple drought-related climate features and their interactive effects, and can be used for forecasting with up to 3-month lead time. the approach was tested out-of-sample across several random selections of training and testing datasets, and demonstrated better predictive capabilities than commonly used drought indicators (e.g., standardised precipitation index and evaporative demand drought index) in a range of performance metrics. furthermore, it showed comparable performance to the (expert elicitation-based) us drought monitor (usdm), the current state-of-the-art record of historical drought in the usa. as well as providing an alternative historical drought indicator to usdm, the rf approach offers additional advantages by being automated, by providing drought information at the grid-scale, and by having forecasting capacity. while traditional drought metrics define drought as extreme anomalies in drought-related variables, the approach presented here reveals the full suite of circumstances that lead to impactful droughts. we highlight several combinations of climate features—such as precipitation, potential evapotranspiration, soil moisture and change in water storage—that led to drought events not detected by commonly used drought metrics. the new rf drought indicator combines meteorological, hydrological, agricultural, and socioeconomic drought, providing drought information for all impacted sectors. as a proof-of-concept, the rf drought indicator was trained on texan climate data and droughts.', 'assessing the joint impact of climatic variables on meteorological drought using machine learning with the intensification of climate change, the coupling effect between climate variables plays an important role in meteorological drought identification. however, little is known about the contribution of climate variables to drought development. this study constructed four scenarios using the random forest model during 1981–2016 in the luanhe river basin (lrb) and quantitatively revealed the contribution of climate variables (precipitation; temperature; wind speed; solar radiation; relative humidity; and evaporative demand) to drought indices and drought characteristics, that is, the standard precipitation evapotranspiration index (spei), standard precipitation index (spi), and evaporative demand drought index (eddi). the result showed that the r2 of the model is above 0.88, and the performance of the model is good. the coupling between climate variables can not only amplify drought characteristics but also lead to the spei, spi, and eddi showing different drought states when identifying drought. with the decrease in timescale, the drought intensity of the three drought indices became stronger and the drought duration shortened, but the drought frequency increased. for short-term drought (1 mon), four scenarios displayed that the spei and spi can identify more drought events. on the contrary, compared with the spei and spi, the eddi can identify long and serious drought events. this is mainly due to the coupling of evaporative demand, solar radiation, and wind speed. evaporation demand also contributed to the spei, but the contribution (6–13%) was much less than the eddi (45–85%). for spei-1, spei-3, and spei-6, the effect of temperature cannot be ignored. these results are helpful to understand and describe drought events for drought risk management under the condition of global warming.']"
128,climate_weather,9,77,128_rainfall_weather_prediction_forecasting,"['rainfall', 'weather', 'prediction', 'forecasting', 'temperature', 'rainfall prediction', 'machine', 'machine learning', 'precipitation', 'rain', 'regression', 'lightning', 'daily', 'meteorological', 'events', 'extreme', 'learning', 'weather forecasting', 'stations', 'mlp', 'monthly', 'using', 'predict', 'error', 'climate', 'forecast', 'series', 'random', 'used', 'random forest']","['ensemble learning for rainfall prediction climate change research is a discipline that analyses the varying weather patterns for a particular period of time. rainfall forecasting is the task of predicting particular future rainfall amount based on the measured information from the past, including wind, humidity, temperature, and so on. rainfall forecasting has recently been the subject of several machine learning (ml) techniques with differing degrees of both short-term and also long-term prediction performance. although several ml methods have been suggested to improve rainfall forecasting, the task of appropriate selection of technique for specific rainfall durations is still not clearly defined. therefore, this study proposes an ensemble learning to uplift the effectiveness of rainfall prediction. ensemble learning as an approach that combines multiple ml multiple rainfall prediction classifiers, which include naïve bayes, decision tree, support vector machine, random forest and neural network based on malaysian data. more specifically, this study explores three algebraic combiners: average probability, maximum probability, and majority voting. an analysis of our results shows that the fused ml classifiers based on majority voting are particularly effective in boosting the performance of rainfall prediction compared to individual classification.', 'automated predictive analytics tool\xa0for rainfall forecasting australia faces a dryness disaster whose impact may be mitigated by rainfall prediction. being an incredibly challenging task, yet accurate prediction of rainfall plays an enormous role in policy making, decision making and organizing sustainable water resource systems. the ability to accurately predict rainfall patterns empowers civilizations. though short-term rainfall predictions are provided by meteorological systems, long-term prediction of rainfall is challenging and has a lot of factors that lead to uncertainty. historically, various researchers have experimented with several machine learning techniques in rainfall prediction with given weather conditions. however, in places like australia where the climate is variable, finding the best method to model the complex rainfall process is a major challenge. the aim of this paper is to: (a) predict rainfall using machine learning algorithms and comparing the performance of different models. (b) develop an optimized neural network and develop a prediction model using the neural network (c) to do a comparative study of new and existing prediction techniques using australian rainfall data. in this paper, rainfall data collected over a span of ten years from 2007 to 2017, with the input from 26 geographically diverse locations have been used to develop the predictive models. the data was divided into training and testing sets for validation purposes. the results show that both traditional and neural network-based machine learning models can predict rainfall with more precision.', 'physical-empirical models for prediction of seasonal rainfall extremes of peninsular malaysia reliable prediction of rainfall extremes is vital for disaster management, particularly in the context of increasing rainfall extremes due to global climate change. physical-empirical models have been developed in this study using three widely used machine learning (ml) methods namely, support vector machines (svm), random forests (rf), bayesian artificial neural networks (bann) for the prediction of rainfall and rainfall related extremes during northeast monsoon (nem) in peninsular malaysia from synoptic predictors. the gridded daily rainfall data of asian precipitation—highly resolved observational data integration towards evaluation of water resources (aphrodite) was used to estimate four rainfall indices namely, rainfall amount, average rainfall intensity, days having >95-th percentile rainfall, and total number of dry days in peninsular malaysia during nem for the period 1951–2015. the national centers for environmental prediction (ncep) reanalysis sea level pressure (slp) data was used for the prediction of rainfall indices with different lead periods. the recursive feature elimination (rfe) method was used to select the slp at different ncep grid points which were found significantly correlated with nem rainfall indices. the results showed superior performance of bann among the ml models with normalised root mean square error of 0.04–0.14, nash-sutcliff efficiency of 0.98–1.0, and modified agreement index of 0.97–0.99 and kling-gupta efficient index 0.65–0.96 for one-month lead period prediction. the 95% confidence interval (ci) band for bann was found narrower than the other ml models. almost all the forecasted values by bann were also found with 95% ci, and therefore, the p-factor and the r-factor for bann in predicting rainfall indices were found in the range of 0.95–1.0 and 0.25–0.49 respectively. application of bann in prediction of rainfall indices with higher lead time was also found excellent. the synoptic pattern revealed that slp over the north of south china sea is the major driver of nem rainfall and rainfall extremes in peninsular malaysia.']"
129,climate_weather,9,32,129_eto_evapotranspiration_reference evapotranspiration_gep,"['eto', 'evapotranspiration', 'reference evapotranspiration', 'gep', 'reference', 'elm', 'evaporation', 'penman', 'mm', 'stations', 'tmin', 'daily', 'climatic', 'pan', 'china', 'tmax', 'water', 'paddy fields', 'svr', 'regression', 'temperature', 'ml', 'error', 'irrigation', 'estimation', 'different', 'woa', 'paddy', 'rcp', 'scenarios']","['monthly evapotranspiration estimation using optimal climatic parameters: efficacy of hybrid support vector regression integrated with whale optimization algorithm for effective planning of irrigation scheduling, water budgeting, crop simulation, and water resources management, the accurate estimation of reference evapotranspiration (eto) is essential. in the current study, the hybrid support vector regression (svr) coupled with whale optimization algorithm (svr-woa) was employed to estimate the monthly eto at algiers and tlemcen meteorological stations positioned in the north of algeria under three different optimal input scenarios. monthly climatic parameters, i.e., solar radiation (rs), wind speed (us), relative humidity (rh), and maximum and minimum air temperatures (tmax and tmin) of 14\xa0years (2000–2013), were obtained from both stations. the accuracy of the hybrid svr-woa model was appraised against hybrid svr-mvo (multi-verse optimizer), and svr-alo (ant lion optimizer) models through performance measures, i.e., mean absolute error (mae), root-mean-square error (rmse), index of scattering (ios), index of agreement (ioa), pearson correlation coefficient (pcc), nash-sutcliffe efficiency (nse), and graphical interpretation (time-variation and scatter plots, radar chart, and taylor diagram). the results showed that the svr-woa model performed superior to the svr-mvo and svr-alo models at both stations in all scenarios. the svr-woa-1 model with five inputs (i.e., tmin,tmax, rh, us, rs: scenario-1) had the lowest value of mae = 0.0658/0.0489\xa0mm/month, rmse = 0.0808/0.0617\xa0mm/month, ios = 0.0259/0.0165, and the highest value of nse = 0.9949/0.9989, pcc = 0.9975/0.9995, and ioa = 0.9987/0.9997 for testing period at both stations, respectively. the proposed hybrid svr-woa model was found to be more appropriate and efficient in comparison to svr-mvo and svr-alo models for estimating monthly eto in the study region.', 'modeling daily evapotranspiration in hyper-arid environment using gene expression programming accurate estimations of reference evapotranspiration (etref) are extremely important for maximizing the beneficial use of water and hydrologic applications, particularly in arid and semiarid regions where water sources are so limited. the aim of this study is to develop mathematical models to calculate the daily etref using a gene expression programming (gep) technique. eight gep models (gep-mod1–8) were developed from combinations of climatic variables. the penman-monteith equation was considered the reference method, with the reference plant height varying from 5 to 105\xa0cm in 5-cm increments. daily climatic variables collected from 13 meteorological stations, one station from every region within the kingdom of saudi arabia, covered the 1980 to 2010 period. of the available climatic data, 65\xa0% was used in the training process for the eight developed gep models, and 35\xa0% was used in the testing process. the accuracy of the eight developed gep models to estimate etref varied in significance depending on the climatic variables that were included. as more climatic parameters were input, the accuracy of the gep model increased. for the testing process, the coefficient of determination (r2) ranged from a low of 63.4\xa0% for gep-mod1 to a high of 95.4\xa0% for gep-mod8, and the root mean square error (rmse) values ranged from 3.19\xa0mm\xa0day−1 for gep-mod1 to 1.14\xa0mm\xa0day−1 for gep-mod8. from the spatial evaluation, the values of rmse ranged from 3.27\xa0mm\xa0day−1 for gep-mod1 to 1.21\xa0mm\xa0day−1 for gep-mod8. in addition, the respective rmse values resulting from gep-mod8 for plant heights of 50 and 12\xa0cm were 0.75 and 0.96\xa0cm. this implies that the developed gep-mod8 can be used for any value of the reference plant height ranging from 5 to 105\xa0cm with insignificant errors. interestingly, solar radiation had an almost insignificant effect on etref in the hyper-arid conditions. in contrast, wind speed and plant height had a large positive effect on increasing the accuracy of calculating etref.', 'generalized reference evapotranspiration models with limited climatic data based on random forest and gene expression programming in guangxi, china accurate estimation of reference evapotranspiration (et0) is very important in hydrological cycle research, and is essential in agricultural water management and allocation. the application of the standard model (fao-56 penman-monteith) to estimate et0 is restricted due to the absence of required meteorological data. although many machine learning algorithms have been applied in modeling et0 with fewer meteorological variables, most of the models are trained and tested using data from the same station, their performances outside the training station are not evaluated. this study aims to investigate generalization ability of the random forest (rf) algorithm in modeling et0 with different input combinations (refer to different circumstances in missing data), and compares this algorithm with the gene-expression programming (gep) method using the data from 24 weather stations in a karst region of southwest china. the et0 estimated by the fao-56 penman-monteith model was used as a reference to evaluate the derived rf-based and gep-based models, and the coefficient of determination (r2), nash-sutcliffe coefficiency of efficiency (nsce), root of mean squared error (rmse), and percent bias (pbias) were used as evaluation criteria. the results revealed that the derived rf-based generalization et0 models are successfully applied in modeling et0 with complete and incomplete meteorological variables (r2, nsce, rmse and pbias ranged from 0.637 to 0.987, 0.626 to 0.986, 0.107 to 0.563 mm day−1, and −2.916% to 1.571%, respectively), and seven rf-based models corresponding to different incomplete data circumstances are proposed. the gep-based generalization et0 models are also proposed, and they produced promising results (r2, nsce, rmse and pbias ranged from 0.639 to 0.944, 0.636 to 0.942, 0.222 to 0.555 mm day−1, and −1.98% to 0.248%, respectively). although the rf-based et0 models performed slightly better than the gep-based models, the gep approach has the ability to give explicit expressions between the dependent and independent variables, which is more convenient for irrigators with minimal computer skills. therefore, we recommend applying the rf-based models in water balance research, and the gep-based models in agricultural irrigation practice. moreover, the models performance decreased with periods due to climate change impact on et0. at last, both of the two methods have the ability to assess the importance of predictors, the order of the importance of meteorological variables on et0 in guangxi is: sunshine duration, air temperature, relative humidity, and wind speed.']"
130,climate_weather,9,61,130_downscaling_precipitation_rainfall_gcms,"['downscaling', 'precipitation', 'rainfall', 'gcms', 'temperature', 'statistical downscaling', 'gcm', 'climate', 'statistical', 'projection', 'future', 'projections', 'daily', 'ensemble', 'uncertainty', 'circulation', 'cmip6', 'pet', 'monsoon', 'general circulation', 'change', 'observed', 'trend', 'climate change', 'regions', 'stations', 'period', 'scenarios', 'extremes', 'rcp']","['new approach to multisite downscaling of precipitation by identifying different set of atmospheric predictor variables estimating reliable projections of precipitation considering climate change scenarios is important for hydrological studies. general circulation models provide future climate simulations at large scale in terms of large-scale atmospheric variables (lsavs). those lsavs can be downscaled to finer special resolution using several downscaling approaches. this paper presents a support vector regression (svr)-based downscaling approach to downscale rainfall at several locations in a study area. because the rainfall generation mechanisms cannot be the same for all the sites in a study area, conventional multisite downscaling approaches that assume the same rainfall generation mechanism should not be used. therefore, a new downscaling approach is proposed that (1) divides the study area in several climatological regions, and (2) develops different downscaling models for each of the climatological regions to obtain future projections of rainfall. the new approach was implemented on rainfall data obtained for republic of ireland to demonstrate the effectiveness of the approach compared with existing approaches. future projections of rainfall were obtained for the period 2012-2050 corresponding to four representative concentration pathway climate change scenarios. the performance of the svr approach was compared with that of relevance vector machine-and deep learning-based downscaling approaches.', 'statistical downscaling of climate change scenarios of rainfall and temperature over indira sagar canal command area in madhya pradesh, india general circulation models (gcms) have been employed by climate agencies to predict future climate change. a challenging issue with gcm output for local relevance is their coarse spatial resolution of the projected variables. statistical downscaling model (sdsm) identifies relationships between large-scale predictors (i.e., gcm-based) and local-scale predictands using multiple linear regression models. in this study (sdsm) was applied to downscale rainfall and temperature from gcms. the data from single station located in the indira sagar canal command area at madhya pradesh, india were used as input of the sdsm. the study included calibration and validation with large-scale atmospheric variables encompassing the ncep reanalysis data, the future estimation due to a climate scenario, which is hadcm3 a2. results of the downscaling experiment demonstrate that during the calibration and validation stages, the sdsm model can be well acceptable regard its performance in the downscaling of daily rainfall and temperature. for a future period (2010-2099), the sdsm model estimated an increase in total average annual rainfall and annual average temperature for station. this indicates that the area of station considered will be wet and humid in the future. also, the mean temperature is projected to rise to 1.5 c to 2.5 c for present study area. however, the model projections show a rise in mean daily precipitation with varying percentage in the months of july (0.59% to 2.09%) and august (0.79% to 1.19) under a2 of hadcm3 model for future periods.', 'multi-site downscaling of maximum and minimum daily temperature using support vector machine climate change impact assessment studies involve downscaling large-scale atmospheric predictor variables (lsapvs) simulated by general circulation models (gcms) to site-scale meteorological variables. this article presents a least-square support vector machine (ls-svm)-based methodology for multi-site downscaling of maximum and minimum daily temperature series. the methodology involves (1) delineation of sites in the study area into clusters based on correlation structure of predictands, (2) downscaling lsapvs to monthly time series of predictands at a representative site identified in each of the clusters, (3) translation of the downscaled information in each cluster from the representative site to that at other sites using ls-svm inter-site regression relationships, and (4) disaggregation of the information at each site from monthly to daily time scale using k-nearest neighbour disaggregation methodology. effectiveness of the methodology is demonstrated by application to data pertaining to four sites in the catchment of beas river basin, india. simulations of canadian coupled global climate model (cgcm3.1/t63) for four ipcc sres scenarios namely a1b, a2, b1 and commit were downscaled to future projections of the predictands in the study area. comparison of results with those based on recently proposed multivariate multiple linear regression (mmlr) based downscaling method and multi-site multivariate statistical downscaling (mmsd) method indicate that the proposed method is promising and it can be considered as a feasible choice in statistical downscaling studies. the performance of the method in downscaling daily minimum temperature was found to be better when compared with that in downscaling daily maximum temperature. results indicate an increase in annual average maximum and minimum temperatures at all the sites for a1b, a2 and b1 scenarios. the projected increment is high for a2 scenario, and it is followed by that for a1b, b1 and commit scenarios. projections, in general, indicated an increase in mean monthly maximum and minimum temperatures during january to february and october to december. © 2013 royal meteorological society.']"
133,climate_weather,9,42,133_groundwater_groundwater level_grace_water,"['groundwater', 'groundwater level', 'grace', 'water', 'groundwater levels', 'level', 'twsa', 'wells', 'groundwater storage', 'downscaling', 'storage', 'groundwater resources', 'resolution', 'recharge', 'ml', 'period', 'climate grace', 'gravity recovery', 'hydrological', 'recovery climate', 'levels', 'resources', 'climate', 'depletion', 'machine', 'gravity', 'km', 'machine learning', 'modeling', 'rf']","['assessment of hellwig method for predictors’ selection in groundwater level time series forecasting effective groundwater planning and management should be based on the prediction of available water volume. the complex nature of groundwater systems makes this complicated and requires the use of complex methods. data-driven models using computational intelligence are becoming increasingly popular in that field. the key issue in predictive modelling is the selection of input variables. wrocław-osobowice irrigation fields were a wastewater treatment plant until 2013. the monitoring of groundwater levels is being continued to assess the water relations in that area after the end of their exploitation. the aim of the study was to assess the hellwig method for predictors’ selection in groundwater level forecasting with support vector regression models. data covered the daily time series of groundwater level in the period 2015–2019. obtained models with a root mean squared error (rmse) of 0.024–0.292 m and r2 of 0.7–0.9 were considered as high quality. moreover, they showed good prediction ability for high as well as low groundwater values. additionally, the proposed method is simple, and its implementation only requires access to groundwater level measurement data. it may be useful in groundwater management and planning in terms of actual climate change and threat of water deficits.', 'a machine learning approach to predict groundwater levels in california reveals ecosystems at risk groundwater dependent ecosystems (gdes) are increasingly threatened worldwide, but the shallow groundwater resources that they are reliant upon are seldom monitored. in this study, we used satellite-based remote sensing to predict groundwater levels under groundwater dependent ecosystems across california, usa. depth to groundwater was modelled for a 35-years period (1985–2019) within all groundwater dependent ecosystems across the state (n = 95,135). our model was developed within google earth engine using landsat satellite imagery, climate data, and field-based groundwater data [n = 627 shallow (< 30\xa0m) monitoring wells] as predictors in a random forest model. our findings show that 1) 44% of groundwater dependent ecosystems have experienced a significant long-term (1985–2019) decline in groundwater levels compared to 28% with a significant increase; 2) groundwater level declines have intensified during the most recent two\xa0decades, with 39% of groundwater dependent ecosystems experiencing declines in the 2003–2019 period compared to 27% in the 1985–2002 period; and 3) groundwater declines are most prevalent within gdes existing in areas of the state where sustainable groundwater management is absent. our results indicate that declining shallow groundwater levels may be adversely impacting california’s groundwater dependent ecosystems. particularly where groundwater levels have fallen beneath plant roots or streams thereby affecting key life processes, such as forest recruitment/succession, or hydrological processes, such as streamflow that affects aquatic habitat. in the absence of groundwater monitoring well data, our model and findings can be used to help state and local water agencies fill in data gaps of shallow groundwater conditions, evaluate potential effects on gdes, and improve sustainable groundwater management policy in california.', 'machine learning based downscaling of grace-estimated groundwater in central valley, california californias central valley, one of the most agriculturally productive regions, is also one of the most stressed aquifers in the world due to anthropogenic groundwater over-extraction primarily for irrigation. groundwater depletion is further exacerbated by climate-driven droughts. gravity recovery and climate experiment (grace) satellite gravimetry has demonstrated the feasibility of quantifying global groundwater storage changes at uniform monthly sampling, though at a coarse resolution and is thus impractical for effective water resources management. here, we employ the random forest machine learning algorithm to establish empirical relationships between grace-derived groundwater storage and in situ groundwater level variations over the central valley during 2002–2016 and achieved spatial downscaling of grace-observed groundwater storage changes from a few hundred km to 5 km. validations of our modeled groundwater level with in situ groundwater level indicate excellent nash-sutcliffe efficiency coefficients ranging from 0.94 to 0.97. in addition, the secular components of modeled groundwater show good agreements with those of vertical displacements observed by gps, and cryosat-2 radar altimetry measurements and is perfectly consistent with findings from previous studies. our estimated groundwater loss is about 30 km3 from 2002 to 2016, which also agrees well with previous studies in central valley. we find the maximum groundwater storage loss rates of −5.7 ± 1.2 km3 yr−1 and -9.8 ± 1.7 km3 yr−1 occurred during the extended drought periods of january 2007–december 2009, and october 2011–september 2015, respectively while central valley also experienced groundwater recharges during prolonged flood episodes. the 5-km resolution central valley-wide groundwater storage trends reveal that groundwater depletion occurs mostly in southern san joaquin valley collocated with severe land subsidence due to aquifer compaction from excessive groundwater over withdrawal.']"
134,climate_weather,9,18,134_evapotranspiration_land_plantation_surface,"['evapotranspiration', 'land', 'plantation', 'surface', 'water', 'drought', 'flux', 'surface energy', 'vegetation', 'december', 'cover types', 'hydrological', 'budget', 'global land', 'carbon budget', 'balance', 'fluxes', 'global', 'land surface', 'energy balance', 'ecosystem', 'precipitation extremes', 'managed', 'meteorological', 'drained', 'heat', 'equations', 'precipitation', 'products', 'eddy covariance']","['ensemble machine learning outperforms empirical equations for the ground heat flux estimation with remote sensing data estimating evapotranspiration at the field scale is a major component of sustainable water management. due to the difficulty to assess some major unknowns of the water cycle at that scale, including irrigation amounts, evapotranspiration is often computed as the residual of the instantaneous surface energy budget. one of the surface energy balance components with the largest un-certainties in their quantification over bare soils and sparse vegetation areas is the ground heat flux (g). over the last decades, the estimation of g with remote sensing (rs) data has been mainly achieved with empirical equations, on the basis of the g and net radiation (rn) ratio, g/rn. the g/rn empirical equations generally require vegetation data (type i empirical equations), in combi-nation with surface temperature (ts) and albedo (type ii empirical equations). in this article, we aim to evaluate the estimation of g with rs data. here, we compared eight g/rn empirical equations against two types of machine learning (ml) methods: an ensemble ml type, the random forest (rf), and the neural networks (nn). the comparison of each method was evaluated using a wide range of climate and land cover datasets, including data from eddy-covariance towers that extend along the mid-latitude areas that encompass the european and african continents. our results have shown evidence that the driver of g in bare soils and sparse vegetation areas (fraction of vegetation, fv ≤ 0.25) is ts, instead of vegetation greenness indexes. on the other hand, the accuracy in the estimation of g with rn, ts or fv decreases in densely vegetated areas (fv ≥ 0.50). there are no significant differences between the most accurate type i and ii empirical equations. for bare soils and sparse vegetation areas the empirical equation which combines the leaf area index (lai) and ts (e7) estimates g best. in densely vegetated areas, an exponential empirical equation based on fv (e4), shows the best performance. however, ml better estimates g than the empirical equa-tions, independently of the fv ranges. an rf model with rn, lai and ts as predictor variables shows the best accuracy and performance metrics, outperforming the nn model.', 'recent decline in the global land evapotranspiration trend due to limited moisture supply more than half of the solar energy absorbed by land surfaces is currently used to evaporate water. climate change is expected to intensify the hydrological cycle and to alter evapotranspiration, with implications for ecosystem services and feedback to regional and global climate. evapotranspiration changes may already be under way, but direct observational constraints are lacking at the global scale. until such evidence is available, changes in the water cycle on land-a key diagnostic criterion of the effects of climate change and variability-remain uncertain. here we provide a data-driven estimate of global land evapotranspiration from 1982 to 2008, compiled using a global monitoring network, meteorological and remote-sensing observations, and a machine-learning algorithm. in addition, we have assessed evapotranspiration variations over the same time period using an ensemble of process-based land-surface models. our results suggest that global annual evapotranspiration increased on average by 7.1 ± 1.0 millimetres per year per decade from 1982 to 1997. after that, coincident with the last major el ni±o event in 1998, the global evapotranspiration increase seems to have ceased until 2008. this change was driven primarily by moisture limitation in the southern hemisphere, particularly africa and australia. in these regions, microwave satellite observations indicate that soil moisture decreased from 1998 to 2008. hence, increasing soil-moisture limitations on evapotranspiration largely explain the recent decline of the global land-evapotranspiration trend. whether the changing behaviour of evapotranspiration is representative of natural climate variability or reflects a more permanent reorganization of the land water cycle is a key question for earth system science. © 2010 macmillan publishers limited. all rights reserved.', 'evapotranspiration in the tono reservoir catchment in upper east region of ghana estimated by a novel tseb approach from aster imagery evapotranspiration (et) is dynamic and influences water resource distribution. sustainable management of water resources requires accurate estimations of the individual components that result in evapotranspiration, including the daily net radiation (dnr). daily et is more useful than the evaporative fraction (ef) provided by remote sensing et models, and to account for daily variations, ef is usually combined with the dnr. dnr exhibits diurnal and spatiotemporal variations due to landscape heterogeneity. in the modified two-source energy balance (tseb) approach by zhuang and wu, 2015, ecophysiological constraint functions of temperature and moisture of plants based on atmospheric moisture and vegetation indices were introduced, but the dnr was not spatially accounted for in the estimation of the daily et. this research adopted a novel approach that accounts for spatiotemporal variations in estimated daily et by incorporating the bisht and bras dnr model in the modified version of the tseb model. advanced spaceborne thermal emission and reflection radiometer (aster) satellite imagery over the tono irrigation watershed within the upper east region of ghana and southern burkina faso were used. we estimated the energy fluxes of latent and sensible heat as well as the net radiation and soil heat fluxes from the satellite images and compared our results with ground-based measurements from an eddy covariance (ec) station established by the west african science service center on climate change and adapted land use (wascal) within the watershed area. we noticed a similarity between our model estimated fluxes and et with the ground-based ec station measurements. eight different land use/cover types were identified in the study area, and each of these contributed significantly to the overall et variations between the two study periods: december 2009 and december 2017. for instance, due to a higher leaf area index (lai) for all vegetation types in december 2009 than in december 2017, the et for december 2017 was higher than that for december 2009. we also noticed that the land use/cover types within the footprint area of the ec station were only six out of the eight. generally, all the surface energy fluxes increased from december 2009 to december 2017. mean et varied from 3.576 to 4.486 (mm/d) for december 2009 while from 4.502 to 5.280 (mm/d) for december 2017 across the different land use/cover classes. knowledge of the dynamics of evapotranspiration and adoption of cost-effective methods to estimate its individual components in an effective and efficient way is critical to water resources management. our findings provide a tool for all water stakeholders within watersheds to manage water resources in an engaging and cost-effective way.']"
135,climate_weather,9,29,135_lstm_rainfall_forecasting_memory,"['lstm', 'rainfall', 'forecasting', 'memory', 'long', 'term memory', 'long short', 'weather', 'meteorological', 'short term', 'prediction', 'short', 'climatic', 'weather forecasting', 'term', 'eto', 'memory lstm', 'forecast', 'humidity', 'soil erosion', 'neural', 'deep', 'deep learning', 'time', 'atmospheric', 'rbf', 'precipitation', 'hybrid', 'forecasts', 'erosion']","['a long short term memory implemented for rainfall forecasting the prediction and its accuracy of the rainfall is needed due to it would be affected to the various areas of life, such as feasibility aircraft departures and, in general issue, is climate change. this paper aimed to apply a long short term memory (lstm) approach to get accurate rainfall forecasting. also, the lstm accuracy would be compared to bpnn (backpropagation neural network) algorithm. in this research, lstm architecture used a hidden layer of 200, a maximum epoch of 250, 1 gradient threshold, and learning rates of 0.005, 0.007, and 0.009. then, standardize data was used gamma ? of 1.05. then, the bpnn architectures of [2-50-10-1, epoch 250] have been explored. the accuracy performance is measured by the root means square error (rmse). the experimental results showed that the lstm had produced a good accuracy than bpnn, with the value of rmse was 0.2367 and 0.1938. it means that the forecast accuracy of the lstm approach outperformed the bpnn to predict the rainfall. this finding would be useful for the climatology station to develop a forecsat rainfall application-based artificial intelligence.', 'implementing a novel deep learning technique for rainfall forecasting via climatic variables: an approach via hierarchical clustering analysis variations in rainfall negatively affect crop productivity and impose severe climatic conditions in developing regions. studies that focus on climatic variations such as variability in rainfall and temperature are vital, particularly in predominant rainfed areas. forecasting rainfall is very essential in the agriculture sector due to the dependence of many people, while it is very complex to accurately predict rainfall due to its dynamic nature. this study aims to present a deep forecasting model based on optimized (gated recurrent unit) gru neural network to predict rainfall in pakistan based on the 30 years of climate data from 1991 to 2020. the climatic variables were first extracted and then fine-tuned by eliminating outliers and extreme values from the data set for precise forecasting. data normalization strategies were further utilized to adjust numeric values into a standard scale without distorting divergences or losing useful information. the proposed model achieved high prediction accuracy by maintaining minimal normalized mean absolute error (nmae) and normalized root mean squared error (nrmse) compared to state-of-the-art rainfall forecasting models. climatic variables used in the forecasting were evaluated in terms of correlation and regression analysis. the correlation results showed that temperature has a negative association and air quality variables have a positive association with rainfall in each quarter of the year. the second and third quarters of the year showed a high association with rainfall, whereas the air quality variables showed a lesser or no association with rainfall during the first and second quarters of the year. the results further showed a strong association of climatic variables with rainfall for all months of the year. the minimal loss achieved by the proposed model also demonstrated the feasibility of selected variables in precise forecasting of rainfall regardless of volatile climatic conditions.', 'dwfh: an improved data-driven deep weather forecasting hybrid model using transductive long short term memory (t-lstm) forecasting climate and the development of the environment have been essential in recent days since there has been a drastic change in nature. weather forecasting plays a significant role in decision-making in traffic management, tourism planning, crop cultivation in agriculture, and warning the people nearby the seaside about the climate situation. it is used to reduce accidents and congestion, mainly based on climate conditions such as rainfall, air condition, and other environmental factors. accurate weather prediction models are required by meteorological scientists. the previous studies have shown complexity in terms of model building, and computation, and based on theory-driven and rely on time and space. this drawback can be easily solved using the machine learning technique with the time series data. this paper proposes the state-of-art deep learning model long short-term memory (lstm) and the transductive long short-term memory (t-lstm) model. the model is evaluated using the evaluation metrics root mean squared error, loss, and mean absolute error. the experiments are carried out on hhwd and jena climate datasets. the dataset comprises 14 weather forecasting features including humidity, temperature, etc. the t-lstm method performs better than other methodologies, producing 98.2% accuracy in forecasting the weather. this proposed hybrid t-lstm method provides a robust solution for the hydrological variables.']"
136,climate_weather,9,107,136_lstm_runoff_deep_deep learning,"['lstm', 'runoff', 'deep', 'deep learning', 'rainfall', 'flood', 'streamflow', 'water', 'river', 'term memory', 'memory', 'short term', 'term', 'long short', 'short', 'long', 'forecasting', 'hydrological', 'prediction', 'memory lstm', 'flow', 'learning', 'time', 'input', 'water resources', 'precipitation', 'dl', 'neural', 'network', 'hydrologic']","['the discharge forecasting of multiple monitoring station for humber river by hybrid lstm models an early warning flood forecasting system that uses machine-learning models can be utilized for saving lives from floods, which are now exacerbated due to climate change. flood forecasting is carried out by determining the river discharge and water level using hydrologic models at the target sites. if the water level and discharge are forecasted to reach dangerous levels, the flood forecasting system sends warning messages to residents in flood-prone areas. in the past, hybrid long short-term memory (lstm) models have been successfully used for the time series forecasting. however, the prediction errors grow exponentially with the forecasting period, making the forecast unreliable as an early warning tool with enough lead time. therefore, this research aimed to improve the accuracy of flood forecasting models by employing real-time monitoring network datasets and establishing temporal and spatial links between adjacent monitoring stations. we evaluated the performance of the long short-term memory (lstm), the convolutional neural networks lstm (cnn-lstm), the convolutional lstm (convlstm), and the spatio-temporal attention lstm (sta-lstm) models for flood forecasting. the dataset, employed for validation, includes hourly discharge records, from 2012 to 2017, on six stations of the humber river in the city of toronto, canada. experiments included forecasting for both 6 and 12 h ahead, using discharge data as input for the past 24 h. the sta-lstm model’s performance was superior to the cnn-lstm, the convlstm, and the basic lstm models when the forecast time was longer than 6 h.', 'a study on the derivation and evaluation of flow duration curve (fdc) using deep learning with a long short-term memory (lstm) networks and soil water assessment tool (swat) climate change brought on by global warming increased the frequency of flood and drought on the korean peninsula, along with the casualties and physical damage resulting therefrom. preparation and response to these water disasters requires national-level planning for water resource management. in addition, watershed-level management of water resources requires flow duration curves (fdc) derived from continuous data based on long-term observations. traditionally, in water resource studies, physical rainfall-runoff models are widely used to generate duration curves. however, a number of recent studies explored the use of data-based deep learning techniques for runoff prediction. physical models produce hydraulically and hydrologically reliable results. however, these models require a high level of understanding and may also take longer to operate. on the other hand, data-based deep-learning techniques offer the benefit if less input data requirement and shorter operation time. however, the relationship between input and output data is processed in a black box, making it impossible to consider hydraulic and hydrological characteristics. this study chose one from each category. for the physical model, this study calculated long-term data without missing data using parameter calibration of the soil water assessment tool (swat), a physical model tested for its applicability in korea and other countries. the data was used as training data for the long short-term memory (lstm) data-based deep learning technique. an anlysis of the time-series data fond that, during the calibration period (2017-18), the nash-sutcliffe efficiency (nse) and the determinanation coefficient for fit comparison were high at 0.04 and 0.03, respectively, indicating that the swat results are superior to the lstm results. in addition, the annual time-series data from the models were sorted in the descending order, and the resulting flow duration curves were compared with the duration curves based on the observed flow, and the nse for the swat and the lstm models were 0.95 and 0.91, respectively, and the determination coefficients were 0.96 and 0.92, respectively. the findings indicate that both models yield good performance. even though the lstm requires improved simulation accuracy in the low flow sections, the lstm appears to be widely applicable to calculating flow duration curves for large basins that require longer time for model development and operation due to vast data input, and non-measured basins with insufficient input data.', 'prediction of yangtze river streamflow based on deep learning neural network with el niño–southern oscillation accurate long-term streamflow and flood forecasting have always been an important research direction in hydrology research. nowadays, climate change, floods, and other anomalies occurring more and more frequently and bringing great losses to society. the prediction of streamflow, especially flood prediction, is important for disaster prevention. current hydrological models based on physical mechanisms can give accurate predictions of streamflow, but the effective prediction period is only about 1 month in advance, which is too short for decision making. the artificial neural network (ann) has great potential for predicting runoff and is not only good at handling non-linear data but can also make long-period forecasts. however, most of ann models are unstable in their predictions when faced with raw flow data, and have excessive errors in predicting extreme flows. previous studies have shown a link between the el niño–southern oscillation (enso) and the streamflow of the yangtze river. in this paper, we use enso and the monthly streamflow data of the yangtze river from 1952 to 2018 to predict the monthly streamflow of the yangtze river in two extreme flood years and a small flood year by using deep neural networks. in this paper, three deep neural network frameworks are used: stacked long short-term memory, conv long short-term memory encoder–decoder long short-term memory and conv long short-term memory encoder–decoder gate recurrent unit. the results show that the use of convlstm improves the stability of the model and increases the accuracy of the flood prediction. besides, the introduction of enso to the experimental data resulted in a more accurate prediction of the time of the occurrence of flood peaks and flood flows. furthermore, the best results were obtained on the convolutional long short-term memory + encoder–decoder gate recurrent unit model.']"
137,climate_weather,9,80,137_climate_cyclones_nam_variability,"['climate', 'cyclones', 'nam', 'variability', 'patterns', 'dynamical', 'temperature', 'machine learning', 'change', 'circulation', 'machine', 'simulations', 'understanding', 'atmospheric', 'climate change', 'weather', 'learning', 'global', 'surface', 'time', 'events', 'earth', 'projections', 'predictions', 'warming', 'decadal', 'internal variability', 'internal', 'numerical', 'forced']","['viewing forced climate patterns through an ai lens many problems in climate science require extracting forced signals from a background of internal climate variability. we demonstrate that artificial neural networks (anns) are a useful addition to the climate science “toolbox” for this purpose. specifically, forced patterns are detected by an ann trained on climate model simulations under historical and future climate scenarios. by identifying spatial patterns that serve as indicators of change in surface temperature and precipitation, the ann can determine the approximate year from which the simulations came without first explicitly separating the forced signal from the noise of both internal climate variability and model uncertainty. thus, the ann indicator patterns are complex, nonlinear combinations of signal and noise and are identified from the 1960s onward in simulated and observed surface temperature maps. this approach suggests that viewing climate patterns through an artificial intelligence (ai) lens has the power to uncover new insights into climate variability and change.', 'applying machine learning to improve simulations of a chaotic dynamical system using empirical error correction dynamical weather and climate prediction models underpin many studies of the earth system and hold the promise of being able to make robust projections of future climate change based on physical laws. however, simulations from these models still show many differences compared with observations. machine learning has been applied to solve certain prediction problems with great success, and recently, it has been proposed that this could replace the role of physically-derived dynamical weather and climate models to give better quality simulations. here, instead, a framework using machine learning together with physically-derived models is tested, in which it is learnt how to correct the errors of the latter from time step to time step. this maintains the physical understanding built into the models, while allowing performance improvements, and also requires much simpler algorithms and less training data. this is tested in the context of simulating the chaotic lorenz 96 system, and it is shown that the approach yields models that are stable and that give both improved skill in initialized predictions and better long-term climate statistics. improvements in long-term statistics are smaller than for single time step tendencies, however, indicating that it would be valuable to develop methods that target improvements on longer time scales. future strategies for the development of this approach and possible applications to making progress on important scientific problems are discussed.', 'detecting climate signals using explainable ai with single-forcing large ensembles it remains difficult to disentangle the relative influences of aerosols and greenhouse gases on regional surface temperature trends in the context of global climate change. to address this issue, we use a new collection of initial-condition large ensembles from the community earth system model version 1 that are prescribed with different combinations of industrial aerosol and greenhouse gas forcing. to compare the climate response to these external forcings, we adopt an artificial neural network (ann) architecture from previous work that predicts the year by training on maps of near-surface temperature. we then utilize layer-wise relevance propagation (lrp) to visualize the regional temperature signals that are important for the anns prediction in each climate model experiment. to mask noise when extracting only the most robust climate patterns from lrp, we introduce a simple uncertainty metric that can be adopted to other explainable artificial intelligence (ai) problems. we find that the north atlantic, southern ocean, and southeast asia are key regions of importance for the neural network to make its prediction, especially prior to the early-21st century. notably, we also find that the ann predictions based on maps of observations correlate higher to the actual year after training on the large ensemble experiment with industrial aerosols held fixed to 1920 levels. this work illustrates the sensitivity of regional temperature signals to changes in aerosol forcing in historical simulations. by using explainable ai methods, we have the opportunity to improve our understanding of (non)linear combinations of anthropogenic forcings in state-of-the-art global climate models.']"
138,climate_weather,9,52,138_runoff_lake_water_hydrological,"['runoff', 'lake', 'water', 'hydrological', 'river', 'basin', 'water storage', 'lakes', 'changes', 'twsa', 'catchment', 'storage', 'grace', 'tws', 'precipitation', 'watershed', 'sediment', 'climate', 'streamflow', 'human activities', 'river basin', 'change', 'scale', 'water resources', 'china', 'activities', 'water balance', 'drought', 'lake water', 'flow']","['forecasting monthly fluctuations of lake surface areas using remote sensing techniques and novel machine learning methods drought is one of the most environmentally impactful hydrologic processes with devastating economic consequences for many rural communities in arid and semi-arid countries all over the world. in this research, we have employed satellite data and a stochastic approach for forecasting the changes in lake surface areas and demonstrated for the application of the new technique for the case study of the lake gregory in australia. high-resolution landsat satellite images are used on a monthly time scale from landsat 5, 7, and 8, on days that are not cloudy. the software envi 5.3, using normalized difference vegetation index (ndvi), and modify normalized difference water index (mndwi) indices were employed to obtain the lake surface maps, and satellite images have been split into water and non-water using a decision tree. the arcgis 10.3 software was used to calculate the area of the lake monthly. the overall trend data shows that from 2004 to 2019, the ls is steadily declining, reaching its lowest area in 2019.the trmm satellite monthly precipitation (p) and temperature (t) measurement were obtained to investigate the correlation between these changes and regional precipitation. we developed a novel generalized group method of data handling (ggmdh) to forecast lake surface (ls) fluctuations, in which the ls time-series database is extracted from the satellite imagery. for downscaling, precipitation and three different scenarios are defined based on climate change projections to forecast the ls in the 2020–2060 period. the comparison of the ggmdh with stochastic models integrated with preprocessing scenarios indicates the ggmdh in long-term ls forecasting outperforms the stochastic model. the result showed ggmdh is the best model among other ones to modeling lake surface by r2 (%) = 94.16, rmse = 8.77 for the forecasting stage. the forecasted surface of the lake gregory fluctuated from 226 to 0.008\xa0km2 in the future.', 'conceptual hydrological model-guided svr approach for monthly lake level reconstruction in the tibetan plateau study region: tibetan plateau (tp) study focus: lakes in the tp that are subject to low human activity serve as an important indicator for quantitative assessment of regional climate change. however, previous studies have mainly focused on annual changes in lake area, level, and storage because of limited monitoring stations, and long-term lake level reconstruction at monthly resolution remains challenging. we propose a conceptual hydrological model-guided monthly lake level reconstruction (wbm-svr) approach that incorporates water balance models (wbms) and support vector regression (svr), to improve the training and testing sets compared with svr alone through consideration of the hydrological process. the wbm-svr approach integrates wbms to select input factors, sub-process control equations to quantify the contributions of input factors, empirical parameters to characterize catchment uniqueness, and svr for water level modelling. new hydrological insights for the region: physically guided wbm-svr is more accurate than svr in reconstructing monthly lake water levels. wbms can quantify the hydrological process in the lake catchment area with efficient quasi-physical mechanisms and refine the input–output factors within lake hydrometeorology. the reconstructability, generalization capability, and transferability of wbm-svr were validated for three different types of lakes (glacier-free inflow lake, glacier-free outflow lake, and glacier-fed inflow lake), and the reconstruction results indicate significant improvements in wbm-svr compared with svr. the wbm-svr approach shows great promise for achieving monthly lake level reconstruction.', 'century-scale reconstruction of water storage changes of the largest lake in the inner mongolia plateau using a machine learning approach lake hulun is the fifth-largest lake in china, playing a substantial role in maintaining the balance of the grassland ecosystem of the mongolia plateau, which is a crucial ecological barrier in north china. to better understand the changing characteristics of lake hulun and the driving mechanisms, it is necessary to investigate the water storage changes of lake hulun on extended timescales. the main objective of this study is to reconstruct the water storage time series of lake hulun over the past century. we employed a machine learning approach termed the extreme gradient boosting tree (xgboost) to reconstruct the water storage changes over a one-century timescale based on the generated bathymetry and satellite altimetry data and investigated the relationships with hydrological and climatic variables in long term. results show that the water storage changes from 1961 to 2019 were featured by four fluctuation phases, with the highest water storage observed in 1991 (14.02\xa0gt) and the lowest point in 2012 (5.18\xa0gt). the century-scale reconstruction result reveals that the water storage of lake hulun reached the highest point in the 1960s within the period of 1910–2019. the lowest stage occurred in the sub-period of the 1930s–1940s, which was even lower than the alerted shrinkage stage in 2012. the predictive model results indicate the effective performance of the xgboost model in reconstructing century-scale water storage variations, with the mean absolute error of 0.68, normalized root mean square error of 0.11, nash–sutcliffe efficiency of 0.97, and correlation coefficient of 0.94. the annual fluctuations of water storage were mostly affected by precipitation, followed by vapor pressure, temperature, potential evapotranspiration, and wet day frequency. the dominating characteristics of different variables vary evidently with different sub-periods. the atmospheric circulations of the arctic oscillation, el nino southern oscillation, pacific decadal oscillation, and north atlantic oscillation have tight associations with the water storage variations of lake hulun, which change with different study periods.']"
139,climate_weather,9,124,139_streamflow_river_water_reservoir,"['streamflow', 'river', 'water', 'reservoir', 'runoff', 'hydrological', 'flow', 'hydropower', 'inflow', 'basins', 'dam', 'elm', 'monthly', 'precipitation', 'climate', 'temperature', 'discharge', 'future', 'basin', 'machine', 'change', 'anfis', 'water temperature', 'climate change', 'forecasting', 'machine learning', 'ensemble', 'period', 'scenarios', 'rainfall']","['application of integrated artificial neural networks based on decomposition methods to predict streamflow at upper indus basin, pakistan consistent streamflow forecasts play a fundamental part in flood risk mitigation. population increase and water cycle intensification are extending not only globally but also among pakistans water resources. the frequency of floods has increased in the last few decades in the country, which emphasizes the importance of efficient practices needed to adopt for various aspects of water resource management such as reservoir scheduling, water sustainability, and water supply. the purpose of this study is to develop a novel hybrid model for streamflow forecasting and validate its efficiency at the upper indus basin (uib), pakistan. maximum streamflow in the river indus from its upper mountain basin results from melting snow or glaciers and climatic unevenness of both precipitation and temperature inputs, which will, therefore, affect rural livelihoods at both a local and a regional scale through effects on runoff in the upper indus basin (uib). this indicates that basins receive the bulk of snowfall input to sustain the glacier system. the present study will help find the runoff from high altitude catchments and estimated flood occurrence for the proposed and constructed hydropower projects of the upper indus basin (uib). due to climate variability, the upper indus basin (uib) was further divided into three zone named as sub-zones, zone one (z1), zone two (z2), and zone three (z3). the hybrid models are designed by incorporating artificial intelligence (ai) models, which includes feedforward backpropagation (ffbp) and radial basis function (rbf) with decomposition methods. this includes a discrete wavelet transform (dwt) and ensemble empirical mode decomposition (eemd). on the basis of the autocorrelation function and the cross-correlation function of streamflow, precipitation and temperature inputs are selected for all developed models. data have been analyzed by comparing the simulation outputs of the models with a correlation coefficient (r), root mean square errors (rmse), nash-sutcliffe efficiency (nse), mean absolute percentage error (mape), and mean absolute errors (mae). the proposed hybrid models have been applied to monthly streamflow observations from three hydrological stations and 17 meteorological stations in the uib. the results show that the prediction accuracy of the decomposition-based models is usually better than those of ai-based models. among the dwt and eemd based hybrid model, eemd has performed significantly well when compared to all other hybrid and individual ai models. the peak value analysis is also performed to confirm the results precision rate during the flood season (may-october). the detailed comparative analysis showed that the rbfnn integrated with eemd has better forecasting capabilities as compared to other developed models and eemd-rbf can capture the nonlinear characteristics of the streamflow time series during the flood season with more precision.', 'streamflow prediction using machine learning models in selected rivers of southern india the need for adequate data on the spatial and temporal variability of freshwater resources is a significant challenge to the water managers of the world in water resource planning and management. the problems will be acute in the coming years because of the increase in frequency and intensity of hydrologic extremes due to climate change. therefore, streamflow prediction has become an important area of research because of its importance in flood mitigation, reservoir operation, and water resource management. in this paper, we have tested four machine learning models (ml models): support vector machines (svm), random forest (rf), long short-term memory (lstm), and multivariate adaptive regression splines (mars) for streamflow prediction at daily and monthly time scales in three rivers draining in the different climatic and geological settings. the svm, rf, lstm, and mars models have been trained and tested in the suvarna, aghanashini, and kunderu river basins in peninsular india. model intercomparison was made to identify the best suitable model for streamflow prediction. the rf outperforms other models for daily streamflow, and mars outperforms other models for monthly streamflow prediction in the suvarna river with nash-sutcliffe efficiency (nse) values of 0.676 and 0.924, respectively. svm (nse = 0.741) and rf (nse = 0.826) are found to be the best models for daily and monthly streamflow prediction in the aghanashini river. mars outperformed other models in the case of high, severe, and extreme flow simulation with nse values of 0.481, 0.374, and 0.455, respectively, in the aghanashini river. other hydrological variables (groundwater level data, antecedent soil moisture, potential evapotranspiration data) and a better spatial resolution of rainfall data can be used to develop more accurate machine-learning models for streamflow predictions.', 'evaluation of the impact of climate change on the streamflow of major pan-arctic river basins through machine learning models under a warmer climate, streamflow from the pan-arctic river basins exhibits increasing trends in recent years. the higher streamflow would have a large impact on the atmospheric and oceanic circulation of the arctic, even the globe. however, due to the limited observation data and the complex hydrologic processes in the polar river basins, it is challenging to simulate and to predict the streamflow under the impact of climate change. this study applied support vector regression model (svr), artificial neural network (ann), and multi-variable regression (mlr) model to simulate the monthly streamflow of four major pan-arctic river basins: mackenzie river basin, ob’ river basin, lena river basin, and yenisei river basin. after validating the svms with datasets independent of the calibration experience, the svms were used to predict the monthly streamflow of the four large pan-arctic river basins, under the climate projection scenarios, ssp245 and ssp585, of five cmip6 gcms: bcc-csm2-mr, cas-esm2-0, cmcc-esm2, fio-esm-2-0, and mpi-esm1-2-hr, for 2020–2100. results show that: 1) the precipitation and temperature are both projected to increase by almost five gcms in almost river basins with larger amplitudes under ssp585 than ssp245; 2) with wetter and warmer climate, the annual streamflow was projected to increase continuously over the twenty-first century for mackenzie, lena, and yenisei river significantly, with larger amplitudes under ssp585 than ssp245. the ob’ river does not show significant increasing trends in its annual streamflow; 3) the higher annual streamflow of lena and yenisei river basins mainly come from the increase in summer streamflow, while for meckanzie, the streamflow in summer and winter increases evenly. as its higher winter streamflow compenstates its lower summer streamflow, the annual streamflow of the ob’ river is projected with minimal changes. in addition, the onset of spring snowmelt is also projected to occur earlier in all river basins due to warmer climate. results of this study agree with previous findings that the arctic streamflow is increasing, but this study also demonstrate different responses in seasonal streamflow between the river basins to warmer temperature and higher precipitation, which is largely controlled by the respectiver terrestrial environment of the river basins.']"
140,climate_weather,9,38,140_downscaling_resolution_statistical downscaling_precipitation,"['downscaling', 'resolution', 'statistical downscaling', 'precipitation', 'super', 'super resolution', 'statistical', 'climate', 'simulations', 'projections', 'high resolution', 'gcm', 'coarse', 'deep', 'spatial', 'convolutional', 'cnns', 'deep learning', 'esm', 'rcm', 'daily', 'resolution climate', 'neural', 'scale', 'local', 'emulator', 'network', 'bias', 'downscale', 'regional']","['generating high-resolution climate change projections using super-resolution convolutional lstm neural networks generating projections of climate change through extreme indices such as precipitation and temperature is crucial to evaluate their potential impacts on critical infrastructures, human health, and natural systems. however, current earth system models (esms) run at spatial resolutions of hundreds of kilometers which is too coarse to analyze localized impacts. to tackle this issue, statistical downscaling is a widely employed technique that uses historical climate observations to learn a coarse-resolution to fine-resolution mapping. traditional statistical methods are inefficient in downscaling precipitation data and vary significantly in terms of accuracy and reliability since local climate variables such as precipitation are dependent on non-linear and complex spatiooral processes. to capture both spatial and temporal variabilities, we develop a super-resolution based convolutional long short term memory neural network and test the robustness and predictability of this model on monthly precipitation data in china. we integrate original climate data from an esm and perform downscaling on precipitation at $(1.25×0.9 to (0.25. experimental data indicates that our convolutional lstm model performs the best compared to existing methods in terms of mean squared error, relative bias, and correlation coefficient.', 'regional analysis of esm models using bias corrected spatial disaggregated superresolution convolutional neural networks climate change is very crucial for ecological systems and society. but global climate models run at coarse spatial resolution which is difficult to do regional analysis. regionalscale projections can be obtained by a technique called statistical downscaling which uses past data to find out the high resolution and low-resolution mapping. there are many methods for statistical downscaling of climate data: 1) conventional methods 2) deep learning architecture. some of the existing works like deepsd downscaled high-resolution climate projections but in such cases, global climate model (gcm) data suffers from concept drift, change of mapping between input and label over time. so applying these deep learning models is not a good idea for statistical downscaling. in my study, i have developed new approach of downscaling which outperforms other deep learning architectures like super-resolution convolutional neural network (srcnn), long short term memory network (lstm) in terms of accuracy and reliability. these existing models focus on minimizing the root mean square error (rmse) and do not take care of the tails or extremes. therefore the objective function of these models should be changed other than root mean square error (rmse). my proposed model focuses on both means and extremes. i provide a comparison between proposed and other existing deep learning models in downscaling daily precipitation and temperature from 1.25 to 0.25 resolution over india. i have downscaled 6 global climate model (gcm) models in our comparative study.', 'deepsd: generating high resolution climate change projections through single image super-resolution the impacts of climate change are felt by most critical systems, such as infrastructure, ecological systems, and power-plants. however, contemporary earth system models (esm) are run at spatial resolutions too coarse for assessing effects this localized. local scale projections can be obtained using statistical downscaling, a technique which uses historical climate observations to learn a low-resolution to high-resolution mapping. depending on statistical modeling choices, downscaled projections have been shown to vary significantly terms of accuracy and reliability. the spatio-temporal nature of the climate system motivates the adaptation of super-resolution image processing techniques to statistical downscaling. in our work, we present deepsd, a generalized stacked super resolution convolutional neural network (srcnn) framework for statistical downscaling of climate variables. deepsd augments srcnn with multi-scale input channels to maximize predictability in statistical downscaling. we provide a comparison with bias correction spatial disaggregation as well as three automated-statistical downscaling approaches in downscaling daily precipitation from 1 degree (100km) to 1/8 degrees (12.5km) over the continental united states. furthermore, a framework using the nasa earth exchange (nex) platform is discussed for downscaling more than 20 esm models with multiple emission scenarios.']"
141,climate_weather,9,34,141_lstm_temperature_forecasting_weather,"['lstm', 'temperature', 'forecasting', 'weather', 'prediction', 'deep', 'deep learning', 'time series', 'series', 'short term', 'temperature forecasting', 'term', 'short', 'memory', 'term memory', 'long short', 'long', 'climate', 'recurrent', 'network', 'encoder', 'series forecasting', 'neural', 'time', 'climate prediction', 'learning', 'error', 'rnn', 'weather prediction', 'feature']","['uncertainty and sensitivity analysis of deep learning models for diurnal temperature range (dtr) forecasting over five indian cities in this article, the maximum and minimum daily temperature data for indian cities were tested, together with the predicted diurnal temperature range (dtr) for monthly time horizons. rclimdex, a user interface for extreme computing indices, was used to advance the estimation because it allowed for statistical analysis and comparison of climatological elements such time series, means, extremes, and trends. during these 69\xa0years, a more erratic dtr trend was seen in the research area. this study investigates the suitability of three deep neural networks for one-step-ahead dtr time series (dtrts) forecasting, including recurrent neural network (rnn), long short-term memory (lstm), gated recurrent unit (gru), and auto-regressive integrated moving average exogenous (arimax). to evaluate the effectiveness of models in the testing set, six statistical error indicators, including root mean square error (rmse), mean absolute error (mae), coefficient of correlation (r), percent bias (pbias), modified index of agreement (md), and relative index of agreement (rd), were chosen. the wilson score approach was used to do a quantitative uncertainty analysis on the prediction error to forecast the outcome dtr. the findings show that the lstm outperforms the other models in terms of its capacity to forget, remember, and update information. it is more accurate on datasets with longer sequences and displays noticeably more volatility throughout its gradient descent. the results of a sensitivity analysis on the lstm model, which used rmse values as an output and took into account different look-back periods, showed that the amount of history used to fit a time series forecast model had a direct impact on the model’s performance. as a result, this model can be applied as a fresh, trustworthy deep learning method for dtrts forecasting.', 'attention based long-term air temperature forecasting network: altf net air temperature is one of the most important meteorological parameters related with atmospheric and environmental research. in this context, accurate prediction and forecasting of temperature is crucial due to the current global climate change. although, the short term temperature forecasting have been more or less conquered in the past by using predictive algorithms, the long-term temperature forecasting is still a challenging task. long term temperature forecasting is previously attempted by deep learning methods like recurrent neural network (rnn), long short term memory (lstm), etc. however, the gradient explosion and gradient vanishing problems of the rnn based networks were the major roadblock in the path of long-term prediction. so, in this paper, an attention-based model called altf net (attention based long term temperature forecasting network) approaches this problem using an encoder–decoder orientation. the encoder encodes the relative dependencies of the auto-regressive time-series into an attention tensor which is used by the decoder to produce the prediction. the encoder is augmented to incorporate a convolution block to recognize the seasonal patterns. the proposed model altf uses a transformer with an augmented encoder to predict temperature up to 150 days with high accuracy, a feat which would be difficult using rnn and lstm. the model has been trained with 25+ years of data from 5 cities around the globe and the performance have been rigorously evaluated in terms of rmse, mae, r2, and correlation values. it is observed that the proposed model dominated over several baselines (arima, rfr, knn, mlp, rnn, cnn, lstm, and transformer) for long term temperature forecasting.', 'prediction of 10-min, hourly, and daily atmospheric air temperature: comparison of lstm, anfis-fcm, and arma prediction of atmospheric air temperature (aat) time series is an important issue as it gives information to society and sustainability for future planning. in this study, a deep learning method, namely, long short-term memory (lstm) network, based on one-step-ahead prediction approach was proposed to predict aat using the actual time series data. for the proposed prediction method, a set of measurement data in 10-min, hourly, and daily intervals obtained from mersin and belen stations located in the eastern mediterranean region of turkey was used. mean absolute percentage error (mape), root mean square error (rmse), correlation coefficient (r), mean absolute error (mae), and average bias were considered as evaluation criteria. according to the testing process, the rmse, mape, mae, r, and bias values for the 10-min interval aat prediction were calculated as 0.35 °c, 1.40%, 0.25 °c, 0.995, and 0.074 °c, respectively. considering the prediction results of the hourly aat predication, the above statistical metrics with the same order were obtained as 0.61 °c, 1.85%, 0.43\xa0°c, 0.945, and −0.013 °c. concerning the daily aat prediction results with lstm, the above statistical metrics with the same order were computed as 1.33\xa0°c, 3.27%, 0.99\xa0°c, 0.97, and −0.116 °c. compared to the hourly and daily aat predictions, lstm provided better accuracy results in predicting 10-min interval aat. the prediction results from the three different time series data show that the prediction of aats using lstm can provide high accuracy results for short-term prediction using data with a long period time. on the other hand, adaptive neuro-fuzzy inference system with fuzzy c-means (anfis-fcm) method and autoregressive moving average (arma) model were also used to compare the results of lstm method. both lstm and anfis-fcm network model showed high accuracy for the prediction of 10-min interval, hourly, and daily aat data with rmse values between 0.31 and 1.52 °c, while arma model failed to provide high accuracies for all predictions.']"
142,climate_weather,9,32,142_precipitation_events_extreme_weather,"['precipitation', 'events', 'extreme', 'weather', 'anomalous', 'circulation', 'precipitation events', 'forecast', 'cyclone', 'deep', 'deep learning', 'extreme weather', 'climate', 'large', 'extreme precipitation', 'convolutional', 'variational', 'weather events', 'dl', 'forecasting', 'neural', 'dynamical', 'geopotential', 'learning', 'occurrence', 'simulations', 'frequency', 'convolutional neural', 'atmospheric', 'seasonal']","['machine learning-based detection of weather fronts and associated extreme precipitation in historical and future climates extreme precipitation events, including those associated with weather fronts, have wide-ranging impacts across the world. here we use a deep learning algorithm to identify weather fronts in high resolution community earth system model (cesm) simulations over the contiguous united states (conus), and evaluate the results using observational and reanalysis products. we further compare results between cesm simulations using present-day and future climate forcing, to study how these features might change with climate change. we find that detected front frequencies in cesm have seasonally varying spatial patterns and responses to climate change and are found to be associated with modeled changes in large scale circulation such as the jet stream. we also associate the detected fronts with precipitation and find that total and extreme frontal precipitation mostly decreases with climate change, with some seasonal and regional differences. decreases in northern hemisphere summer frontal precipitation are largely driven by changes in the frequency of different front types, especially cold and stationary fronts. on the other hand, northern hemisphere winter exhibits some regional increases in frontal precipitation that are largely driven by changes in frontal precipitation intensity. while conus mean and extreme precipitation generally increase during all seasons in these climate change simulations, the likelihood of frontal extreme precipitation decreases, demonstrating that extreme precipitation has seasonally varying sources and mechanisms that will continue to evolve with climate change.', 'long-term changes, synoptic behaviors, and future projections of large-scale anomalous precipitation events in china detected by a deep learning autoencoder the frequency of large-scale anomalous precipitation events over china has increased during 1961–2018. however, it remains challenging to understand the mechanisms associated with these anomalous events showing different spatial patterns. here, we applied an autoencoder technique to identify large-scale anomalous precipitation events for both observations and model simulations, which were then classified into several patterns through a self-organizing map method. the synoptic behavior and atmospheric circulation background of different anomalous patterns were also analyzed using simultaneous composite analyses. results show that occurrences of different anomalous precipitation patterns have increased significantly, except those centered in north china, northeast china, and the yangtze river basin. the anomalous precipitation patterns manifest various intraseasonal distributions, which are linked to zonal oscillations of the western north pacific subtropical high (wnpsh) and meridional displacements of east asia westerly jet (eaj). accompanied by the westward movement of wnpsh, anticyclonic systems transport warm moist air from the indian ocean and the south china sea to converge with the cold air caused by anomalous cyclones over the northwest flank of the wnpsh, leading to large-scale anomalous precipitation in these regions. besides wnpsh, the northward and southward displacements of eaj also favor the occurrence of anomalous precipitation events in northern and southern china, respectively. our study also illustrates that the occurrence frequency of anomalous precipitation events is projected to increase remarkably under the shared socioeconomic pathway 5–8.5 (ssp585) scenario by rates of twofold to fourfold.', 'increasing frequency of anomalous precipitation events in japan detected by a deep learning autoencoder the frequency of large-scale anomalous precipitation events associated with heavy precipitation has been increasing in japan. however, it is unclear if the increase is due to anthropogenic warming or internal variability. also, it is challenging to develop an objective methodology to identify anomalous events because of the large variety of anomalous precipitation cases. in this study, we applied a deep learning technique to objectively detect anomalous precipitation events in japan for both observations and simulations using high-resolution climate models. the results show that the observed increases in anomalous heavy precipitation events in western japan during 1977–2015 were not made only by internal variability but the increases in anthropogenic forcing played an important role. such events will continue to increase in frequency this century. the increases are attributable to the increasing frequency of tropical cyclones and enhanced frontal rainbands near japan. these results highlight the mitigation challenge posed by the increasing occurrence of unprecedented precipitation events in the future.']"
3,wildfires,10,47,3_wildfire_fires_wildfires_detection,"['wildfire', 'fires', 'wildfires', 'detection', 'smoke', 'forest fires', 'forest', 'burned', 'early', 'images', 'detect', 'early detection', 'image', 'segmentation', 'dataset', 'real time', 'area mapping', 'deep', 'areas', 'network', 'real', 'deep learning', 'satellite', 'using', 'burning', 'onboard', 'time', 'detection using', 'aerial', 'natural']","['forest fire smoke detection based on deep learning approaches and unmanned aerial vehicle images wildfire poses a significant threat and is considered a severe natural disaster, which endangers forest resources, wildlife, and human livelihoods. in recent times, there has been an increase in the number of wildfire incidents, and both human involvement with nature and the impacts of global warming play major roles in this. the rapid identification of fire starting from early smoke can be crucial in combating this issue, as it allows firefighters to respond quickly to the fire and prevent it from spreading. as a result, we proposed a refined version of the yolov7 model for detecting smoke from forest fires. to begin, we compiled a collection of 6500 uav pictures of smoke from forest fires. to further enhance yolov7’s feature extraction capabilities, we incorporated the cbam attention mechanism. then, we added an sppf+ layer to the network’s backbone to better concentrate smaller wildfire smoke regions. finally, decoupled heads were introduced into the yolov7 model to extract useful information from an array of data. a bifpn was used to accelerate multi-scale feature fusion and acquire more specific features. learning weights were introduced in the bifpn so that the network can prioritize the most significantly affecting characteristic mapping of the result characteristics. the testing findings on our forest fire smoke dataset revealed that the proposed approach successfully detected forest fire smoke with an ap50 of 86.4%, 3.9% higher than previous single- and multiple-stage object detectors.', 'deep learning and transformer approaches for uav-based wildfire detection and segmentation wildfires are a worldwide natural disaster causing important economic damages and loss of lives. experts predict that wildfires will increase in the coming years mainly due to climate change. early detection and prediction of fire spread can help reduce affected areas and improve firefighting. numerous systems were developed to detect fire. recently, unmanned aerial vehicles were employed to tackle this problem due to their high flexibility, their low-cost, and their ability to cover wide areas during the day or night. however, they are still limited by challenging problems such as small fire size, background complexity, and image degradation. to deal with the aforementioned limitations, we adapted and optimized deep learning methods to detect wildfire at an early stage. a novel deep ensemble learning method, which combines efficientnet-b5 and densenet-201 models, is proposed to identify and classify wildfire using aerial images. in addition, two vision transformers (transunet and transfire) and a deep convolutional model (efficientseg) were employed to segment wildfire regions and determine the precise fire regions. the obtained results are promising and show the efficiency of using deep learning and vision transformers for wildfire classification and segmentation. the proposed model for wildfire classification obtained an accuracy of 85.12% and outperformed many state-of-the-art works. it proved its ability in classifying wildfire even small fire areas. the best semantic segmentation models achieved an f1-score of 99.9% for transunet architecture and 99.82% for transfire architecture superior to recent published models. more specifically, we demonstrated the ability of these models to extract the finer details of wildfire using aerial images. they can further overcome current model limitations, such as background complexity and small wildfire areas.', 'a wildfire smoke detection system using unmanned aerial vehicle images based on the optimized yolov5 wildfire is one of the most significant dangers and the most serious natural catastrophe, endangering forest resources, animal life, and the human economy. recent years have witnessed a rise in wildfire incidents. the two main factors are persistent human interference with the natural environment and global warming. early detection of fire ignition from initial smoke can help firefighters react to such blazes before they become difficult to handle. previous deep-learning approaches for wildfire smoke detection have been hampered by small or untrustworthy datasets, making it challenging to extrapolate the performances to real-world scenarios. in this study, we propose an early wildfire smoke detection system using unmanned aerial vehicle (uav) images based on an improved yolov5. first, we curated a 6000-wildfire image dataset using existing uav images. second, we optimized the anchor box clustering using the k-mean++ technique to reduce classification errors. then, we improved the network’s backbone using a spatial pyramid pooling fast-plus layer to concentrate small-sized wildfire smoke regions. third, a bidirectional feature pyramid network was applied to obtain a more accessible and faster multi-scale feature fusion. finally, network pruning and transfer learning approaches were implemented to refine the network architecture and detection speed, and correctly identify small-scale wildfire smoke areas. the experimental results proved that the proposed method achieved an average precision of 73.6% and outperformed other one- and two-stage object detectors on a custom image dataset.']"
4,wildfires,10,19,4_wildfire_deep learning_deep_burning,"['wildfire', 'deep learning', 'deep', 'burning', 'prediction', 'forest', 'fires', 'forest fires', 'wildfires', 'spread', 'smart cities', 'danger', 'repair', 'construction', 'susceptibility', 'learning', 'disaster prevention', 'campus', 'forecasting', 'prevention', 'prediction forest', 'disaster', 'forest prediction', 'accidents', 'construction sites', 'natural disasters', 'weather', 'events', 'disasters', 'increase']","['creation of wildfire susceptibility maps in the mediterranean region (turkey) using convolutional neural networks and multilayer perceptron techniques considering the natural disasters that have developed in the world in recent years, it is known that there is an increase in wildfire disasters with the effects of climate change. in this study, wildfire susceptible areas were determined in the provinces of muğla, antalya, mersin, adana, osmaniye, and hatay in the mediterranean region (turkey). within the scope of this purpose, convolutional neural network (cnn) and multilayer perceptron (mlp) methods, the most widely used deep learning techniques in the literature in recent years, were preferred to create wildfire susceptibility models. seventeen environmental variables were used in the analyses, and these variables were grouped as topographic factors, anthropological and environmental factors, climatic factors, and vegetation factors. in addition, the number of fire inventory data has been balanced with the help of the synthetic minority oversampling technique (smote) used to increase the model result performance of the scarce inventory data. in the maps obtained by cnn and mlp methods, 17% and 28% of the study area were determined as high and very high susceptible areas, respectively. the results demonstrated that the cnn model had superior performance in wildfire susceptibility assessment with accuracy (%85.8), precision (%98.7), sensitivity (%85.5), f- score (%91.6), and roc curve (%78.6). this model was followed by the mlp model with slightly lower accuracy values, which indicates that the cnn models can reach considerably better prediction capability than the mlp models. finally, the wildfire susceptibility maps produced by deep learning methods could aid decision-makers and government organizations in the mediterranean region in preventing future natural disasters.', 'research on forest fire prediction method based on deep learning in recent years, the number of occurrences of forest fire has been increased, which causes serious damage to the ecosystem, indirectly causing global warming and extreme weather. there is a clear feedback between climate change and forest fire. climate change induces weather extremes, which leads to forest fires, while fire emissions con⁃ tribute to climate change. accurate prediction of the forest fire can enable relevant personnel to take effective preven⁃ tion and control measures in advance. controlling the occurrence of forest fire not only protects the ecological environ⁃ ment, but also greatly addresses climate issues, so accurate prediction of forest fires is of great significance. tradition⁃ al forest fire prediction models are mostly mathematical methods and shallow neural networks. they take the meteoro⁃ logical factors as the inputs to predict forest fire. when the amount of data increases, they are prone to be problems, such as modeling difficulties and reduced prediction accuracy. deep learning has certain advantages in dealing with a large amount of nonlinear data, and its model has a multi⁃layer network structure. by training the deep learning mod⁃ el, more representative feature values can be extracted and the implicit relationship among data can be found, to a⁃ chieve the purpose of accurate classification and prediction. compared with the traditional fire prediction model, the deep learning model has a deep network, and the parameters can be adjusted autonomously through the training data, which is suitable for a large amount of nonlinear data. therefore, this study used the deep belief network (dbn) in the deep learning model as the predictive model and the meteorological factor as the input data to solve the problem that the traditional forest fire prediction model could not predict well when faced with large amounts of data.in this study, the smote (synthetic minority oversampling technique) algorithm was combined with dbn to balance the forest fire data set and increase the amount of training data, which made the forest fire data more suitable for dbn. as a result, the prediction accuracy of forest fire was improved. the prediction result of the model was compared with the support vector machine (svm) and back propagation (bp) models. the modeling results showed that the prediction accuracy of the model can reach 84%, which was obviously better than svm and bp network. it was demonstrated the superiority of applying deep learning to forest fire prediction. this study can provide a theoretical foundation for the application of deep learning in the field of forestry.', 'a deep learning ensemble model for wildfire susceptibility mapping devastating wildfires have increased in frequency and intensity over the last few years, worsened by climate change and prolonged droughts. wildfire susceptibility mapping with machine learning has been proven useful for fire-prevention plans, turning into an indispensable tool in wildfire prevention. however, applications of deep learning models in wildfire susceptibility prediction to date are scarce. this study proposes a new ensemble model based on two deep learning networks previously presented in literature that achieved remarkable results for forest fire susceptibility and other environmental risks. we compare our model with each of its sub-models, two more deep learning networks, and other machine learning benchmark, namely, xgboost and svm. furthermore, we analyze the effects that different sample patch sizes have on the predictive performance of the algorithms. as case study we selected the fire occurrences in two regions in chile, from 2013 to 2019. satellite imagery data for fifteen fire influencing factors in the study area were retrieved to build a dataset to extract the samples to train the models. these factors include elevation, aspect, surface roughness, slope, minimum and maximum temperature, wind speed, precipitation, actual evapotranspiration, climatic water deficit, ndvi, land cover type, distance to rivers, distance to roads and distance to urban areas. during training, the best sample patch size was found to be 25 × 25 pixels. as a result, the highest area under the curve (auc) was 0.953 achieved by the ensemble model, followed by cnn-1 with auc = 0.902. the ensemble model also achieved the best accuracy, sensitivity, specificity, negative predictive value and f1 score. finally, the predicted susceptibility maps suggest that static variables can be considered as predisposing factors, while dynamic variables affect the intensity of the predicted probabilities, with an important role of the anthropogenic variables. these resulting maps may be useful to prioritize wildfire surveillance and monitoring in extensive high risk areas.']"
5,wildfires,10,48,5_wildfire_fires_severity_wildfires,"['wildfire', 'fires', 'severity', 'wildfires', 'forest', 'occurrence', 'burned', 'drivers', 'risk', 'danger', 'area', 'forest fires', 'forests', 'weather', 'exposure', 'areas', 'burnt', 'climate', 'fuel', 'burn', 'climatic', 'smoke', 'topography', 'conditions', 'pm2', 'probability', 'albedo', 'wildland', 'mediterranean', 'machine learning']","['high-resolution mapping of wildfire drivers in california based on machine learning wildfires are important natural disturbances of ecosystems; however, they threaten the sustainability of ecosystems, climate and humans worldwide. it is vital to quantify and map the controlling drivers of wildfires for effective wildfire prediction and risk management. however, high-resolution mapping of wildfire drivers remains challenging. here we established machine-learning (random forests) models using 23 climate and land surface variables as model inputs to reconstruct the spatial variability and seasonality of wildfire occurrence and extent in california. the importance of individual drivers was then quantified based on the shapley value method. thus, we provided spatially resolved maps of wildfire drivers at high resolutions up to 0.004° × 0.004°. the results indicated that precipitation and soil moisture are the major drivers dominating 37% of the total burnt area for large and extreme wildfires in summer and 63% in autumn, while elevation plays a major role for 15–58% of burnt areas in small wildfires in all seasons. winds are also an important contributor to summer wildfires, accounting for 41% of large and extreme burnt areas. this study enhanced our knowledge of spatial variability of wildfire drivers across diverse landscapes in a fine-scale mapping, providing valuable perspectives and case studies for other regions of the world with frequently occurred wildfire.', 'drivers of fire severity shift as landscapes transition to an active fire regime, klamath mountains, usa fire severity patterns are driven by interactions between fire, vegetation, and terrain, and they generate legacy effects that influence future fire severity. a century of fire exclusion and fuel buildup has eroded legacy effects, and contemporary fire severity patterns may diverge from historical patterns. in recent decades, area burned and area burned at high severity have increased and landscapes are transitioning back to an active fire regime where disturbance legacies will again play a strong role in determining fire severity. understanding the drivers of fire severity is crucial for anticipating future fire severity patterns as active fire regimes are reestablished. we identified drivers of fire severity in the klamath mountains, a landscape with an active fire regime, using two machine learning statistical models: one model for non-reburns (n\xa0=\xa092) and one model for reburns (n\xa0=\xa061). both models predicted low better than moderate or high-severity fire. fire severity drivers contrasted sharply between non-reburns and reburns. fire weather and fuels were dominant controls in non-reburns, while previous burn severity, fuel characteristics, and time since last fire were drivers for reburns. in reburns, areas initially burned at low (high) severity burned the same way again. this tendency was sufficiently strong that reburn fire severity could be predicted equally well with only severity of the previous fire in the model. thus, reburn fire severity is more predictable than severity in non-reburns that are driven by the stochastic influences of fire weather. reburn severity in aggregate was also higher than non-reburn severity suggesting a positive feedback effect that could contribute to an upward drift in fire severity as area burned increases. terrain had low importance in both models. this indicates strong terrain controls in the past may not carry into the future. low- and moderate-severity fire effects were prevalent in non-reburns under moderate fire weather and self-reinforcing behavior maintained these effects in reburns even under more extreme weather, particularly in reburns within 10\xa0yr. our findings suggest deliberate use of wildfire and prescribed fire under moderate conditions would increase fire resilience in landscapes transitioning to an active fire regime.', 'intensified burn severity in californias northern coastal mountains by drier climatic condition the severity of wildfire burns in interior lands of western us ecosystems has been increasing. however, less is known about its coastal mountain ecosystems, especially under extreme weather conditions, raising concerns about the vulnerability of these populated areas to catastrophic fires. here we examine the fine-scale association between burn severity and a suite of environmental drivers including explicit fuel information, weather, climate, and topography, for diverse ecosystems in californias northern coastal mountains. burn severity was quantified using relative difference normalized burn ratio from landsat multispectral imagery during 1984-2017. we found a significant increasing trend in burned areas and severity. during low-precipitation years, areas that burned had much lower fuel moisture and higher climatic water deficit than in wetter years, and the percentage of high-severity areas doubled, especially during the most recent 2012-2016 drought. the random forest (rf) machine learning model achieved overall accuracy of 79% in classifying categories of burn severity. aspect, slope, fuel type and availability, and temperature were the most important drivers, based on both classification and regression rf models. we further examined the importance of drivers under four climatic conditions: dry vs. wet years, and during two extended drought periods (the 2012-2016 warmer drought vs. the 1987-1992 drought). during warm and dry years, the spatial variability of burn severity was a mixed effect of slope, long-term minimum temperature, fuel amount, and fuel moisture. in contrast, climatic water deficit and short-term weather became dominant factors for fires during wetter years. these results suggest that relative importance of drivers for burn severity in the broader domain of californias northern coastal mountains varied with weather scenarios, especially when exacerbated by warm and extended drought. our findings highlight the importance of targeting areas with high burn severity risk for fire adaptation and mitigation strategies in a changing climate and intensifying extremes.']"
6,wildfires,10,34,6_smoke_forest_fires_wildfire,"['smoke', 'forest', 'fires', 'wildfire', 'burned', 'wildfires', 'forest fires', 'burn', 'spectral', 'severity', 'area', 'forests', 'rf', 'prediction', 'machine', 'classification', 'patches', 'rating', '97', 'random', 'svm', 'machine learning', 'support vector', 'random forest', 'wine', 'detection', 'danger', 'vector', 'regression', 'algorithms']","['remote sensing technology for rapid extraction of burned areas and ecosystem environmental assessment forest fires are one of the significant disturbances in forest ecosystems. it is essential to extract burned areas rapidly and accurately to formulate forest restoration strategies and plan restoration plans. in this work, we constructed decision trees and used a combination of differential normalized burn ratio (dnbr) index and otsu threshold method to extract the heavily and mildly burned areas. the applicability of this method was evaluated with three fires in muli county, sichuan, china, and we concluded that the extraction accuracy of this method could reach 97.69% and 96.37% for small area forest fires, while the extraction accuracy was lower for large area fires, only 89.32%. in addition, the remote sensing environment index (rsei) was used to evaluate the ecological environment changes. it analyzed the change of the rsei level through the transition matrix, and all three fires showed that the changes in rsei were stronger for heavily burned areas than for mildly burned areas, after the forest fire the ecological environment (rsei) was reduced from good to moderate. these results realized the quantitative evaluation and dynamic evaluation of the ecological environment condition, providing an essential basis for the restoration, decision making and management of the affected forests.', 'predicting forest fires using supervised and ensemble machine learning algorithms forest fires have become one of the most frequently occurring disasters in recent years. the effects of forest fires have a lasting impact on the environment as it lead to deforestation and global warming, which is also one of its major cause of occurrence. forest fires are dealt by collecting the satellite images of forest and if there is any emergency caused by the fires then the authorities are notified to mitigate its effects. by the time the authorities get to know about it, the fires would have already caused a lot of damage. data mining and machine learning techniques can provide an efficient prevention approach where data associated with forests can be used for predicting the eventuality of forest fires. this paper uses the dataset present in the uci machine learning repository which consists of physical factors and climatic conditions of the montesinho park situated in portugal. various algorithms like logistic regression, support vector machine, random forest, k-nearest neighbors in addition to bagging and boosting predictors are used, both with and without principal component analysis (pca). among the models in which pca was applied, logistic regression gave the highest f-1 score of 68.26 and among the models where pca was absent, gradient boosting gave the highest score of 68.36.', 'suppressing forest fires in global climate change through artificial intelligence: a case study on british columbia forest fires used to be a tool for human survival. in primitive society, humans could use forest fires to complete hunting and land reclamation work. in modern society, forest fires pose a massive threat to the environment. this paper analyzes a forest fire case in british columbia. firstly, this paper summarizes the most controversial factors that cause forest fires, such as global warming, changes in local precipitation, and bark beetles. secondly, this paper explores the impact of forest fires, the losses to the logging industry, such as the recovery costs of forests, the reduction of carbon sinks against climate change and the influence on human health. the conclusion of this paper through case study is: firstly, global warming and changes in local precipitation and other factors will provide conditions for the spread of forest fires, increasing the number of wildfires in the forest. secondly, bark beetles will produce more fuel. they also reduce the availability of forest logs and limit the development of logging. thirdly, uncontrollable forest fires will destroy forests and nearby infrastructure and increase recovery costs. fourthly, reducing forest areas will destroy carbon sinks. fifthly, the particles produced by forest fires will cause severe heart and lung diseases in the human body. finally, this paper summarizes four solutions to forest fires. forest fire suppression through artificial intelligence, bark beetle removal solution and climate change technological solution. in the artificial intelligence solution part, support vector machine (svm) and random forest were used to analyze four groups of different characteristics, distribution space, time, climate index and fwi system index. by inputting four basic meteorologies, such as temperature, relative humidity, wind speed and precipitation, the model can accurately predict the affected area of small-scale and frequent fires.']"
53,business_communication,11,28,53_stem_voice_peace_scientific,"['stem', 'voice', 'peace', 'scientific', 'attitudes', 'public', 'political', 'beliefs', 'meat', 'climate change', 'change', 'science', 'communication', 'behavioral', 'individuals', 'respondents', 'climate', 'risk perception', 'taxes', 'survey', 'personal', 'policy', 'mental', 'behaviors', 'environmental', 'positive', 'social', 'people', 'trust', 'messages']","['climate policy support as a tool to control others’ (but not own) environmental behavior? drastic reductions in greenhouse gas emissions are necessary to successfully mitigate climate change. individual environmental behavior is central to this change. given that environmental behavior necessitates 1) effortful individual self-control and 2) cooperation by others, public policy may constitute an attractive instrument for regulating one’s own as well as others’ environmental behavior. framing climate change mitigation as a cooperative self-control problem, we explore the incremental predictive power of self-control and beliefs surrounding others’ cooperation beyond established predictors of policy support in study 1 using machine-learning (n = 610). in study 2, we systematically test and confirm the effects of self-control and beliefs surrounding others’ cooperation (n = 270). both studies showed that personal importance of climate change mitigation and perceived insufficiency of others’ environmental behavior predict policy support, while there was no strong evidence for a negative association between own-self control success and policy support. these results emerge beyond the effects of established predictors, such as environmental attitudes and beliefs, risk perception (study 1), and social norms (study 2). results are discussed in terms of leveraging policy as a behavioral enactment constraint to control others’ but not own environmental behavior.', 'the role of beliefs, expectations and values in decision-making favoring climate change adaptation - implications for communications with european forest professionals beliefs, expectations and values are often assumed to drive decisions about climate change adaptation. we tested hypotheses based on this assumption using survey responses from 508 european forest professionals in ten countries. we used the survey results to identify communication needs and the decision strategies at play, and to develop guidelines on adequate communications about climate change adaptation. we observed polarization in the positive and negative values associated with climate change impacts accepted by survey respondents. we identified a mechanism creating the polarization that we call the blocked belief effect. we found that polarized values did not correlate with decisions about climate change adaptation. strong belief in the local impacts of climate change on the forest was, however, a prerequisite of decision-making favoring adaptation. decision-making in favor of adaptation to climate change also correlated with net values of expected specific impacts on the forest and generally increased with the absolute value of these in the absence of tipping point behavior. tipping point behavior occurs when adaptation is not pursued in spite of the strongly negative or positive net value of expected climate change impacts. we observed negative and positive tipping point behavior, mainly in sw europe and n-ne europe, respectively. in addition we found that advice on effective adaptation may inhibit adaptation when the receiver is aware of effective adaptation measures unless it is balanced with information explaining how climate change leads to negative impacts. forest professionals with weak expectations of impacts require communications on climate change and its impacts on forests before any advice on adaptation measures can be effective. we develop evidence-based guidelines on communications using a new methodology which includes bayesian machine learning modeling of the equivalent of an expected utility function for the adaptation decision problem.', 'longitudinal predictors of perceived climate change importance and worry among italian youths: a machine learning approach the current study aimed to investigate the longitudinal predictors of perceived importance of climate change and personal worry among italian youths. specifically, we used machine learning techniques to examine the predictive importance of a wide range of socio-demographic factors, political perceptions, attitudes on a national and european level (identity, attitudes, tolerance, support for democracy, authoritarianism, nationalism, political trust), efficacy beliefs, social well-being, political interest, and different forms of participation on perceived importance of climate change and personal worry. in this longitudinal study, we collected data using a questionnaire in two waves at a one-year interval—in 2016 and 2017. participants were 1288 italian young adults (61.3% were female; 38.7% were male) whose mean age was 19.18 (sd = 3.29) ranging between 15 and 30 years. breiman’s random forest algorithm performed better than friedman’s gradient boosting machines algorithm. the random forest algorithm revealed that age, tolerance toward migrants, and tolerance toward refugees were the most important predictors of perceived importance of climate change and personal worry. other important predictors were national/european identity, political interest, internal political efficacy, nationalism, social well-being, self-efficacy, authoritarianism, anti-democratic attitudes, eu warmth, and online and civic participation.']"
55,business_communication,11,46,55_adaptation_topics_literature_topic,"['adaptation', 'topics', 'literature', 'topic', 'climate', 'publications', 'articles', 'topic modeling', 'health', 'climate change', 'evidence', 'papers', 'change', 'scientific', 'tropical subtropical', 'bibliometric', 'mitigation', 'subtropical', 'keywords', 'sh', 'nlp', 'plans', 'sciences', 'lda', 'trends', 'text', 'field', 'documents', 'abstracts', 'actions']","['progress in climate change adaptation research the scientific literature on climate change adaptation has become too large to assess manually. beyond standard scientometrics, questions about if and how the field is progressing thus remain largely unanswered. here we provide a novel, inquisitive, computer-assisted evidence mapping methodology that combines expert interviews (n = 26) and structural topic modelling to evaluate open-ended research questions on progress in the field. we apply this to 62 191 adaptation-relevant scientific publications (1988-2020), selected through supervised machine learning from a comprehensive climate change query. comparing the literature to key benchmarks of mature adaptation research, our findings align with trends in the adaptation literature observed by most experts: the field is maturing, growing rapidly, and diversifying, with social science and implementation topics arising next to the still-dominant natural sciences and impacts-focused research. formally assessing the representativeness of ipcc citations, we find evidence of a delay effect for fast-growing areas of research like adaptation strategies and governance. similarly, we show significant topic biases by geographic location: especially disaster and development-related topics are often studied in southern countries by authors from the north, while northern countries dominate governance topics. moreover, there is a general paucity of research in some highly vulnerable countries. experts lastly signal a need for meaningful stakeholder involvement. expanding on the methods presented here would aid the comprehensive and transparent monitoring of adaptation research. for the evidence synthesis community, our methodology provides an example of how to move beyond the descriptive towards the inquisitive and formally evaluating research questions.', 'mapping global research on climate and health using machine learning (a systematic evidence map) climate change is already affecting health in populations around the world, threatening to undermine the past 50 years of global gains in public health. health is not only affected by climate change via many causal pathways, but also by the emissions that drive climate change and their co-pollutants. yet there has been relatively limited synthesis of key insights and trends at a global scale across fragmented disciplines. compounding this, an exponentially increasing literature means that conventional evidence synthesis methods are no longer sufficient or feasible. here, we outline a protocol using machine learning approaches to systematically synthesize global evidence on the relationship between climate change, climate variability, and weather (ccvw) and human health. we will use supervised machine learning to screen over 300,000 scientific articles, combining terms related to ccvw and human health. our inclusion criteria comprise articles published between 2013 and 2020 that focus on empirical assessment of: ccvw impacts on human health or health-related outcomes or health systems; relate to the health impacts of mitigation strategies; or focus on adaptation strategies to the health impacts of climate change. we will use supervised machine learning (topic modeling) to categorize included articles as relevant to impacts, mitigation, and/or adaptation, and extract geographical location of studies. unsupervised machine learning using topic modeling will be used to identify and map key topics in the literature on climate and health, with outputs including evidence heat maps, geographic maps, and narrative synthesis of trends in climate-health publishing. to our knowledge, this will represent the first comprehensive, semi-automated, systematic evidence synthesis of the scientific literature on climate and health.', 'systematic mapping of global research on climate and health: a machine learning review background: the global literature on the links between climate change and human health is large, increasing exponentially, and it is no longer feasible to collate and synthesise using traditional systematic evidence mapping approaches. we aimed to use machine learning methods to systematically synthesise an evidence base on climate change and human health. methods: we used supervised machine learning and other natural language processing methods (topic modelling and geoparsing) to systematically identify and map the scientific literature on climate change and health published between jan 1, 2013, and april 9, 2020. only literature indexed in english were included. we searched web of science core collection, scopus, and pubmed using title, abstract, and keywords only. we searched for papers including both a health component and an explicit mention of either climate change, climate variability, or climate change-relevant weather phenomena. we classified relevant publications according to the fields of climate research, climate drivers, health impact, date, and geography. we used supervised and unsupervised machine learning to identify and classify relevant articles in the field of climate and health, with outputs including evidence heat maps, geographical maps, and narrative synthesis of trends in climate health-related publications. we included empirical literature of any study design that reported on health pathways associated with climate impacts, mitigation, or adaptation. findings: we predict that there are 15 963 studies in the field of climate and health published between 2013 and 2019. climate health literature is dominated by impact studies, with mitigation and adaptation responses and their co-benefits and co-risks remaining niche topics. air quality and heat stress are the most frequently studied exposures, with all-cause mortality and infectious disease incidence being the most frequently studied health outcomes. seasonality, extreme weather events, heat, and weather variability are the most frequently studied climate-related hazards. we found major gaps in evidence on climate health research for mental health, undernutrition, and maternal and child health. geographically, the evidence base is dominated by studies from high-income countries and china, with scant evidence from low-income counties, which often suffer most from the health consequences of climate change. interpretation: our findings show the importance and feasibility of using automated machine learning to comprehensively map the science on climate change and human health in the age of big literature. these can provide key inputs into global climate and health assessments. the scant evidence on climate change response options is concerning and could significantly hamper the design of evidence-based pathways to reduce the effects on health of climate change. in the post-2015 paris agreement era of climate solutions, we believe much more attention should be given to climate adaptation and mitigation options and their effects on human health. funding: foreign, commonwealth & development office.']"
60,business_communication,11,22,60_social media_media_social_disaster,"['social media', 'media', 'social', 'disaster', 'twitter', 'disasters', 'crisis', 'bert', 'natural', 'crowdsourced', 'text', 'wildfire', 'flood', 'events', 'waterlogging', 'online', 'reports', 'using social', 'supervision', 'urgency', 'natural disasters', 'messages', 'responders', 'information', 'disaster resilience', 'rainstorm', 'public', 'disaster management', 'related', 'natural hazard']","['deep learning-based assessment of flood severity using social media streams with a rapid change in climate, flood events have become a common issue in many parts of india. cities like pune, chennai experience heavy rainfall every year, followed by devastating floods. to better support the flood emergency plan operations, it is essential to have real-time flood maps depicting flood levels across the city. it can be made possible by mining information from flood-related social media posts shared by the public during floods. in this paper, we propose a deep learning-based method to assess the flood severity by using text and image data extracted from the social media posts. in the first stage of the methodology, the text data from the social media posts are analyzed, and flood-related posts are then passed to the second stage. in the second stage, the images associated with the social media posts are analyzed, and the severity of the flood in the particular location is updated in real-time. the text and image classification models are trained using the social media feeds posted during pune, chennai, and kerala floods. the accuracy obtained using the proposed methodology is 98% and 78% for text and image classification, respectively. by introducing text classification in the flood severity estimation task, social media posts that are irrelevant to the flood severity estimation task are ignored without processing the multimedia data associated with it. this, in turn, results in reduced usage of valuable computational resources as classification of multimedia data is expensive compared to the classification of microblog text data.', 'detecting natural hazard-related disaster impacts with social media analytics: the case of australian states and territories natural hazard-related disasters are disruptive events with significant impact on people, communities, buildings, infrastructure, animals, agriculture, and environmental assets. the expo-nentially increasing anthropogenic activities on the planet have aggregated the climate change and consequently increased the frequency and severity of these natural hazard-related disasters, and consequential damages in cities. the digital technological advancements, such as monitoring systems based on fusion of sensors and machine learning, in early detection, warning and disaster response systems are being implemented as part of the disaster management practice in many countries and presented useful results. along with these promising technologies, crowdsourced social media disaster big data analytics has also started to be utilized. this study aims to form an understanding of how social media analytics can be utilized to assist government authorities in estimating the damages linked to natural hazard-related disaster impacts on urban centers in the age of climate change. to this end, this study analyzes crowdsourced disaster big data from twitter users in the testbed case study of australian states and territories. the methodological approach of this study employs the social media analytics method and conducts sentiment and content analyses of location-based twitter messages (n = 131,673) from australia. the study informs authorities on an innovative way to analyze the geographic distribution, occurrence frequency of various disasters and their damages based on the geo-tweets analysis.', 'toward reduction of detrimental effects of hurricanes using a social media data analytic approach: how climate change is perceived? during natural disasters, there is a noticeably increased use of social media sites such as twitter. substantial research on social media data use during disasters has been conducted in the past decade since various social media platforms have emerged and gained popularity. this research highlights a thorough examination of the textual content of users’ posts shared on twitter across the 48 contiguous u.s. states (conus) during hurricanes harvey (2017) and dorian (2019). we processed and analyzed 35 million tweets by classifying them into the main topics of concern discussed on twitter over the conus. sentiment analysis, topic modeling, and topic classification are a few of the artificial intelligence techniques from natural language processing (nlp) that we employed in this work to analyze the twitter data. applying the nlp techniques on this large volume of data, made it possible to classify the tweet content into distinct categories in order to reveal valuable information on social response to hurricanes and assist crisis management agencies and disaster responders during and post disasters. furthermore, this study offers helpful insights on the way climate change is discussed on twitter before, during and after hurricane harvey and dorian. the outcome of this study uncovers detailed information on social response to hurricanes which benefits disaster managers and responders in reducing the detrimental effects of such extreme events and enhancing community readiness when these events occur.']"
61,business_communication,11,87,61_sentiment_media_twitter_social,"['sentiment', 'media', 'twitter', 'social', 'social media', 'public', 'opinion', 'text', 'climate change', 'topics', 'change', 'climate', 'opinions', 'news', 'topic', 'users', 'reputation', 'classifier', 'people', 'language', 'issues', 'textual', 'online', 'related', 'natural language', 'discussions', 'narratives', 'tourism', 'comments', 'perceptions']","['climate change sentiment analysis using lexicon, machine learning and hybrid approaches the emissions of greenhouse gases, such as carbon dioxide, into the biosphere have the consequence of warming up the planet, hence the existence of climate change. sentiment analysis has been a popular subject and there has been a plethora of research conducted in this area in recent decades, typically on social media platforms such as twitter, due to the proliferation of data generated today during discussions on climate change. however, there is not much research on the performances of different sentiment analysis approaches using lexicon, machine learning and hybrid methods, particularly within this domain-specific sentiment. this study aims to find the most effective sentiment analysis approach for climate change tweets and related domains by performing a comparative evaluation of various sentiment analysis approaches. in this context, seven lexicon-based approaches were used, namely sentiwordnet, textblob, vader, sentistrength, hu and liu, mpqa, and wkwsci. meanwhile, three machine learning classifiers were used, namely support vector machine, naïve bayes, and logistic regression, by using two feature extraction techniques, which were bag-of-words and tf–idf. next, the hybridization between lexicon-based and machine learningbased approaches was performed. the results indicate that the hybrid method outperformed the other two approaches, with hybrid textblob and logistic regression achieving an f1-score of 75.3%; thus, this has been chosen as the most effective approach. this study also found that lemmatization improved the accuracy of machine learning and hybrid approaches by 1.6%. meanwhile, the tf–idf feature extraction technique was slightly better than bow by increasing the accuracy of the logistic regression classifier by 0.6%. however, tf–idf and bow had an identical effect on svm and nb. future works will include investigating the suitability of deep learning approaches toward this domain-specific sentiment on social media platforms.', 'using artificial intelligence to monitor the evolution of opinion leaders sentiments: case study on global warming emotional and sentiment analysis of social media content is essential for smart city analytics. in the past few years, researchers have relied on online contents sentiment analysis to capture public opinion about current events. despite their merits, the existing solutions take a retrospective coarse-grained approach that analyses millions of social media posts to study public opinion about past events (e.g., presidential elections). such models give late insights, which makes it difficult to intervene or adjust strategies based on the evolution of public opinion over time. moreover, such approaches lack efficiency and scalability since they require the analysis of millions of posts to obtain accurate results. in this work, we address those limitations by proposing a framework for the real-time monitoring of the evolution of public opinion over time. to ensure efficiency and scalability, we focus on the analysis of high impact social media content generated by opinion leaders and their followers. to build our framework, we leveraged our opinion leaders identification algorithm, along with text mining and text classification techniques, to capture and analyze the evolution of the sentiments and emotions of 34 opinion leaders concerning the topic of global warming. the results obtained are very promising and open the door to advanced social media analytics to monitor public opinion in real-time.', 'an improved intelligent approach to enhance the sentiment classifier for knowledge discovery using machine learning aims: the proposed research work is on an evolutionary enhanced method for sentiment or emotion classification on unstructured review text in the big data field. the sentiment analysis plays a vital role for current generation of people for extracting valid decision points about any aspect such as movie ratings, education institute or politics ratings, etc. the proposed hybrid approach combined the optimal feature selection using particle swarm optimization (pso) and sentiment classification through support vector machine (svm). the current approach performance is evaluated with statisti-cal measures, such as precision, recall, sensitivity, specificity, and was compared with the existing ap-proaches. the earlier authors have achieved an accuracy of sentiment classifier in the english text up to 94% as of now. in the proposed scheme, an average accuracy of sentiment classifier on distinguishing datasets outperformed as 99% by tuning various parameters of svm, such as constant c value and kernel gamma value in association with pso optimization technique. the proposed method utilized three datasets, such as airline sentiment data, weather, and global warming datasets, that are publically available. the current experiment produced results that are trained and tested based on 10-fold cross-validations (fcv) and confusion matrix for predicting sentiment classifier accuracy. background: the sentiment analysis plays a vital role for current generation people for extracting valid de-cisions about any aspect such as movie rating, education institute or even politics ratings, etc. sentiment analysis (sa) or opinion mining has become fascinated scientifically as a research domain for the present environment. the key area is sentiment classification on semi-structured or unstructured data in distinguish languages, which has become a major research aspect. user-generated content [ugc] from distinguishing sources has been hiked significantly with rapid growth in a web environment. the huge user-generated data over social media provides substantial value for discovering hidden knowledge or correlations, patterns, and trends or sentiment extraction about any specific entity. sa is a computational analysis to determine the actual opinion of an entity which is expressed in terms of text. sa is also called as computation of emotional polarity expressed over social media as natural text in miscellaneous languages. usually, the automatic su-perlative sentiment classifier model depends on feature selection and classification algorithms. methods: the proposed work used support vector machine as classification technique and particle swarm optimization technique as feature selection purpose. in this methodology, we tune various permutations and combination parameters in order to obtain expected desired results with kernel and without kernel technique for sentiment classification on three datasets, including airline, global warm-ing, weather sentiment datasets, that are freely hosted for research practices. results: in the proposed scheme, the proposed method has outperformed with 99.2% of average accuracy to classify the sentiment on different datasets, among other machine learning techniques. the attained high accuracy in classifying sentiment or opinion about review text proves superior effectiveness over existing sentiment classifiers. the current experiment produced results that are trained and tested based on 10-fold cross-validations (fcv) and confusion matrix for predicting sentiment classifier accuracy. conclusion: the objective of the research issue sentiment classifier accuracy has been hiked with the help of kernel-based support vector machine (svm) based on parameter optimization. the optimal feature selection to classify sentiment or opinion towards review documents has been determined with the help of a particle swarm optimization approach. the proposed method utilized three datasets to simulate the results, such as airline sentiment data, weather sentiment data, and global warming data that are freely available datasets.']"
67,business_communication,11,144,67_students_education_educational_teaching,"['students', 'education', 'educational', 'teaching', 'student', 'learning', 'engineering', 'course', 'school', 'courses', 'skills', 'teachers', 'university', 'higher education', 'knowledge', 'science', 'virtual', 'sustainability', 'sustainable', 'development', 'environment', 'design', 'active learning', 'feedback', 'academic', 'engagement', 'technology', 'active', 'intelligence', 'online']","['ai in education: improving quality for both centralized and decentralized frameworks education is essential for achieving many sustainable development goals (sdgs). therefore, the education system focuses on empowering more educated people and improving the quality of the education system. one of the latest technologies to enhance the quality of education is artificial intelligence (ai)-based machine learning (ml). as a result, ml has a significant influence on the education system. ml is currently widely applied in the education system for various tasks, such as creating models by monitoring student performance and activities that accurately predict student outcomes, their engagement in learning activities, decision-making, problem-solving capabilities, etc. in this research, we provide a survey of machine learning frameworks for both distributed (clusters of schools and universities) and centralized (university or school) educational institutions to predict the quality of students learning outcomes and find solutions to improve the quality of their education system. additionally, this work explores the application of ml in teaching and learning for further improvements in the learning environment for centralized and distributed education systems.', 'mapping knowledge domain analysis in deep learning research of global education with the rapid development of the global digital knowledge economy, educational activities are facing more challenges. sustainable development education aims to cultivate students’ thinking ability to better integrate with the contemporary world view, so classroom practice should involve innovative teaching and learning. the goal of sustainable development education is to cultivate talents with high-level thinking and sustainable development abilities. the concept of deep learning emphasizes mobilizing students’ internal motivation, focusing on problem-solving ability, improving students’ critical thinking level, and developing students’ lifelong learning ability. the concept of deep learning has evolved with the times. the introduction of the concept of deep learning in teaching can enhance students’ understanding of the nature of knowledge, cultivate students’ high-level thinking, and enable students to achieve better learning results. integrating the concept of deep learning into teaching has extremely important significance and value for sustainable development education. it has become a hot topic in the world to comprehensively analyze the research status of deep learning and explore how deep learning can help education achieve sustainable development. in this study, citespace (6.1.r2) visualization analysis software was used to visualize and quantitatively analyze the literature on deep learning in the social science citation index (ssci). the visualized analysis is conducted on the annual publication amount, authors, institutions, countries, keywords, and high-frequency cited words of deep learning, to obtain the basic information, development status, hot spots, and evolution trends of deep learning research. the results show that the annual publication volume of deep learning is on the rise; deep learning research has entered a rapid growth stage since 2007; the united states has published the most papers and is the center of the global deep learning research collaboration network; the countries involved in the study were often interconnected, but the institutions and authors were relatively dispersed; research in the field of deep learning mainly focuses on concept exploration, influencing factors, implementation strategies and effectiveness of deep learning; learning method, learning strategy, curriculum design, interactive learning environment are the high-frequency keywords of deep learning research. it can be seen that deep learning research has the characteristics of transnationality, multidisciplinary nature and multi-perspective. in addition, this paper systematically analyzes the latest progress in global deep learning research and objectively predicts that using intelligent technology to design appropriate teaching and learning scenarios and evaluation methods may become the future development trend of deep learning. the research results of this paper will help readers to have a comprehensive understanding of deep learning research, provide deeper and more targeted resources for integrating deep learning concepts into teaching, and promote better sustainable development of education.', 'lifelong learning in higher education using learning analytics the conventional education system lacks the focus to create employable graduates. the existing industries in the information technology sector widely recruit based on a very specific set of skills and academic performances. to create better career opportunities, the colleges and universities should ensure that the graduates are qualified to accomplish the basic skills required by any organizations. this demands systematic approaches to be adopted with an innovative teaching style during the academic curriculum-based training in engineering colleges. the international accreditation organization abet is a worldwide recognized educational board that provides streamlined guidelines for competency skills and to deliver a quality education for students. in this research work, an extensive study has been performed on the needs of the industry and it has been compared to the quality of the course being offered to the students. the curriculum is designed based on the industry experts feedback. to achieve the student outcome criteria as per abet accreditation, the practices of structured approach is adopted along with the vision of lifelong learning in this research work. the teaching-learning process of first year under-graduate programming course and its evaluation techniques is considered in this research work. the competency skills like problem solving skills, critical thinking, and creative thinking are analysed using learning analytics strategies for first year python programming course. the performances of the students are broadly categorized based on the metrics like logical, conceptual, analytical and conceptual thinking, while simultaneously focusing on their time management skills and commitment to learn. artificial neural network (ann), naive bayesian algorithm and logistic regression models are used to identify and measure the competency skills of the learners achieved in this course and validating these metrics with the student learning outcome. implementing artificial intelligence concepts will provide results that can aid in creating the most suitable teaching-learning environment resulting in the best outcome for disruptive engineering education. learning analytics will provide an understanding and optimization of learning and its environments thereby ensuring sustainable development. this analysis presents an opportunity to identify the gap between the academic curriculum and the industrys expectations in terms of competency skills to be acquired by the learners. additionally, it helps to improve the teaching-learning process according to the dynamic changes from the industries and builds the foundation for students to be lifelong learners.']"
76,business_communication,11,567,76_ai_industry_business_manufacturing,"['ai', 'industry', 'business', 'manufacturing', 'intelligence', 'artificial intelligence', 'digital', 'artificial', 'sustainability', 'sustainable', 'technology', 'technologies', 'chain', 'development', 'social', 'supply chain', 'environment', 'supply', 'innovation', 'new', 'human', 'paper', 'process', 'production', 'systems', 'companies', 'intelligence ai', 'design', 'sustainable development', 'decision']","['blockchain facilitates a resilient supply chain in steel manufacturing under covid-19 the impact of covid-19 caused a crash in the supply chain across almost all manufacturers, retailers, and wholesalers and now dramatically impacts many of the processes in the manufacturing sectors. in particular, the steel industry, as an energy-intensive industry, is being urged to adopt digitalisation technology to achieve optimization and sustainable production. industries are fortunate to showcase an opportunity to advance the supply chain towards sustainability to deal with such a crisis. it is acknowledged that digital technology like blockchain can provide intercession by discovering real-time problems to perform certain sustainable development goals related to procurement, production and processing, logistics and transportation and the environment. however, the data structure which requires implementation by blockchain technology in the steel supply chain is restricted. blockchain technology has the outstanding position to transform the supply chain. the main challenge is that tracing border activities in the steel supply chain has not well been monitored. the digital system is drastically required to develop advanced technology to handle the disruptions and build a resilient supply chain. using blockchain technology can capture the goods movement across the entire supply chain to verify the quality and product provenance. furthermore, the rapid technology innovation in the steel sector involves the continuous update of skills and personnel. this study aims to explore the role of blockchain technology in the steel supply chain industry, such as inter-organizational trust, data transparency and immutability, interoperability and product type, and social influence and behavioural intention. a qualitative research approach was adopted on exploring multiple-case studies. the study reveals that blockchain technology contributes to a resilient steel supply chain in reducing risks and uncertainties with its transparency and immutability function. in the future, an integrated technology implementation framework should be developed to deploy artificial intelligence and blockchain technology in manufacturing industries to accelerate the digital transformation.', 'connecting engineering technology with enterprise systems for sustainable supply chain management organizations have been facing quite a few challenges, including a growing global competitive market, shorter time to market, rising product variants, and adjustments in production because of fluctuation in demand. to handle these challenges, industries need to connect engineering technology with enterprise systems to transform their practices toward industry 4.0 requirements. the supply chain sector is targeting stakeholders to enhance their product competitiveness by leveraging innovative digital technologies such as artificial intelligence, the internet of things (iot), and blockchain to make effective decisions instantaneously. this article will help in contextualizing emerging adaptive intelligence technology to drive connected intelligence and achieve supply chain operational excellence. a real-time case study in the manufacturing industry will be discussed. subsequently, how adaptive intelligence can help quality management in real-time will be explored to manage the production quality, which is measured by rejections, scraps, and cost savings. additionally, this article discusses how technology-embedded enterprise systems help the organization to manage the daily production, which is measured by production rate, quality, and yield. for the case organization, the iot architecture is proposed and the performance metric framework for the supply chain is described. furthermore, the article discusses how materials can be reused to extract economic benefits with collaborated diverse industries. this influences the eco-friendly environment across the supply chain with the focus on reducing the carbon footage.', 'evolution of artificial intelligence research in technological forecasting and social change: research topics, trends, and future directions artificial intelligence (ai) is a set of rapidly expanding disruptive technologies that are radically transforming various aspects related to people, business, society, and the environment. with the proliferation of digital computing devices and the emergence of big data, ai is increasingly offering significant opportunities for society and business organizations. the growing interest of scholars and practitioners in ai has resulted in the diversity of research topics explored in bulks of scholarly literature published in leading research outlets. this study aims to map the intellectual structure and evolution of the conceptual structure of overall ai research published in technological forecasting and social change (tf&sc). this study uses machine learning-based structural topic modeling (stm) to extract, report, and visualize the latent topics from the ai research literature. further, the disciplinary patterns in the intellectual structure of ai research are examined with the additional objective of assessing the disciplinary impact of ai. the results of the topic modeling reveal eight key topics, out of which the topics concerning healthcare, circular economy and sustainable supply chain, adoption of ai by consumers, and ai for decision-making are showing a rising trend over the years. ai research has a significant influence on disciplines such as business, management, and accounting, social science, engineering, computer science, and mathematics. the study provides an insightful agenda for the future based on evidence-based research directions that would benefit future ai scholars to identify contemporary research issues and develop impactful research to solve complex societal problems.']"
77,business_communication,11,61,77_financial_firms_corporate_companies,"['financial', 'firms', 'corporate', 'companies', 'risk', 'disclosure', 'ratings', 'reports', 'finance', 'investors', 'credit', 'environmental', 'firm', 'sustainable', 'enterprises', 'reporting', 'governance', 'financing', 'sustainability', 'performance', 'environmental performance', 'risks', 'social', 'banks', 'investment', 'returns', 'market', 'projects', 'text', 'listed']","['the pertinence of incorporating esg ratings to make investment decisions: a quantitative analysis using machine learning global sustainability being the major goal ahead, socially conscious investors are concerned about non-financial dimensions of investments like impact on environment (e), social relations (s), and corporate governance (g). this research aims to answer whether including esg data points is conducive to profitable investments while promoting sustainability. web-scraped a unique dataset of esg and key financial data of 1400+ companies from 34 stock markets internationally. quantitative analysis is performed on this data with the aim of determining whether the qualitative aspect of sustainable investments is tantamount to financial parameters. better esg scores indicate better financial performance. return on equity was 14% greater for top 10% esg companies than bottom 10%. prediction accuracy of ml models like linear, random forest regression increased when training data included both esg and financial data. the research concludes with a propitious relationship between esg data and financial growth parameters which are worth probing further.', 'incorporating esg in decision making for responsible and sustainable investments using machine learning over the years, the number of firms measuring and reporting environmental, social, and governance data has seen a massive shift due to the overwhelming demand and pressure from different stakeholders. the introduction of various international regulatory bodies like the corporate sustainability reporting directive (csrd), has also been intentional in encouraging companies to disclose publicly documents like annual reports, integrated reports in regards to topics like social, environmental, employee affairs and human rights. when it comes to investing, esg issues take into account a firms operational influence on the native environment. customers, policy makers, investors, and regulators are exerting huge amount of pressure on companies to carry out environmental, social, and governance (esg) reporting also known as non-financial reporting. sustainability reporting has previously exhibited numerous advantages to businesses as accurate data collection and reporting are essential for managing the companys sustainability performance as well as improving financial decision making. it is vital for a companys long-term performance to actively disclose and communicate its non-financial practices and approaches. therefore, in order to answer questions like; is it vital for developing market firms to disclose non-financial information, such as that relating to environmental, social, and governance (esg)?, this paper will attempt to provide a deeper insight into esg disclosure and the impact it has on firm performance using machine learning techniques (regression) and performance ratios (return on assets return on equity).', 'fundamental ratios as predictors of esg scores: a machine learning approach sustainable and responsible finance incorporates environmental, social, and governance (esg) principles into business decisions and investment strategies. in recent years, investors have rushed to sustainable and responsible investments in response to growing concerns about the risks of climate change. asset managers look for some assessment of sustainability for guidance and benchmarking, for instance, $30 trillion of assets are invested using some esg ratings. several studies argue that good esg ratings helped to prop up stock returns during the 2008 global financial crisis (lins et al. j finance 72(4):1785–1824,\xa02017). the esg score represents a benchmark of disclosures on public and private firms, it is based on different characteristics which are not directly related to the financial performance (harvard law school forum on corporate governance, esg reports and ratings:what they are, why they matter. https://corpgov.law.harvard.edu/2017/07/27/esg-reports-and-ratings-what-they-are-why-they-matter/, 2017). the role of esg ratings and their reliability have been widely discussed (berg et al. aggregate confusion: the divergence of esg ratings, mit sloan research paper no. 5822-19,\xa02019). sustainable investment professionals are unsatisfied with publicly traded companies’ climate-related disclosure. this negative sentiment is particularly strong in the usa, and within asset managers who do not believe that markets are consistently and correctly pricing climate risks into company and sector valuations. we believe that esg ratings, when available, still affect business and finance strategies and may represent a crucial element in the company’s fundraising process and on shares returns. we aim to assess how structural data as balance sheet items and income statements items for traded companies affect esg scores. using the bloomberg esg scores, we investigate the role of structural variables adopting a machine learning approach, in particular, the random forest algorithm. we use balance sheet data for a sample of the constituents of the euro stoxx 600 index, referred to the last decade, and investigate how these explain the esg bloomberg ratings. we find that financial statements items represent a powerful tool to explain the esg score.']"
